{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113095,
     "end_time": "2022-06-20T12:41:41.215410",
     "exception": false,
     "start_time": "2022-06-20T12:41:41.102315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-20T12:42:48.194197Z",
     "iopub.status.busy": "2022-06-20T12:42:48.193938Z",
     "iopub.status.idle": "2022-06-20T12:42:50.735561Z",
     "shell.execute_reply": "2022-06-20T12:42:50.734610Z"
    },
    "papermill": {
     "duration": 2.672311,
     "end_time": "2022-06-20T12:42:50.737847",
     "exception": false,
     "start_time": "2022-06-20T12:42:48.065536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "from params import DATA_PATH, OUT_PATH, RESSOURCES_PATH, IS_TEST\n",
    "from ressources import *\n",
    "from cleaning import *\n",
    "\n",
    "random.seed(13)\n",
    "# warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.128224,
     "end_time": "2022-06-20T12:42:52.888296",
     "exception": false,
     "start_time": "2022-06-20T12:42:52.760072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TEST:\n",
    "    train = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "else:\n",
    "    train = pd.read_csv(DATA_PATH + \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning & processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"lang\"] = train[\"name\"].apply(isEnglish).astype(\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill-in missing categories, based on words in name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Key_words_for_cat = pd.read_pickle(RESSOURCES_PATH + \"dict_for_missing_cat.pkl\")\n",
    "\n",
    "train[\"categories\"] = train[\"categories\"].fillna(\"\")\n",
    "idx_missing_cat = train[train[\"categories\"] == \"\"].index\n",
    "train.loc[idx_missing_cat, \"categories\"] = (\n",
    "    train.loc[idx_missing_cat, \"name\"]\n",
    "    .fillna(\"\")\n",
    "    .apply(lambda x: find_cat(x, Key_words_for_cat))\n",
    ")\n",
    "del Key_words_for_cat, idx_missing_cat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"point_of_interest\"] = (\n",
    "    train[\"point_of_interest\"].astype(\"category\").cat.codes\n",
    ")  # turn POI into ints to save spacetime\n",
    "train[\"latitude\"] = train[\"latitude\"].astype(\"float32\")\n",
    "train[\"longitude\"] = train[\"longitude\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorted by count in candidate training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_di = {}\n",
    "for i, c in enumerate(COUNTRIES):  # map train/test countries the same way\n",
    "    c_di[c] = min(\n",
    "        50, i + 1\n",
    "    )  # cap country at 50 - after that there are too few cases per country to split them\n",
    "train[\"country\"] = (\n",
    "    train[\"country\"].fillna(\"ZZ\").map(c_di).fillna(50).astype(\"int16\")\n",
    ")  # new country maps to missing (ZZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index()\n",
    "train = train.sort_values(by=[\"point_of_interest\", \"id\"]).reset_index(drop=True)\n",
    "id_all = np.array(train[\"id\"])\n",
    "poi_all = np.array(train[\"point_of_interest\"])\n",
    "poi0 = poi_all[0]\n",
    "id0 = id_all[0]\n",
    "di_poi = {}\n",
    "for i in range(1, train.shape[0]):\n",
    "    if poi_all[i] == poi0:\n",
    "        id0 = str(id0) + \" \" + str(id_all[i])\n",
    "    else:\n",
    "        di_poi[poi0] = str(id0) + \" \"  # need to have trailing space in m_true\n",
    "        poi0 = poi_all[i]\n",
    "        id0 = id_all[i]\n",
    "di_poi[poi0] = str(id0) + \" \"  # need to have trailing space in m_true\n",
    "train[\"m_true\"] = train[\"point_of_interest\"].map(di_poi)\n",
    "train = train.sort_values(by=\"index\").reset_index(\n",
    "    drop=True\n",
    ")  # sort back to original order\n",
    "train.drop(\"index\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"name_svg\"] = train[\"name\"].copy()\n",
    "train[\"categories_svg\"] = train[\"categories\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"name\"] = train[\"name\"].apply(lambda x: unidecode(str(x).lower()))\n",
    "train[\"name\"] = train[\"name\"].apply(lambda text: replace_seven_eleven(text))\n",
    "train[\"name\"] = train[\"name\"].apply(lambda text: replace_seaworld(text))\n",
    "train[\"name\"] = train[\"name\"].apply(lambda text: replace_mcdonald(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"category_simpl\"] = (\n",
    "    train[\"categories\"]\n",
    "    .astype(str)\n",
    "    .apply(lambda text: simplify_cat(text, CAT_REGROUP))\n",
    "    .astype(\"int16\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Simpl categories found :\", len(train[train[\"category_simpl\"] > 0]), \"/\", len(train)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go back to initial columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"name\"] = train[\"name_svg\"].copy()\n",
    "train[\"categories\"] = train[\"categories_svg\"].copy()\n",
    "train.drop([\"name_svg\", \"categories_svg\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save names separated by spaces for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"categories_split\"] = (\n",
    "    train[\"categories\"]\n",
    "    .astype(str)\n",
    "    .apply(lambda x: [st(cat, remove_space=True) for cat in x.split(\",\")])\n",
    "    .copy()\n",
    ")  # Create a new columns to split the categories\n",
    "train[\"name_initial\"] = train[\"name\"].astype(str).apply(lambda x: x.lower()).copy()\n",
    "train[\"name_initial_decode\"] = (\n",
    "    train[\"name\"].astype(str).apply(lambda x: st(x, remove_space=False)).copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the score of the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pairings = pd.read_pickle(\n",
    "    RESSOURCES_PATH + \"howmanytimes_groupedcat_are_paired_with_other_groupedcat.pkl\"\n",
    ")  # link-between-grouped-cats\n",
    "\n",
    "# Find the score of the categories\n",
    "train[\"freq_pairing_with_other_groupedcat\"] = (\n",
    "    train[\"category_simpl\"].apply(lambda cat: cat_pairings[cat]).fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solo_cat_scores = pd.read_pickle(\n",
    "    RESSOURCES_PATH + \"solo_cat_score.pkl\"\n",
    ")  # link-between-categories - 1858 values\n",
    "\n",
    "train[\"cat_solo_score\"] = (\n",
    "    train[\"categories_split\"]\n",
    "    .apply(lambda List_cat: apply_solo_cat_score(List_cat, solo_cat_scores))\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the scores\n",
    "Dist_quantiles = pd.read_pickle(\n",
    "    RESSOURCES_PATH + \"Dist_quantiles_per_cat.pkl\"\n",
    ")  # dist-quantiles-per-cat - 869 values\n",
    "\n",
    "col_cat_distscores = [\"Nb_multiPoi\", \"mean\", \"q25\", \"q50\", \"q75\", \"q90\", \"q99\"]\n",
    "train.loc[:, col_cat_distscores] = (\n",
    "    train[\"categories_split\"]\n",
    "    .apply(lambda x: apply_cat_distscore(x, Dist_quantiles))\n",
    "    .to_list()\n",
    ")  # 'Nb_multiPoi', 'mean', 'q25', 'q50', 'q75', 'q90','q99'\n",
    "for col in [\n",
    "    \"cat_solo_score\",\n",
    "    \"freq_pairing_with_other_groupedcat\",\n",
    "    \"Nb_multiPoi\",\n",
    "    \"mean\",\n",
    "    \"q25\",\n",
    "    \"q50\",\n",
    "    \"q75\",\n",
    "    \"q90\",\n",
    "    \"q99\",\n",
    "]:\n",
    "    train[col] = train[col].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some expressions from name\n",
    "train[\"name\"] = train[\"name\"].apply(rem_expr)\n",
    "\n",
    "# drop abbreviations all caps in brakets for long enough names\n",
    "train[\"name\"] = train[\"name\"].apply(rem_abr)\n",
    "\n",
    "# select capitals only, or first letter of each word (which could have been capital)\n",
    "train[\"nameC\"] = train[\"name\"].fillna(\"\").apply(get_caps_leading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More cleaning\n",
    "- A bit slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm([\"name\", \"address\", \"city\", \"state\", \"zip\", \"url\", \"categories\"]):\n",
    "    train[col] = train[col].astype(\"str\").apply(st)  # keep spaces\n",
    "    if col in [\"name\", \"address\"]:\n",
    "        train[col] = train[col].apply(rem_words)\n",
    "        train[col] = train[col].apply(clean_nums)\n",
    "        if col == \"address\":\n",
    "            train[\"address\"] = train[\"address\"].apply(clean_address)\n",
    "    train[col] = train[col].apply(lambda x: x.replace(\" \", \"\"))  # remove spaces\n",
    "\n",
    "train[\"city\"] = train[\"city\"].apply(st2)  # remove digits from cities\n",
    "train[\"latitude\"] = np.round(train[\"latitude\"], 5).astype(\"float32\")\n",
    "train[\"longitude\"] = np.round(train[\"longitude\"], 5).astype(\"float32\")\n",
    "\n",
    "# for sorting - rounded coordinates\n",
    "train[\"lat2\"] = np.round(train[\"latitude\"], 0).astype(\"float32\")\n",
    "train[\"lon2\"] = np.round(train[\"longitude\"], 0).astype(\"float32\")\n",
    "\n",
    "# for sorting - short name\n",
    "train[\"name2\"] = train[\"name\"].str[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Name again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix some misspellings\n",
    "for w in NAME_DI.keys():\n",
    "    train[\"name\"] = train[\"name\"].apply(lambda x: x.replace(w, NAME_DI[w]))\n",
    "\n",
    "# new code from V *************************************************************\n",
    "# Group names\n",
    "name_groups = pd.read_pickle(RESSOURCES_PATH + \"name_groups.pkl\")\n",
    "# Translation\n",
    "trans = {}\n",
    "for best, group in name_groups.items():\n",
    "    for n in group:\n",
    "        trans[n] = best\n",
    "train[\"name_grouped\"] = train[\"name\"].apply(lambda n: trans[n] if n in trans else n)\n",
    "print(\n",
    "    f\"Grouped names : {len(train[train['name_grouped'] != train['name']])}/{len(train)}.\"\n",
    ")\n",
    "train[\"name\"] = train[\"name_grouped\"].copy()\n",
    "train = train.drop(columns=[\"name_grouped\"])\n",
    "del name_groups, trans\n",
    "gc.collect()\n",
    "\n",
    "# cap length at 76\n",
    "train[\"name\"] = train[\"name\"].str[:76]\n",
    "# eliminate some common words that do not change meaning\n",
    "for w in [\"center\"]:\n",
    "    train[\"name\"] = train[\"name\"].apply(lambda x: x.replace(w, \"\"))\n",
    "train[\"name\"].loc[train[\"name\"] == \"nan\"] = \"\"\n",
    "# walmart\n",
    "train[\"name\"] = train[\"name\"].apply(lambda x: \"walmart\" if \"walmart\" in x else x)\n",
    "# carrefour\n",
    "train[\"name\"] = train[\"name\"].apply(lambda x: \"carrefour\" if \"carrefour\" in x else x)\n",
    "# drop leading 'the' from name\n",
    "idx = train[\"name\"].str[:3] == \"the\"  # happens 17,712 times = 1.5%\n",
    "train[\"name\"].loc[idx] = train[\"name\"].loc[idx].str[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in CITY_DI.keys():\n",
    "    train[\"city\"].loc[train[\"city\"] == key] = CITY_DI[key]\n",
    "# second pass\n",
    "\n",
    "for key in CITY_DI_2.keys():\n",
    "    train[\"city\"].loc[train[\"city\"] == key] = CITY_DI_2[key]\n",
    "\n",
    "# cap length at 38\n",
    "train[\"city\"] = train[\"city\"].str[:38]\n",
    "# eliminate some common words that do not change meaning\n",
    "for w in [\"gorod\"]:\n",
    "    train[\"city\"] = train[\"city\"].apply(lambda x: x.replace(w, \"\"))\n",
    "train[\"city\"].loc[train[\"city\"] == \"nan\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"address\"].loc[train[\"address\"] == \"nan\"] = \"\"\n",
    "train[\"address\"] = train[\"address\"].str[:99]  # cap length at 99\n",
    "train[\"address\"] = train[\"address\"].apply(lambda x: x.replace(\"street\", \"str\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"state\"] = train[\"state\"].str[:33]  # cap length at 33\n",
    "state_di = {\n",
    "    \"calif\": \"ca\",\n",
    "    \"jakartacapitalregion\": \"jakarta\",\n",
    "    \"moscow\": \"moskva\",\n",
    "    \"seoulteugbyeolsi\": \"seoul\",\n",
    "}\n",
    "for key in state_di.keys():\n",
    "    train[\"state\"].loc[train[\"state\"] == key] = state_di[key]\n",
    "train[\"state\"].loc[train[\"state\"] == \"nan\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"url\"] = train[\"url\"].str[:129]  # cap length at 129\n",
    "train[\"url\"].loc[train[\"url\"] == \"nan\"] = \"\"\n",
    "idx = train[\"url\"].str[:8] == \"httpswww\"\n",
    "train[\"url\"].loc[idx] = train[\"url\"].str[8:].loc[idx]\n",
    "idx = train[\"url\"].str[:7] == \"httpwww\"\n",
    "train[\"url\"].loc[idx] = train[\"url\"].str[7:].loc[idx]\n",
    "idx = train[\"url\"].str[:5] == \"https\"\n",
    "train[\"url\"].loc[idx] = train[\"url\"].str[5:].loc[idx]\n",
    "idx = train[\"url\"].str[:4] == \"http\"\n",
    "train[\"url\"].loc[idx] = train[\"url\"].str[4:].loc[idx]\n",
    "train[\"url\"].loc[train[\"url\"] == \"nan\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"phone\"] = train[\"phone\"].apply(lambda text: process_phone(text))\n",
    "# all matches of last 9 digits look legit - drop leading digit\n",
    "train[\"phone\"] = train[\"phone\"].str[1:]\n",
    "# set invalid numbers to empty\n",
    "idx = (train[\"phone\"] == \"000000000\") | (train[\"phone\"] == \"999999999\")\n",
    "train[\"phone\"].loc[idx] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"categories\"] = train[\"categories\"].str[:68]  # cap length at 68\n",
    "train[\"categories\"].loc[train[\"categories\"] == \"nan\"] = \"\"\n",
    "cat_di = {\"aiport\": \"airport\", \"terminal\": \"airport\"}\n",
    "for key in cat_di.keys():\n",
    "    train[\"categories\"] = train[\"categories\"].apply(\n",
    "        lambda x: x.replace(key, cat_di[key])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate Indonesian\n",
    "idx = train[\"country\"] == 2  # ID\n",
    "\n",
    "for col in [\"name\", \"address\", \"city\", \"state\"]:\n",
    "    train[col].loc[idx] = train[col].loc[idx].apply(lambda x: id_translate(x, ID_DI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate russian words\n",
    "dict_ru_en = pd.read_pickle(RESSOURCES_PATH + \"dict_translate_russian.pkl\")\n",
    "\n",
    "idx = train[\"country\"] == 6  # RU\n",
    "for k in [\"city\", \"state\", \"address\", \"name\"]:\n",
    "    train.loc[idx, k] = (\n",
    "        train.loc[idx, k]\n",
    "        .astype(str)\n",
    "        .apply(lambda x: translate_russian_word_by_word(x, dict_ru_en))\n",
    "    )\n",
    "    train.loc[idx, k] = train.loc[idx, k].apply(lambda x: \"\" if x == \"nan\" else x)\n",
    "del dict_ru_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match some identical names - based on analysis of mismatched names for true pairs\n",
    "# soekarno-hatta international airport - Jakarta, ID\n",
    "\n",
    "idx = train[\"country\"] == 2  # ID - this is where this location is\n",
    "\n",
    "for l1 in L1S_ID:\n",
    "    train[\"name\"].loc[idx] = (\n",
    "        train[\"name\"].loc[idx].apply(lambda x: x if x not in l1[1:] else l1[0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1 in LL1:\n",
    "    train[\"name\"] = train[\"name\"].apply(lambda x: x if x not in l1[1:] else l1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"name\"] = train[\"name\"].apply(replace_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cat 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cat2 (clean category with low cardinality)\n",
    "# base it on address, name and catogories - after those have been cleaned (then do not need to include misspellings)\n",
    "train[\"cat2\"] = \"\"  # init (left: 129824*)\n",
    "\n",
    "for col in [\"address\", \"categories\", \"name\"]:\n",
    "    for word in ALL_WORDS.keys():\n",
    "        words = ALL_WORDS[word]\n",
    "        for w in words:\n",
    "            train[\"cat2\"].loc[train[col].str.contains(w, regex=False)] = word\n",
    "\n",
    "train[\"cat2\"] = train[\"cat2\"].map(CAT2_DI).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"zip\",\n",
    "    \"url\",\n",
    "    \"phone\",\n",
    "    \"categories\",\n",
    "    \"m_true\",\n",
    "    \"categories_split\",\n",
    "    \"name_initial\",\n",
    "    \"name_initial_decode\",\n",
    "    \"nameC\",\n",
    "    \"name2\",\n",
    "]:\n",
    "    train.loc[train[c] == \"null\", c] = \"\"\n",
    "    train.loc[train[c] == \"nan\", c] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TEST:\n",
    "    train.to_csv(OUT_PATH + \"cleaned_data_test.csv\", index=False)\n",
    "else:\n",
    "    train.to_csv(OUT_PATH + \"cleaned_data_train.csv\", index=False)\n",
    "    \n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload & check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPES = {\n",
    "    \"id\": np.dtype(\"O\"),\n",
    "    \"name\": np.dtype(\"O\"),\n",
    "    \"latitude\": np.dtype(\"float32\"),\n",
    "    \"longitude\": np.dtype(\"float32\"),\n",
    "    \"address\": np.dtype(\"O\"),\n",
    "    \"city\": np.dtype(\"O\"),\n",
    "    \"state\": np.dtype(\"O\"),\n",
    "    \"zip\": np.dtype(\"O\"),\n",
    "    \"country\": np.dtype(\"int16\"),\n",
    "    \"url\": np.dtype(\"O\"),\n",
    "    \"phone\": np.dtype(\"O\"),\n",
    "    \"categories\": np.dtype(\"O\"),\n",
    "    \"point_of_interest\": np.dtype(\"int32\"),\n",
    "    \"lang\": np.dtype(\"int8\"),\n",
    "    \"m_true\": np.dtype(\"O\"),\n",
    "    \"category_simpl\": np.dtype(\"int16\"),\n",
    "    \"categories_split\": np.dtype(\"O\"),\n",
    "    \"name_initial\": np.dtype(\"O\"),\n",
    "    \"name_initial_decode\": np.dtype(\"O\"),\n",
    "    \"freq_pairing_with_other_groupedcat\": np.dtype(\"float32\"),\n",
    "    \"cat_solo_score\": np.dtype(\"float32\"),\n",
    "    \"Nb_multiPoi\": np.dtype(\"float32\"),\n",
    "    \"mean\": np.dtype(\"float32\"),\n",
    "    \"q25\": np.dtype(\"float32\"),\n",
    "    \"q50\": np.dtype(\"float32\"),\n",
    "    \"q75\": np.dtype(\"float32\"),\n",
    "    \"q90\": np.dtype(\"float32\"),\n",
    "    \"q99\": np.dtype(\"float32\"),\n",
    "    \"nameC\": np.dtype(\"O\"),\n",
    "    \"lat2\": np.dtype(\"float32\"),\n",
    "    \"lon2\": np.dtype(\"float32\"),\n",
    "    \"name2\": np.dtype(\"O\"),\n",
    "    \"cat2\": np.dtype(\"int16\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaded = pd.read_csv(OUT_PATH + \"cleaned_data.csv\", dtype=TYPES)\n",
    "# train_loaded = pd.read_csv(OUT_PATH + \"TRAIN_CHECK.csv\", dtype=TYPES)\n",
    "\n",
    "train_loaded.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "train_loaded[\"categories_split\"] = train_loaded[\"categories_split\"].apply(\n",
    "    ast.literal_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"cat2\"\n",
    "\n",
    "display(train[(train[col] != train_loaded[col])][col])\n",
    "display(train_loaded[(train[col] != train_loaded[col])][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.shape == train_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train[train.columns] != train_loaded[train.columns]).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_yv = pd.read_csv(OUT_PATH + \"p1_yv.csv\")\n",
    "p2_yv = pd.read_csv(OUT_PATH + \"p2_yv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.read_csv(\"P1_CHECK.csv\")\n",
    "p2 = pd.read_csv(\"P2_CHECK.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_yv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1[p1_yv.columns] == p1_yv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
