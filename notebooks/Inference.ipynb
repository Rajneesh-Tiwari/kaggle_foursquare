{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import ast\n",
    "import cudf\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pandarallel.initialize(progress_bar=False, use_memory_fs=False)\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from inference.knn import *\n",
    "from inference.predict import predict\n",
    "\n",
    "from data.features import *\n",
    "from data.preparation import *\n",
    "from data.post_processing import *\n",
    "from data.dataset import SingleDataset\n",
    "from data.tokenization import get_tokenizer\n",
    "\n",
    "from model_zoo.models import SingleTransformer\n",
    "\n",
    "from utils.logger import Config\n",
    "from utils.torch import load_model_weights\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # df = cudf.read_csv(DATA_PATH + \"test.csv\").set_index('id')\n",
    "    df = cudf.read_csv(DATA_PATH + \"train.csv\").set_index('id')\n",
    "    folds = cudf.read_csv(DATA_PATH + \"folds_2.csv\")[['id', 'fold']]\n",
    "    df = df.merge(folds, how=\"left\", on=\"id\").set_index(\"id\")\n",
    "\n",
    "    df = df[df['fold'] == 0]\n",
    "    \n",
    "    gt_matches = build_gt(df.reset_index().to_pandas(), save=False)\n",
    "else:\n",
    "    df = cudf.read_csv(DATA_PATH + \"test.csv\").set_index('id')\n",
    "    gt_matches = None\n",
    "    \n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_DIST = None\n",
    "MAX_DIST = 0.5\n",
    "NEIGHBORS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERT_JAP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = \"../output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matches = get_nearest_neighbors(df, n_neighbors=NEIGHBORS, max_dist=MAX_DIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phone matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['phone_len'] = df[['phone']].to_pandas()['phone'].fillna('').apply(len)\n",
    "df_phone = df[df['phone_len'] > 5]\n",
    "df_phone = df_phone[df_phone['phone_len'] < 25]\n",
    "\n",
    "df_phone = df_phone.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_matches = {}\n",
    "\n",
    "for country, df_phone_c in tqdm(df_phone.groupby(\"country\")):\n",
    "    if country == \"US\":\n",
    "        # Group by state\n",
    "        for state, df_phone_c_s in tqdm(df_phone_c.groupby(\"state\")):\n",
    "            for id_ in df_phone_c_s.index:\n",
    "                m = find_phone_matches(id_, df_phone_c_s)\n",
    "                if len(m):\n",
    "                    phone_matches[id_] = m\n",
    "    else:\n",
    "        for id_ in df_phone_c.index:\n",
    "            m = find_phone_matches(id_, df_phone_c)\n",
    "            if len(m):\n",
    "                phone_matches[id_] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_matches = {\n",
    "    k : matches + [m for m in phone_matches.get(k, []) if m not in matches] for k, matches in dist_matches.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    found_prop, missed_pos = compute_found_prop(naive_matches, gt_matches)\n",
    "    n_matches = sum([len(naive_matches[k]) for k in naive_matches])\n",
    "    print(f'Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_FT_FOLDERS = [\n",
    "#     (\"xlm-large\", LOG_PATH + \"2022-05-19/4/\"),            # 1 ep, d=256, large\n",
    "#     (\"roberta\", LOG_PATH + \"2022-05-20/1/\"),              # roberta-large\n",
    "    (\"xlm-base+url\", LOG_PATH + \"2022-05-20/2/\"),         # base + url\n",
    "#     (\"xlm-large+noaddress\", LOG_PATH + \"2022-05-20/3/\"),  # large + no address\n",
    "]\n",
    "\n",
    "FOLD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = prepare_nn_data(df)\n",
    "\n",
    "assert (df_nn.index == df.index.to_pandas()).all(), \"Indexes do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, EXP_FOLDER in NN_FT_FOLDERS:      \n",
    "#     if OUT_PATH + f\"fts_{name}.npy\":\n",
    "#         print(' -> Retrieved already computed embeddings :',  OUT_PATH + f\"fts_{name}.npy \\n\")\n",
    "#         continue\n",
    "\n",
    "    config = Config(json.load(open(EXP_FOLDER + \"config.json\", 'r')))\n",
    "\n",
    "    tokenizer = get_tokenizer(config.name)\n",
    "    dataset = SingleDataset(\n",
    "        df_nn,\n",
    "        tokenizer,\n",
    "        config.max_len,\n",
    "        use_url=\"+url\" in name,\n",
    "        use_address=not \"+noaddress\" in name,\n",
    "    )\n",
    "\n",
    "    model = SingleTransformer(\n",
    "        config.name,\n",
    "        nb_layers=config.nb_layers,\n",
    "        no_dropout=config.no_dropout,\n",
    "        embed_dim=config.embed_dim,\n",
    "        nb_features=config.nb_features,\n",
    "    ).cuda()\n",
    "    model.zero_grad()\n",
    "\n",
    "    weights = sorted(glob.glob(EXP_FOLDER + \"*.pt\"))\n",
    "    model = load_model_weights(model, weights[FOLD])\n",
    "\n",
    "    preds = predict(model, dataset, config.data_config)\n",
    "\n",
    "    np.save(OUT_PATH + f\"fts_{name}.npy\", preds)\n",
    "    print(f' -> Saved features to \"{OUT_PATH}fts_{name}.npy\"\\n')\n",
    "\n",
    "    del preds, model, tokenizer, dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"xlm-large\"\n",
    "# NAME = \"xlm-base+url\"\n",
    "\n",
    "preds = np.load(OUT_PATH + f'fts_{NAME}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_matches = find_matches(preds, df_nn, NEIGHBORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    found_prop, missed_pos = compute_found_prop(nn_matches, gt_matches)\n",
    "    n_matches = sum([len(nn_matches[k]) for k in nn_matches])\n",
    "    print(f'Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')\n",
    "    \n",
    "    merged_matches = {k : list(set(naive_matches[k] + nn_matches[k])) for k in nn_matches}\n",
    "    found_prop, missed_pos = compute_found_prop(merged_matches, gt_matches)\n",
    "    n_matches = sum([len(merged_matches[k]) for k in merged_matches])\n",
    "    print(f'Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs = create_pairs(nn_matches, naive_matches, NEIGHBORS, gt_matches=gt_matches)\n",
    "\n",
    "df_pairs.to_csv(OUT_PATH + \"pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, df_p, idx=0, save=False):\n",
    "    features = []\n",
    "\n",
    "    df_p = df_p.merge(df, how=\"left\", left_on=\"id_1\", right_on=\"id\")\n",
    "    df_p = df_p.merge(df, how=\"left\", left_on=\"id_2\", right_on=\"id\", suffixes=(\"_1\", \"_2\"))\n",
    "\n",
    "    df_p.loc[df_p['rank'] == -1, 'rank'] = np.nan\n",
    "    df_p.loc[df_p['rank_nn'] == -1, 'rank_nn'] = np.nan\n",
    "\n",
    "    print('- Computing rank features')\n",
    "    df_p['rank_nan'] = df_p[\"rank\"].isna().astype(np.uint8)\n",
    "    df_p['rank_nn_nan'] = df_p[\"rank_nn\"].isna().astype(np.uint8)\n",
    "    df_p[\"rank_both_nan\"] = df_p[[\"rank_nan\", \"rank_nn_nan\"]].min(axis=1)\n",
    "    df_p[\"rank_any_nan\"] = df_p[[\"rank_nan\", \"rank_nn_nan\"]].max(axis=1)\n",
    "        \n",
    "    features += [\n",
    "        \"rank\", \"rank_nn\", \n",
    "        \"rank_nan\", \"rank_nn_nan\", \n",
    "        \"rank_both_nan\", \"rank_any_nan\",\n",
    "    ]\n",
    "\n",
    "    print('- Computing nan features')\n",
    "    features += compute_nan_features(df_p, NAN_COLS)\n",
    "    \n",
    "    for name, folder in NN_FT_FOLDERS:\n",
    "        print(f'- Adding features using model {name}')\n",
    "        nn_preds = np.load(OUT_PATH + f\"fts_{name}.npy\").astype(np.float16)\n",
    "        nn_preds = torch.from_numpy(nn_preds).cuda()\n",
    "\n",
    "        features += compute_nn_distances(df_p, nn_preds, suffix=\"_\" + name)\n",
    "\n",
    "        del nn_preds\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print('- Computing position distances')\n",
    "    features += compute_position_distances(df_p)\n",
    "    \n",
    "    for col in TF_IDF_COLS:\n",
    "        for ngram_range, analyzer in TF_IDF_PARAMS:\n",
    "            ft_name = f\"{col}_tf_idf_{ngram_range[0]}{ngram_range[1]}_{analyzer}_sim\"\n",
    "            print(f'- Computing feature {ft_name}')\n",
    "\n",
    "            tf_idf = TfidfVectorizer(use_idf=False, ngram_range=ngram_range, analyzer=analyzer)\n",
    "            tf_idf_mat = tf_idf.fit_transform(df[col].fillna('nan'))\n",
    "\n",
    "            df_p[ft_name] = tf_idf_similarity(df_p, tf_idf_mat)\n",
    "            features.append(ft_name)\n",
    "\n",
    "    if not isinstance(df_p, pd.DataFrame):\n",
    "        df_p = df_p.to_pandas()\n",
    "        \n",
    "    for col, fct in FEATURES_SAME:\n",
    "        print(f'- Computing feature same_{col}')\n",
    "        df_p[f\"same_{col}\"] = df_p[[f\"{col}_1\", f\"{col}_2\"]].fillna('').parallel_apply(\n",
    "            lambda x: fct(x[0], x[1]), axis=1\n",
    "        ).astype(float)\n",
    "\n",
    "        features.append(f\"same_{col}\")\n",
    "        \n",
    "    features += compute_string_distances(df_p, STRING_DIST_COLS, verbose=1)\n",
    "    \n",
    "    to_keep = ['id_1', 'id_2', 'point_of_interest_1', 'point_of_interest_2', 'match'] + features\n",
    "\n",
    "    df_p.drop([c for c in df_p.columns if c not in to_keep], axis=1, inplace=True)\n",
    "    df_p['match'] = df_p['match'].astype(int)\n",
    "    \n",
    "    if save:\n",
    "        print('\\n -> Saving features to :', OUT_PATH + f\"df_p_{idx}.csv\")\n",
    "        df_p.to_csv(OUT_PATH + f\"df_p_{idx}.csv\")\n",
    "    \n",
    "    return df_p, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_xgb(df_p, exp_folder, debug=False):\n",
    "    \n",
    "    pred_test = np.zeros(len(df_p))\n",
    "    model_paths = sorted(glob.glob(EXP_FOLDER + \"*.pkl\"))\n",
    "\n",
    "    for fold, model_path in enumerate(model_paths):\n",
    "        print(f'- Model {model_path.split(\"/\")[-1].split(\".\")[0]} ')\n",
    "        config = Config(json.load(open(EXP_FOLDER + \"config.json\", 'r')))\n",
    "\n",
    "        model = pickle.load(open(model_path, 'rb'))    \n",
    "\n",
    "        if debug:\n",
    "            df_val = df_p[(df_p[\"fold_1\"] == fold) | (df_p[\"fold_2\"] == fold)]\n",
    "\n",
    "            val_idx = (\n",
    "                df_val.index.values if isinstance(df_val, pd.DataFrame) else df_val.index.values.get()\n",
    "            )\n",
    "            pred_test[val_idx] = model.predict_proba(df_val[config.features])[:, 1]\n",
    "        else:\n",
    "            pred_test += model.predict_proba(df_p[config.features])[:, 1] / len(model_paths)\n",
    "            \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = cudf.read_csv(OUT_PATH + \"pairs.csv\")# .sort_values('id_1', 'id_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = df.to_pandas().copy()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if CONVERT_JAP:\n",
    "    df = convert_japanese_alphabet(df)\n",
    "\n",
    "# df = reduce_mem_usage(df)\n",
    "df['idx'] = np.array(range(len(df)))\n",
    "\n",
    "df = cudf.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params\n",
    "- todo: fit tf-idf mats only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAN_COLS = ['address', 'city', 'state', 'zip', 'url', 'phone']\n",
    "\n",
    "TF_IDF_COLS = ['name', 'categories', 'address', 'url']\n",
    "\n",
    "TF_IDF_PARAMS = [\n",
    "    ((1, 1), 'word'),  # word unigrams\n",
    "    ((3, 3), 'char_wb'),  # char trigrams\n",
    "]\n",
    "\n",
    "STRING_DIST_COLS = ['name', \"categories\", 'address', 'url', 'phone']\n",
    "\n",
    "FEATURES_SAME = [\n",
    "    ('country', is_equal),\n",
    "    ('state', is_equal),\n",
    "    ('zip', is_included),\n",
    "    ('phone', is_included),\n",
    "    ('city', is_included),\n",
    "    ('categories', is_included),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_FT_FOLDERS = [\n",
    "    (\"xlm-large\", LOG_PATH + \"2022-05-19/4/\"),            # 1 ep, d=256, large\n",
    "#     (\"roberta\", LOG_PATH + \"2022-05-20/1/\"),              # roberta-large\n",
    "#     (\"xlm-base+url\", LOG_PATH + \"2022-05-20/2/\"),         # base + url\n",
    "    (\"xlm-large+noaddress\", LOG_PATH + \"2022-05-20/3/\"),  # large + no address\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GROUPS = [\n",
    "    \"nn_dist_l1_*\",\n",
    "    \"nn_dist_l2_*\",\n",
    "    \"nn_cosine_sim_*\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 3000000\n",
    "BATCH_SIZE = 15000000\n",
    "# BATCH_SIZE = 5000\n",
    "BATCHES = list(np.arange(0, len(pairs), BATCH_SIZE)) + [len(pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-21/2/\"  # 0.8714 - xgb 20 neighbors fix fewer nn\n",
    "FOLD = 0\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(BATCHES) - 1)):\n",
    "    print(f' -> Indices {BATCHES[i]} -> {BATCHES[i + 1]}\\n')\n",
    "    pairs_ = pairs.iloc[range(BATCHES[i], BATCHES[i + 1])]\n",
    "\n",
    "    print('# Feature engineering \\n')\n",
    "    df_p, features = feature_engineering(df, pairs_, idx=i, save=False)\n",
    "    \n",
    "    print('\\n# Inference \\n')\n",
    "\n",
    "    if DEBUG:  # retrieve folds\n",
    "        df_split = pd.read_csv(DATA_PATH + f\"folds_{FOLD}_{N_SPLITS}.csv\")\n",
    "\n",
    "        df_p = df_p.merge(df_split[['id', 'fold']], how=\"left\", left_on=\"id_1\", right_on=\"id\")\n",
    "        df_p.drop('id', axis=1, inplace=True)\n",
    "        df_p = df_p.merge(\n",
    "            df_split[['id', 'fold']], how=\"left\", left_on=\"id_2\", right_on=\"id\", suffixes=(\"_1\", \"_2\")\n",
    "        )\n",
    "        df_p.drop('id', axis=1, inplace=True)\n",
    "    \n",
    "    pred_test = inference_xgb(df_p, EXP_FOLDER, debug=DEBUG)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f'\\nAUC : {roc_auc_score(df_p[\"match\"], pred_test) :.4f}')\n",
    "\n",
    "    df_p['preds'] = pred_test\n",
    "    df_p[[\"id_1\", \"id_2\", \"preds\", \"match\"]].to_csv(OUT_PATH + f\"df_preds_{i}.csv\")\n",
    "    \n",
    "#     del df_p\n",
    "#     gc.collect()\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.concat([\n",
    "    pd.read_csv(OUT_PATH + f\"df_preds_{i}.csv\").set_index(\"Unnamed: 0\")\n",
    "    for i in range(len(BATCHES) - 1)], 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(df_preds['match'], df_preds['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(df.index.to_pandas().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, scores = preds_to_matches(df_preds['preds'], df_preds, threshold=THRESHOLD, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CV IoU : {compute_iou(preds, gt_matches) :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pp = post_process_matches(preds, mode=\"append\")\n",
    "\n",
    "print(f\"CV IoU : {compute_iou(preds_pp, gt_matches) :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict({k : \" \".join(v) for k, v in preds_pp.items()}, orient=\"index\")\n",
    "sub.columns = [\"matches\"]\n",
    "\n",
    "sub.to_csv(OUT_PATH + \"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
