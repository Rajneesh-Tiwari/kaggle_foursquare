{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.preparation import prepare_train_data, prepare_triplet_data\n",
    "from data.dataset import TripletDataset\n",
    "from data.tokenization import get_tokenizer\n",
    "\n",
    "from model_zoo.models import SingleTransformer\n",
    "\n",
    "from utils.logger import Config\n",
    "from utils.torch import load_model_weights\n",
    "from utils.metrics import *\n",
    "\n",
    "from inference.knn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = prepare_train_data(root=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv(DATA_PATH + f\"folds_{K}.csv\")[['id', 'fold']]\n",
    "\n",
    "df = df.merge(folds, how=\"left\", on=\"id\").set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_matches = json.load(open(DATA_PATH + \"gt.json\", 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_IDX = 0\n",
    "df_val = df[df['fold'] == FOLD_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for max_dist in [0.5]:\n",
    "    for n_neighbors in [20, 30, 40, 50]:\n",
    "        print(f'\\n -> n_neighbors={n_neighbors} - max_dist={max_dist}')\n",
    "\n",
    "        dist_matches = get_nearest_neighbors(df_val, n_neighbors=n_neighbors, max_dist=max_dist)\n",
    "        found_prop, missed = compute_found_prop(dist_matches, gt_matches)\n",
    "        n_matches = sum([len(dist_matches[k]) - 1 for k in dist_matches])\n",
    "        print(f'Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')\n",
    "\n",
    "        with open(OUT_PATH + f\"dist_matches_{n_neighbors}_{FOLD_IDX}.json\", \"w\") as f:\n",
    "            json.dump(dist_matches, f)\n",
    "            print(\"\\n- Saved to \", OUT_PATH + f\"dist_matches_{n_neighbors}_{FOLD_IDX}.json\")\n",
    "\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_neighbors = 100\n",
    "max_dist = 0.5\n",
    "lim = 20  # ??\n",
    "\n",
    "all_fps = {}\n",
    "for fold_idx in tqdm(range(K)):\n",
    "    df_val = df[df['fold'] == fold_idx]\n",
    "    dist_matches = get_nearest_neighbors(df_val, n_neighbors=n_neighbors)\n",
    "    fps = {id_ : \" \".join(list(set(dist_matches[id_]) - set(gt_matches[id_]))[:lim]) for id_ in dist_matches}\n",
    "    all_fps.update(fps)  # TODO : update ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fps = pd.DataFrame.from_dict(all_fps, orient=\"index\").reset_index()\n",
    "df_fps.columns = [\"id\", 'fp_ids']\n",
    "\n",
    "triplets = prepare_triplet_data(root=DATA_PATH)\n",
    "triplets.drop('fp_ids', axis=1, inplace=True)\n",
    "\n",
    "triplets = triplets.merge(df_fps, how=\"left\")\n",
    "triplets.to_csv(DATA_PATH + 'triplets_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets.isna().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phone number\n",
    "- Count encoding for phone numbers present too many times ?\n",
    "- Phone found twice = match ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_IDX = 0\n",
    "df_val = df[df['fold'] == FOLD_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x=df_val['phone'].apply(len))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phone = df_val[df_val['phone'].apply(len) > 0]\n",
    "df_phone = df_phone[df_phone['phone'].apply(len) > 5]\n",
    "df_phone = df_phone[df_phone['phone'].apply(len) < 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phone_matches(id_, df):\n",
    "    number = df['phone'][id_]\n",
    "\n",
    "#     matches = list(df[df['phone'] == number].index)\n",
    "    matches = list(df[df['phone'].apply(lambda x: x in number or number in x)].index)\n",
    "    matches.remove(id_)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_matches = {}\n",
    "\n",
    "for country, df_phone_c in tqdm(df_phone.groupby(\"country\")):\n",
    "#     print(country, len(df_phone_c))\n",
    "    if country == \"US\":\n",
    "        # Group by state\n",
    "        for state, df_phone_c_s in tqdm(df_phone_c.groupby(\"state\")):\n",
    "            for id_ in df_phone_c_s.index:\n",
    "                m = find_phone_matches(id_, df_phone_c_s)\n",
    "                if len(m):\n",
    "                    phone_matches[id_] = m\n",
    "    else:\n",
    "        for id_ in df_phone_c.index:\n",
    "            m = find_phone_matches(id_, df_phone_c)\n",
    "            if len(m):\n",
    "                phone_matches[id_] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    k = list(phone_matches.keys())[i]\n",
    "    k = np.random.choice(list(phone_matches.keys()))\n",
    "    display(df.loc[[k] + phone_matches[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [20, 30, 40, 50]:\n",
    "    print(f'\\n -> n_neighbors={n_neighbors}\\n')\n",
    "\n",
    "    position_matches = json.load(open(OUT_PATH + f\"dist_matches_{n_neighbors}_0.json\", 'r'))\n",
    "    found_prop, missed_pos = compute_found_prop(position_matches, gt_matches)\n",
    "    n_matches = sum([len(position_matches[k]) - 1 for k in position_matches])\n",
    "    print(f'Position :\\t Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')\n",
    "\n",
    "    merged_matches = {k : list(set(position_matches[k] + phone_matches.get(k, []))) for k in preds_matches}\n",
    "    found_prop, missed = compute_found_prop(merged_matches, gt_matches)\n",
    "    n_matches = sum([len(merged_matches[k]) - 1 for k in merged_matches])\n",
    "    print(f'Merged :\\t found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Url\n",
    "- Count encoding for phone numbers present too many times ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_IDX = 0\n",
    "df_val = df[df['fold'] == FOLD_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x=df_val['url'].apply(len))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_url = df_val[df_val['url'].apply(len) > 0]\n",
    "df_url = df_url[df_url['url'].apply(len) > 20]\n",
    "len(df_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url_matches(id_, df):\n",
    "    url = df['url'][id_]\n",
    "\n",
    "#     matches = list(df[df['url'] == url].index)\n",
    "    matches = list(df[df['url'].apply(lambda x: x in url or url in x)].index)\n",
    "    matches.remove(id_)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_matches = {}\n",
    "\n",
    "for country, df_url_c in tqdm(df_url.groupby(\"country\")):\n",
    "    if country == \"US\":\n",
    "        # Group by state\n",
    "        for state, df_url_c_s in tqdm(df_url_c.groupby(\"state\")):\n",
    "            for id_ in df_url_c_s.index:\n",
    "                m = find_url_matches(id_, df_url_c_s)\n",
    "                if len(m):\n",
    "                    url_matches[id_] = m\n",
    "    else:\n",
    "        for id_ in df_url_c.index:\n",
    "            m = find_url_matches(id_, df_url_c)\n",
    "            if len(m):\n",
    "                url_matches[id_] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    k = list(url_matches.keys())[i]\n",
    "    k = np.random.choice(list(url_matches.keys()))\n",
    "    display(df.loc[[k] + url_matches[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_neighbors in [20, 30, 40, 50]:\n",
    "    print(f'\\n -> n_neighbors={n_neighbors}\\n')\n",
    "\n",
    "    position_matches = json.load(open(OUT_PATH + f\"dist_matches_{n_neighbors}_0.json\", 'r'))\n",
    "    found_prop, missed_pos = compute_found_prop(position_matches, gt_matches)\n",
    "    n_matches = sum([len(position_matches[k]) - 1 for k in position_matches])\n",
    "    print(f'Position :\\t Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')\n",
    "\n",
    "    merged_matches = {k : list(set(position_matches[k] + phone_matches.get(k, []))) for k in preds_matches}\n",
    "    found_prop, missed = compute_found_prop(merged_matches, gt_matches)\n",
    "    n_matches = sum([len(merged_matches[k]) - 1 for k in merged_matches])\n",
    "    print(f'Pos+Phone :\\t found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')\n",
    "\n",
    "    merged_matches = {k : list(set(\n",
    "        position_matches[k] + url_matches.get(k, []) + phone_matches.get(k, [])\n",
    "    )) for k in preds_matches}\n",
    "\n",
    "    found_prop, missed = compute_found_prop(merged_matches, gt_matches)\n",
    "    n_matches = sum([len(merged_matches[k]) - 1 for k in merged_matches])\n",
    "    print(f'Pos+Phone+Url :\\t found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')\n",
    "    \n",
    "    with open(OUT_PATH + f\"dist-phone-url_matches_{n_neighbors}_{FOLD_IDX}.json\", \"w\") as f:\n",
    "        json.dump(merged_matches, f)\n",
    "        print(\"\\n- Saved to \", OUT_PATH + f\"dist-phone-url_matches_{n_neighbors}_{FOLD_IDX}.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
