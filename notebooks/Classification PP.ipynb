{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to train classification models\n",
    "**TODO**:\n",
    "- verif dist fts\n",
    "- GroupKFold ?\n",
    "- inspect cat features\n",
    "- catboost + xgboost + lgbm\n",
    "- NaNs ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import cudf\n",
    "import lofo\n",
    "import torch\n",
    "import pickle\n",
    "import optuna\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pandarallel.initialize(progress_bar=False, use_memory_fs=False)\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 200\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pp import *\n",
    "from dtypes import *\n",
    "from params import *\n",
    "from utils.metrics import *\n",
    "from inference.main import k_fold_inf\n",
    "from data.preparation import reduce_mem_usage\n",
    "from matching import get_CV, load_cleaned_data, cci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "THRESHOLD = 0.0075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + \"train.csv\")[[\"id\", \"point_of_interest\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_p = pd.read_csv(OUT_PATH + f\"features_train_{LEVEL}.csv\")\n",
    "\n",
    "df_p = pd.read_csv(OUT_PATH + f\"features_train_{LEVEL}_{THRESHOLD}.csv\", dtype=DTYPES_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_p.sort_values(['id_1', 'id_2']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-28/0/\"  # lgb 10kf - 0.8982\n",
    "\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-30/5/\"  # lgb gkf - 0.8837\n",
    "# EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-30/6/\"  # xgb gkf - 0.8782\n",
    "\n",
    "EXP_FOLDERS = [\n",
    "    LOG_PATH + \"lvl_2/\" + \"2022-06-30/5/\",  # lgb gkf - 0.8837\n",
    "    LOG_PATH + \"lvl_2/\" + \"2022-07-01/2/\",   # xgb gkf - 0.8818\n",
    "]\n",
    "\n",
    "# EXP_FOLDERS = [\n",
    "#     LOG_PATH + \"lvl_2/\" + \"2022-07-01/1/\",  # lgb 10kf -\n",
    "#     LOG_PATH + \"lvl_2/\" + \"2022-07-01/0/\",  # xgb 10kf -\n",
    "# ]\n",
    "\n",
    "WEIGHTS = [0.75, 0.25]\n",
    "# WEIGHTS = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_oof = np.load(EXP_FOLDER + \"pred_oof.npy\")\n",
    "\n",
    "pred_oof_ = np.average([\n",
    "    np.load(f + \"pred_oof.npy\") for f in EXP_FOLDERS\n",
    "], weights=WEIGHTS, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k_fold_inf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cf7236861dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m preds_lgbm = k_fold_inf(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdf_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mEXP_FOLDERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k_fold_inf' is not defined"
     ]
    }
   ],
   "source": [
    "preds_lgbm = k_fold_inf(\n",
    "    df_p,\n",
    "    EXP_FOLDERS[0],\n",
    ")\n",
    "\n",
    "preds_xgb = k_fold_inf(\n",
    "    df_p,\n",
    "    EXP_FOLDERS[1],\n",
    ")\n",
    "\n",
    "pred_oof = WEIGHTS[0] * preds_lgbm + WEIGHTS[1] * preds_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9508260754268621"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_p['match'], pred_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507679122799162"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_p['match'], pred_oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict to a subset comparable to LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET = [0, 1, 2]\n",
    "\n",
    "# df = pd.read_csv(f'../output/folds_5.csv')\n",
    "# df = df[df['fold'].isin(SUBSET)]\n",
    "\n",
    "# pred_oof = pred_oof[(df_p['fold_1'].isin(SUBSET)) & (df_p['fold_2'].isin(SUBSET))]\n",
    "\n",
    "# df_p = df_p[(df_p['fold_1'].isin(SUBSET)) & (df_p['fold_2'].isin(SUBSET))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9508480425294981"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_p['match'], pred_oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add m_true column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"m_true\" not in df.columns:\n",
    "    df = df.reset_index()\n",
    "    df = df.sort_values(by=[\"point_of_interest\", \"id\"]).reset_index(drop=True)\n",
    "\n",
    "    id_all = np.array(df[\"id\"])\n",
    "    poi_all = np.array(df[\"point_of_interest\"])\n",
    "    poi0 = poi_all[0]\n",
    "    id0 = id_all[0]\n",
    "\n",
    "    di_poi = {}\n",
    "    for i in range(1, df.shape[0]):\n",
    "        if poi_all[i] == poi0:\n",
    "            id0 = str(id0) + \" \" + str(id_all[i])\n",
    "        else:\n",
    "            di_poi[poi0] = str(id0) + \" \"  # need to have trailing space in m_true\n",
    "            poi0 = poi_all[i]\n",
    "            id0 = id_all[i]\n",
    "\n",
    "    di_poi[poi0] = str(id0) + \" \"  # need to have trailing space in m_true\n",
    "    df[\"m_true\"] = df[\"point_of_interest\"].map(di_poi)\n",
    "\n",
    "    df = df.sort_values(by=\"index\").reset_index(\n",
    "        drop=True\n",
    "    )  # sort back to original order\n",
    "    df.drop(\"index\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLDS = [0.45, 0.6, 0.6, 0.9, 0.]  # FULL DATA\n",
    "\n",
    "# THRESHOLDS = [0.3, 0.45, 0.45, 0.8, 0]  # RESTRICTED  - cuts = [0.37, 0.48, 0.43, 0.77]\n",
    "# THRESHOLDS = [0.5, 0.5, 0.5, 0.8]\n",
    "\n",
    "# THRESHOLDS = [0.37, 0.48, 0.43, 0.77, 0.]  # LB / RESTRICTED\n",
    "\n",
    "threshold, threshold_small, threshold_big, threshold_merge_max, threshold_merge_avg = THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0.8823\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-013c1e6a4d7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid_to_poi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoi_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoi_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_improved_CV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_oof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTHRESHOLDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "id_to_poi, poi_to_id, preds = get_improved_CV(df_p, pred_oof, df, THRESHOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0.8843\n"
     ]
    }
   ],
   "source": [
    "id_to_poi, poi_to_id, poi_counts, preds, cv = get_improved_CV(df_p, pred_oof, df, THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_p.copy()\n",
    "df2[\"match\"] = np.copy(pred_oof) #.astype(\"float32\")\n",
    "\n",
    "try:\n",
    "    df2 = df2[['id_1', 'id_2', \"match\"]]\n",
    "    df2.columns = ['id', 'id2', \"match\"]\n",
    "except KeyError:\n",
    "    df2 = df2[['id', 'id2', \"match\"]]\n",
    "\n",
    "# sort by decr prediction\n",
    "df2 = df2.sort_values(by=[\"match\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct POI from pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_poi, poi_to_id, poi_counts = match_pois(\n",
    "    df2,\n",
    "    threshold=threshold,\n",
    "    threshold_small=threshold_small,\n",
    "    threshold_big=threshold_big\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_poi, poi_to_id, poi_counts = merge_pois_simple(\n",
    "    df2,\n",
    "    id_to_poi,\n",
    "    poi_to_id,\n",
    "    poi_counts,\n",
    "    threshold=threshold,\n",
    "    threshold_merge=threshold_merge_max,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_poi, poi_to_id, poi_counts = merge_pois_advanced(\n",
    "#     df2,\n",
    "#     id_to_poi,\n",
    "#     poi_to_id,\n",
    "#     poi_counts,\n",
    "#     threshold_merge_avg=threshold_merge_avg,\n",
    "#     threshold_merge_max=threshold_merge_max\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_poi_svg = deepcopy(id_to_poi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers\n",
    "- TODO : add lvl 1 as nomatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_poi = deepcopy(id_to_poi_svg)\n",
    "\n",
    "poi_to_id = get_poi_to_id(id_to_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THRESHOLD_OUT = 0.3\n",
    "THRESHOLD_OUT = 0.25  # sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ccd63eb70447b8a8578580ee7ebc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=185040.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "id_to_poi, poi_to_id = remove_outliers(df2, id_to_poi, poi_to_id, threshold_out=THRESHOLD_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(removed)  # TODO : retrieve matches for removed ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poi_to_id = {}\n",
    "# for k in id_to_poi.keys():\n",
    "#     if id_to_poi[k] not in poi_to_id:\n",
    "#         poi_to_id[id_to_poi[k]] = []\n",
    "#     poi_to_id[id_to_poi[k]].append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame.from_dict(id_to_poi, orient=\"index\").reset_index()\n",
    "preds['matches'] = preds[0].map(poi_to_id).apply(lambda x: \" \".join(x))\n",
    "preds.columns = [\"id\", \"poi\", \"m2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0.90380\n"
     ]
    }
   ],
   "source": [
    "cv = evaluate(df, preds)\n",
    "print(f\"CV {cv:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0.90410\n"
     ]
    }
   ],
   "source": [
    "cv = evaluate(df, preds)\n",
    "print(f\"CV {cv:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- base : 0.90228\n",
    "- theo : 0.90312\n",
    "- theo - outlier : 0.90405"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- base : 0.9192\n",
    "- theo : 0.91975\n",
    "- theo - outlier : 0.9205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_p.copy()\n",
    "df2[\"match\"] = np.copy(pred_oof) #.astype(\"float32\")\n",
    "\n",
    "try:\n",
    "    df2 = df2[['id_1', 'id_2', \"match\"]]\n",
    "    df2.columns = ['id', 'id2', \"match\"]\n",
    "except KeyError:\n",
    "    df2 = df2[['id', 'id2', \"match\"]]\n",
    "\n",
    "# sort by decr prediction\n",
    "df2 = df2.sort_values(by=[\"match\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THS = [0.45, 0.6, 0.6, 0.9]\n",
    "THS = [0.5, 0.6, 0.6, 0.8, 0]\n",
    "\n",
    "THS = [0.2, 0.2, 0.2, 0.2, 0]\n",
    "\n",
    "# THS = [0.37, 0.48, 0.43, 0.77, 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0.8624\n"
     ]
    }
   ],
   "source": [
    "id_to_poi, poi_to_id, poi_counts, preds, cv = get_improved_CV(df_p, pred_oof, df, THS, max_size=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct all possible pairs for each poi\n",
    "p1 = []\n",
    "p2 = []\n",
    "\n",
    "for poi in poi_to_id.keys():\n",
    "    ids = poi_to_id[poi]\n",
    "    ids.sort()\n",
    "    step = max(1, int(0.5 + len(ids) ** 2 / 2 / 2000)) # limit to approx 2000 per poi\n",
    "    for i in range(len(ids) - 1):\n",
    "        for j in range(i + 1, len(ids), step):\n",
    "            p1.append(ids[i])\n",
    "            p2.append(ids[j])\n",
    "\n",
    "p1 = pd.DataFrame(p1)\n",
    "p1.columns = ['id']\n",
    "p2 = pd.DataFrame(p2)\n",
    "p2.columns = ['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1['id'], p2['id']], axis=1)\n",
    "p12.columns = ['id','id2']\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12['id'] > p12['id2']\n",
    "p12['t'] = p12['id']\n",
    "p12['id'].loc[idx] = p12['id2'].loc[idx]\n",
    "p12['id2'].loc[idx] = p12['t'].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=['id', 'id2']).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=['id', 'id2'])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12['id'] != p12['id2']]\n",
    "p1 = p1.loc[p12['index']].reset_index(drop=True)\n",
    "p2 = p2.loc[p12['index']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only new pairs\n",
    "new_pairs = pd.concat([p1, p2], 1)\n",
    "new_pairs.columns = [\"id\", \"id2\"]\n",
    "\n",
    "new_pairs = new_pairs.merge(df2, how=\"left\")\n",
    "new_pairs = new_pairs[new_pairs[\"match\"].isna()].drop(\"match\", axis=1).reset_index()\n",
    "\n",
    "p1 = new_pairs[[\"id\"]]\n",
    "p2 = new_pairs[[\"id2\"]]\n",
    "p2.columns = ['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 340292 pairs\n"
     ]
    }
   ],
   "source": [
    "print(f\"Added {len(p1)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add other columns needed for FE\n",
    "cols = [\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"address\",\n",
    "    \"country\",\n",
    "    \"url\",\n",
    "    \"phone\",\n",
    "    \"city\",\n",
    "    \"categories\",\n",
    "    \"category_simpl\",\n",
    "    \"categories_split\",\n",
    "    \"cat2\",\n",
    "    \"idx\",\n",
    "    \"state\",\n",
    "    \"zip\",\n",
    "]\n",
    "train = load_cleaned_data(OUT_PATH + \"cleaned_data_train.csv\")\n",
    "train[\"idx\"] = np.arange(len(train))\n",
    "\n",
    "p1 = p1[[\"id\"]].merge(train[cols], on=\"id\", how=\"left\")\n",
    "p2 = p2[[\"id\"]].merge(train[cols], on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fe_main import feature_engineering_1, feature_engineering_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Distances\n",
      "- Features for column : name\n",
      "- Features for column : categories\n",
      "- Features for column : address\n",
      "- Nan features\n",
      "- Matching\n",
      "- Category match\n",
      "- Ratios\n",
      "- Count encodings\n",
      "- Computing position distances\n",
      "- Computing feature same_state\n",
      "- Computing feature same_zip\n",
      "- Computing feature same_city\n",
      "- Column : name  -  Function : levenshtein\n",
      "- Column : address  -  Function : levenshtein\n",
      "- Column : url  -  Function : levenshtein\n"
     ]
    }
   ],
   "source": [
    "df_p3 = feature_engineering_1(p1, p2, train.copy(), ressources_path=RESSOURCES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Cat links & quantiles\n",
      "- Simply connected components\n",
      "- Strongly connected components\n",
      "- Cat link score\n",
      "- Link between grouped categories\n",
      "- Features for column name_initial\n",
      "- Features for column name_initial_decode\n",
      "- Features for column nameC\n",
      "- Features for column name\n",
      "- Features for column categories\n",
      "- Features for column address\n",
      "- Features for column url\n",
      "- Features for column city\n",
      "- Features for column state\n",
      "- Features for column zip\n",
      "- Features for column phone\n",
      "- Count encoding\n",
      "- Words in categories\n",
      "- Words in names\n",
      "- Close count\n",
      "- Close count of same category2\n",
      "- Close count of same category_simpl\n",
      "- Compare numeric part of the name/address\n",
      "- Number in names features\n",
      "- Computing nan features\n",
      "- Computing feature name_tf_idf_33_char_wb_sim\n",
      "- Computing feature address_tf_idf_33_char_wb_sim\n",
      "- Computing feature url_tf_idf_33_char_wb_sim\n",
      "- Column : name  -  Function : wratio\n",
      "- Column : name  -  Function : partial_ratio\n",
      "- Column : address  -  Function : wratio\n",
      "- Column : address  -  Function : partial_ratio\n",
      "- Column : url  -  Function : wratio\n",
      "- Column : url  -  Function : partial_ratio\n"
     ]
    }
   ],
   "source": [
    "df_p3 = feature_engineering_2(df_p3, train.copy(), ressources_path=RESSOURCES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "\n",
    "path = f'../output/folds_{N_FOLDS}.csv'\n",
    "df_split = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"fold_1\" not in df_p3.columns:\n",
    "    df_p3 = df_p3.merge(df_split, left_on=\"id_1\", right_on=\"id\", how=\"left\").drop(\"id\", axis=1)\n",
    "    df_p3 = df_p3.merge(df_split, left_on=\"id_2\", right_on=\"id\", how=\"left\", suffixes=('_1', '_2')).drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"match\" not in df_p3.columns:\n",
    "    df_p3['match'] = (df_p3['point_of_interest_1'] == df_p3['point_of_interest_2']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------   Fold 1 / 5  -------------\n",
      "\n",
      "- Scoring 172276 pairs\n",
      "\n",
      "[W] [10:56:43.196391] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [10:56:43.693205] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "- AUC = 0.9237\n",
      "\n",
      "-------------   Fold 2 / 5  -------------\n",
      "\n",
      "- Scoring 163185 pairs\n",
      "\n",
      "[W] [10:56:44.225203] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [10:56:44.644870] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "- AUC = 0.9119\n",
      "\n",
      "-------------   Fold 3 / 5  -------------\n",
      "\n",
      "- Scoring 174615 pairs\n",
      "\n",
      "[W] [10:56:45.052487] Treelite currently does not support float64 model parameters. Accuracy may degrade slightly relative to native LightGBM invocation.\n",
      "[W] [10:56:45.439983] Casting all thresholds and leaf values to float32, as FIL currently doesn't support inferencing models with float64 values. This may lead to predictions with reduced accuracy.\n",
      "- AUC = 0.9185\n",
      "\n",
      "-------------   Fold 4 / 5  -------------\n",
      "\n",
      "- Scoring 0 pairs\n",
      "\n",
      "\n",
      "-------------   Fold 5 / 5  -------------\n",
      "\n",
      "- Scoring 0 pairs\n",
      "\n",
      "\n",
      " -> CV AUC = 0.9175\n",
      "\n",
      "\n",
      "-------------   Fold 1 / 5  -------------\n",
      "\n",
      "- Scoring 172276 pairs\n",
      "\n",
      "- AUC = 0.9168\n",
      "\n",
      "-------------   Fold 2 / 5  -------------\n",
      "\n",
      "- Scoring 163185 pairs\n",
      "\n",
      "- AUC = 0.9108\n",
      "\n",
      "-------------   Fold 3 / 5  -------------\n",
      "\n",
      "- Scoring 174615 pairs\n",
      "\n",
      "- AUC = 0.9137\n",
      "\n",
      "-------------   Fold 4 / 5  -------------\n",
      "\n",
      "- Scoring 0 pairs\n",
      "\n",
      "\n",
      "-------------   Fold 5 / 5  -------------\n",
      "\n",
      "- Scoring 0 pairs\n",
      "\n",
      "\n",
      " -> CV AUC = 0.9084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_lgbm = k_fold_inf(\n",
    "    df_p3,\n",
    "    EXP_FOLDERS[0],\n",
    ")\n",
    "\n",
    "preds_xgb = k_fold_inf(\n",
    "    df_p3,\n",
    "    EXP_FOLDERS[1],\n",
    ")\n",
    "\n",
    "preds_2 = WEIGHTS[0] * preds_lgbm + WEIGHTS[1] * preds_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df_p3[[\"id_1\", \"id_2\"]].copy()\n",
    "df3[\"match\"] = preds_2\n",
    "\n",
    "df3.columns = [\"id\", \"id2\", \"match\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final PP\n",
    "- Could work with my approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df2, df3], 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLDS = [0.5, 0.6, 0.6, 0.8, 0]\n",
    "# THRESHOLDS = [0.45, 0.6, 0.6, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLDS = [0.37, 0.48, 0.43, 0.77, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0.9007\n"
     ]
    }
   ],
   "source": [
    "_ = get_improved_CV(df_all, df_all[\"match\"], df, THRESHOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0.9041\n"
     ]
    }
   ],
   "source": [
    "_ = get_improved_CV(df2, df2[\"match\"], df, THRESHOLDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
