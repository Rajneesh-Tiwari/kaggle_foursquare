{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to train classification models\n",
    "**TODO**:\n",
    "- Missing values ?\n",
    "- Switch to fp32 / fp16 if memory issues\n",
    "- Categorical features\n",
    "- Use it to filter out obvious non-matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import cudf\n",
    "import pylcs\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "pandarallel.initialize(progress_bar=False, use_memory_fs=False)\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.preparation import prepare_train_data, prepare_triplet_data\n",
    "\n",
    "from utils.logger import prepare_log_folder, create_logger, save_config\n",
    "from utils.metrics import *\n",
    "from inference.knn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = LOG_PATH + \"2022-05-19/4/\"  # 1 ep, d=256, large\n",
    "\n",
    "N_NEIGHBORS = 50\n",
    "\n",
    "FOLD = 0\n",
    "\n",
    "PRECOMPUTED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    df = prepare_train_data(root=DATA_PATH)\n",
    "#     df = cudf.from_pandas(df)\n",
    "\n",
    "    folds = pd.read_csv(DATA_PATH + \"folds_2.csv\")[['id', 'fold']]\n",
    "    df = df.merge(folds, how=\"left\", on=\"id\").set_index(\"id\")\n",
    "\n",
    "    df = df[df['fold'] == FOLD]\n",
    "    df['idx'] = np.array(range(len(df)))\n",
    "    \n",
    "    df = cudf.from_pandas(df)\n",
    "    \n",
    "    df_p = cudf.read_csv(EXP_FOLDER + f'df_pairs_{N_NEIGHBORS}.csv')\n",
    "\n",
    "    df_p = df_p.merge(df, how=\"left\", left_on=\"id_1\", right_on=\"id\")\n",
    "    df_p = df_p.merge(df, how=\"left\", left_on=\"id_2\", right_on=\"id\", suffixes=(\"_1\", \"_2\"))\n",
    "    \n",
    "    nn_preds = np.load(EXP_FOLDER + f\"fts_val_{FOLD}.npy\").astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_matches = json.load(open(DATA_PATH + \"gt.json\", 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "- Add features based on model preds\n",
    "- Categories clusters using pois (leaky ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylcs\n",
    "import difflib\n",
    "import Levenshtein\n",
    "\n",
    "def compute_lcs(a, b):\n",
    "    return pylcs.lcs(a, b)\n",
    "\n",
    "def compute_gesh(a, b):\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def compute_levenshtein(a, b):\n",
    "    return Levenshtein.distance(a, b)\n",
    "\n",
    "def compute_jaro(a, b):\n",
    "    return Levenshtein.jaro_winkler(a, b)\n",
    "\n",
    "def compute_levenshtein_n(a, b):\n",
    "    return Levenshtein.distance(a, b) / max(len(a), len(b))\n",
    "\n",
    "def compute_string_distance(fct, a, b):\n",
    "    if a != \"\" and b != \"\":\n",
    "        return fct(a.lower(), b.lower())\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(ft, a, b):\n",
    "    if a != \"\" and b != \"\":\n",
    "        return ft / max(len(a), len(b))\n",
    "    else:\n",
    "        return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lats1, lats2, longs1, longs2):\n",
    "    dlat = np.radians(lats2 - lats1)\n",
    "    dlon = np.radians(longs2 - longs1)\n",
    "\n",
    "    a = (\n",
    "        np.sin(dlat / 2) ** 2 + \n",
    "        np.cos(np.radians(lats1)) * np.cos(np.radians(lats2)) * np.sin(dlon / 2) ** 2\n",
    "    )\n",
    "    dist = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    return 6371 * dist\n",
    "\n",
    "\n",
    "def manhattan_distance(lat1, long1, lat2, long2):\n",
    "    return np.abs(lat2 - lat1) + np.abs(long2 - long1)\n",
    "\n",
    "\n",
    "def euclidian_distance(lat1, long1, lat2, long2):\n",
    "    return np.sqrt((lat2 - lat1) ** 2 + (long2 - long1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_included(a, b):\n",
    "    if a == \"\" or b == \"\":\n",
    "        return np.nan\n",
    "    else:\n",
    "        return a.lower() in b.lower() or b.lower() in a.lower()\n",
    "\n",
    "def is_equal(a, b):\n",
    "    if a == \"\" or b == \"\":\n",
    "        return np.nan\n",
    "    else:\n",
    "        return a.lower() == b.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_similarity(pairs, matrix):\n",
    "    i1s = pairs['idx_1'].values.tolist()\n",
    "    i2s = pairs['idx_2'].values.tolist()    \n",
    "    \n",
    "    sims = matrix[i1s].multiply(matrix[i2s]).sum(axis=1).ravel()\n",
    "    return sims.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_distance(pairs, matrix):\n",
    "    i1s = pairs['idx_1'].values.tolist()\n",
    "    i2s = pairs['idx_2'].values.tolist()\n",
    "\n",
    "    return ((matrix[i1s] - matrix[i2s]) ** 2).mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_dist_fcts = {\n",
    "    \"lcs\": compute_lcs,\n",
    "    \"gesh\": compute_gesh,\n",
    "    \"levenshtein\": compute_levenshtein,\n",
    "    \"jaro\": compute_jaro,\n",
    "}\n",
    "\n",
    "TO_NORMALIZE = [\"lcs\", \"levenshtein\"]\n",
    "\n",
    "TF_IDF_COLS = ['name', 'categories', 'address', 'url']\n",
    "STRING_DIST_COLS = ['name', \"categories\", 'address', 'url', ]\n",
    "\n",
    "FEATURES_SAME = [\n",
    "    ('country', is_equal),\n",
    "    ('state', is_equal),\n",
    "    ('zip', is_included),\n",
    "    ('phone', is_included),\n",
    "    ('city', is_included),\n",
    "    ('categories', is_included),\n",
    "]\n",
    "\n",
    "FEATURES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    df_p['nn_dist'] = nn_distance(df_p, nn_preds)\n",
    "    FEATURES.append('nn_dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "if not PRECOMPUTED:\n",
    "    tf_idf_mats = {}\n",
    "\n",
    "    for col in TF_IDF_COLS:\n",
    "        tf_idf = TfidfVectorizer()\n",
    "        tf_idf_mat = tf_idf.fit_transform(df[col].fillna('noname'))\n",
    "        tf_idf_mats[col] = tf_idf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    for col in TF_IDF_COLS:\n",
    "        print(f'Computing feature {col}_tf_idf_sim')\n",
    "        df_p[f\"{col}_tf_idf_sim\"] = tf_idf_similarity(df_p, tf_idf_mats[col])\n",
    "        FEATURES.append(f\"{col}_tf_idf_sim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion / equality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    try:\n",
    "        df_p = df_p.to_pandas()\n",
    "        df_p = reduce_mem_usage(df_p)\n",
    "    except AttributeError:\n",
    "        print('df_p already in pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    for col, fct in FEATURES_SAME:\n",
    "        print(f'Computing feature same_{col}')\n",
    "        df_p[f\"same_{col}\"] = df_p[[f\"{col}_1\", f\"{col}_2\"]].parallel_apply(\n",
    "            lambda x: fct(x[0], x[1]), axis=1\n",
    "        ).astype(float)\n",
    "\n",
    "        FEATURES.append(f\"same_{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    for col in tqdm(STRING_DIST_COLS):\n",
    "        for fct_name in string_dist_fcts:\n",
    "            print(f\"Column : {col}  -  Function : {fct_name}\")\n",
    "            df_p[col + \"_\" + fct_name] = df_p[[col + \"_1\", col + \"_2\"]].parallel_apply(\n",
    "                lambda x: compute_string_distance(string_dist_fcts[fct_name], x[0], x[1]), axis=1\n",
    "            )\n",
    "            FEATURES.append(col + \"_\" + fct_name)\n",
    "\n",
    "            if fct_name in TO_NORMALIZE:\n",
    "                df_p[col + \"_\" + fct_name + \"_n\"] = df_p[\n",
    "                    [col + \"_\" + fct_name, col + \"_1\", col + \"_2\"]\n",
    "                ].parallel_apply(\n",
    "                    lambda x: normalize(x[0], x[1], x[2]), axis=1\n",
    "                )\n",
    "                FEATURES.append(col + \"_\" + fct_name + \"_n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    lats_1, longs_1, lats_2, longs_2 = np.hsplit(\n",
    "        df_p[['latitude_1', 'longitude_1', 'latitude_2', 'longitude_2']].values, 4\n",
    "    )\n",
    "\n",
    "    df_p['longitude_diff'] = np.abs(longs_2 - longs_1)\n",
    "    df_p['latitude_diff'] = np.abs(lats_2 - lats_1)\n",
    "    df_p['haversine_distance'] = haversine_distance(lats_1, longs_1, lats_2, longs_2)\n",
    "    df_p['manhattan_distance'] = manhattan_distance(lats_1, longs_1, lats_2, longs_2)\n",
    "\n",
    "    df_p['euclidian_distance'] = euclidian_distance(lats_1, longs_1, lats_2, longs_2)\n",
    "    df_p['euclidian_distance'] = np.clip(df_p['euclidian_distance'], 0, 10000)\n",
    "\n",
    "    FEATURES += [\n",
    "        'longitude_diff', 'latitude_diff', 'haversine_distance', 'manhattan_distance', 'euclidian_distance'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_KEEP = ['id_1', 'id_2', 'point_of_interest_1', 'point_of_interest_2', 'match'] + FEATURES\n",
    "\n",
    "if not PRECOMPUTED:\n",
    "    df_p.drop([c for c in df_p.columns if c not in TO_KEEP], axis=1, inplace=True)\n",
    "    # df_p.to_csv(EXP_FOLDER + f'df_p_{N_NEIGHBORS}.csv', index=False)\n",
    "else:\n",
    "    df_p = cudf.read_csv(EXP_FOLDER + f'df_p_{N_NEIGHBORS}.csv')\n",
    "    FEATURES = [col for col in df_p.columns[5:] if \"fold\" not in col]\n",
    "    \n",
    "df_p['match'] = df_p['match'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds\n",
    "\n",
    "TODO : non-leaky splits by considering pairs: \n",
    "- gkf on poi, val set considers left and right pois -> (x1, x2) if x1 is in fold 1 val, x2 can be seen during training but not with any element of same poi as x1. This should not be leaky ?\n",
    "- split before looking for pairs ? No bc it's important to have a 600k set to look for pairs in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "if \"fold_1\" not in df_p.columns:\n",
    "    if not os.path.exists(DATA_PATH + f\"folds_{FOLD}_{N_SPLITS}.csv\"):\n",
    "        gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "        splits = gkf.split(df, groups=df['point_of_interest'])\n",
    "\n",
    "        df_split = df.reset_index()[['id', 'point_of_interest']]\n",
    "        df_split['fold'] = -1\n",
    "\n",
    "        for i, (_, val_idx) in enumerate(splits):\n",
    "            df_split.loc[val_idx, 'fold'] = i\n",
    "\n",
    "        df_split.to_csv(DATA_PATH + f\"folds_{FOLD}_{N_SPLITS}.csv\", index=False)\n",
    "        \n",
    "    df_split = pd.read_csv(DATA_PATH + f\"folds_{FOLD}_{N_SPLITS}.csv\")\n",
    "\n",
    "#     df_split = cudf.read_csv(DATA_PATH + f\"folds_{FOLD}_{N_SPLITS}.csv\")  \n",
    "#     try:\n",
    "#         df_p = cudf.from_pandas(df_p)\n",
    "#     except TypeError:\n",
    "#         print('df_p already in pandas')\n",
    "    \n",
    "    df_p = df_p.merge(df_split[['id', 'fold']], how=\"left\", left_on=\"id_1\", right_on=\"id\")\n",
    "    df_p.drop('id', axis=1, inplace=True)\n",
    "    df_p = df_p.merge(df_split[['id', 'fold']], how=\"left\", left_on=\"id_2\", right_on=\"id\", suffixes=(\"_1\", \"_2\"))\n",
    "    df_p.drop('id', axis=1, inplace=True)\n",
    "    df_p.drop([c for c in df_p.columns if c not in TO_KEEP + ['fold_1', 'fold_2']], axis=1, inplace=True)\n",
    "\n",
    "    try:\n",
    "        df_p.to_pandas().to_csv(EXP_FOLDER + f'df_p_{N_NEIGHBORS}.csv', index=False)\n",
    "    except AttributeError:\n",
    "        df_p.to_csv(EXP_FOLDER + f'df_p_{N_NEIGHBORS}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from model_zoo.xgb import train_xgb, objective_xgb\n",
    "from utils.plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FCTS = {\n",
    "#     \"lgbm\": train_lgbm,\n",
    "    \"xgb\": train_xgb,\n",
    "#     \"xgb_rf\": train_xgbrf,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def k_fold(\n",
    "    df,\n",
    "    config,\n",
    "    log_folder=None,\n",
    "):\n",
    "    train_fct = TRAIN_FCTS[config.model]\n",
    "\n",
    "    ft_imps, models = [], []\n",
    "    pred_oof = np.zeros(len(df))\n",
    "\n",
    "    for fold in range(config.n_folds):\n",
    "        print(f\"\\n-------------   Fold {fold + 1} / {config.n_folds}  -------------\\n\")\n",
    "\n",
    "        df_train = df[(df['fold_1'] != fold) & (df['fold_2'] != fold)].reset_index(drop=True)\n",
    "        df_val = df[(df['fold_1'] == fold) | (df['fold_2'] == fold)]\n",
    "        \n",
    "        val_idx = df_val.index.values if isinstance(df, pd.DataFrame) else df_val.index.values.get()\n",
    "\n",
    "        pred_val, model = train_fct(\n",
    "            df_train, df_val, None, config.features, config.target, params=config.params\n",
    "        )\n",
    "\n",
    "        pred_oof[val_idx] = pred_val\n",
    "        ft_imp = pd.DataFrame(\n",
    "            pd.Series(model.feature_importances_, index=config.features), columns=[\"importance\"]\n",
    "        )\n",
    "\n",
    "        ft_imps.append(ft_imp)\n",
    "        models.append(model)\n",
    "\n",
    "        if log_folder is None:\n",
    "            return pred_oof, models, ft_imp\n",
    "\n",
    "        pickle.dump(model, open(log_folder + f'{config.model}_{fold}.pkl', 'wb'))\n",
    "\n",
    "    y = df[config.target].values if isinstance(df, pd.DataFrame) else df[config.target].get()\n",
    "    auc = roc_auc_score(y, pred_oof)\n",
    "    print(f\"\\n Local CV is {auc:.4f}\")\n",
    "\n",
    "    ft_imp = pd.concat(ft_imps, axis=1).mean(1)\n",
    "    ft_imp.to_csv(log_folder + f'ft_imp.csv')\n",
    "    np.save(log_folder + \"pred_oof.npy\", pred_oof)\n",
    "\n",
    "    return pred_oof, models, ft_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_p.to_pandas()\n",
    "df_p['euclidian_distance'] = np.clip(df_p['euclidian_distance'], 0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE = False\n",
    "TRAIN = False\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPT_FOLD = 2\n",
    "\n",
    "if OPTIMIZE:\n",
    "    df_train_opt = df_p[(df_p['fold_1'] != OPT_FOLD) & (df_p['fold_2'] != OPT_FOLD)].reset_index(drop=True)\n",
    "    df_val_opt = df_p[\n",
    "        (df_p['fold_1'] == OPT_FOLD) | (df_p['fold_2'] == OPT_FOLD)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    objective = lambda x: objective_xgb(x, df_train_opt, df_val_opt, FEATURES, \"match\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    print(\"Final params :\\n\", study.best_params)\n",
    "\n",
    "else:\n",
    "    params = {\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.05,\n",
    "        'min_child_weight': 1,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        \"colsample_bytree\": 0.75,\n",
    "        \"subsample\": 0.75,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    exp_folder = EXP_FOLDER\n",
    "\n",
    "    fold = FOLD\n",
    "    n_folds = 5\n",
    "    n_neighbors = N_NEIGHBORS\n",
    "    \n",
    "    features = FEATURES\n",
    "    target = \"match\"\n",
    "\n",
    "    model = \"xgb\"\n",
    "    params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    log_folder = None\n",
    "    if not DEBUG:\n",
    "        log_folder = prepare_log_folder(LOG_PATH + \"lvl_2/\")\n",
    "        print(f'Logging results to {log_folder}')\n",
    "        save_config(Config, log_folder + 'config')\n",
    "        create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "    pred_oof, models, ft_imp = k_fold(df_p, Config, log_folder=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-05-25/6/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN:\n",
    "    pred_oof = np.load(EXP_FOLDER + \"pred_oof.npy\")\n",
    "    ft_imp = pd.read_csv(EXP_FOLDER + \"ft_imp.csv\").set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     df_p = cudf.from_pandas(df_p)\n",
    "# except:\n",
    "#     print('df_p already on gpu')\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_p[Config.target].values if isinstance(df_p, pd.DataFrame) else df_p[Config.target].get()\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    pred_oof > 0.5,\n",
    "    y,\n",
    "    display_labels=['No Match', 'Match'],\n",
    "#     normalize=\"pred\"\n",
    ")\n",
    "\n",
    "plt.title(f\"AUC = {roc_auc_score(y, pred_oof) :.4f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds_to_matches(preds, df, threshold=0.5):\n",
    "    gpu = not isinstance(df_p, pd.DataFrame)\n",
    "\n",
    "    identity = df[['id_1']].drop_duplicates(keep=\"first\").copy()\n",
    "    identity['id_2'] = identity['id_1']\n",
    "    identity['pred'] = 1\n",
    "\n",
    "    df['pred'] = preds\n",
    "    df = df[df['pred'] > threshold].reset_index(drop=True)\n",
    "    df = df[['id_1', 'id_2', 'pred']].reset_index(drop=True)\n",
    "    \n",
    "    if gpu:        \n",
    "        df = cudf.concat([df, identity])\n",
    "        dfg = df.groupby('id_1').agg(list).to_pandas()\n",
    "    else:\n",
    "        df = pd.concat([df, identity])\n",
    "        dfg = df.groupby('id_1').agg(list)\n",
    "    \n",
    "    dfg['id_2'] = dfg['id_2'].apply(list)\n",
    "    dfg['pred'] = dfg['pred'].apply(list)\n",
    "    dfg = dfg.to_dict()\n",
    "    return dfg['id_2'], dfg['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds, scores = preds_to_matches(pred_oof, df_p, threshold=0.01)\n",
    "\n",
    "# preds, scores = preds_to_matches(df_p['match'].values.get(), df_p.copy(), threshold=0.5)  # Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"CV IoU : {compute_iou(preds, gt_matches) :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found_prop, missed = compute_found_prop(preds, gt_matches)\n",
    "\n",
    "n_matches = sum([len(preds[k]) for k in preds])\n",
    "\n",
    "print(f\"Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=[min(len(gt_matches[k]), 10) for k in gt_matches if k in preds.keys()])\n",
    "plt.title('Number of pred matches per id')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=[min(len(preds[k]), 10) for k in preds])\n",
    "plt.title('Number of gt matches per id')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_importances(ft_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_train_data(root=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, id_ in enumerate(preds):\n",
    "    if not len(list(missed[i])):\n",
    "        continue\n",
    "\n",
    "    print('Query')\n",
    "    display(df.loc[[id_]])\n",
    "\n",
    "    print('Target')\n",
    "    display(df.loc[[g for g in gt_matches[id_] if g != id_]])\n",
    "\n",
    "    print('Missed')\n",
    "    display(df.loc[list(missed[i])])\n",
    "\n",
    "#     print('Preds')\n",
    "#     display(df.loc[preds_matches[df.index[i]]].head(5))\n",
    "\n",
    "#     break\n",
    "    print('-' * 50)\n",
    "    \n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def limit_numbers(preds, scores, n=2):\n",
    "    preds_pp = copy.deepcopy(preds)\n",
    "    for k in preds:\n",
    "        if len(preds[k]) > n:\n",
    "            order = np.argsort(scores[k])\n",
    "            preds_pp[k] = list(np.array(preds[k])[order[:n]])\n",
    "        \n",
    "    return preds_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pp = limit_numbers(preds, scores, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CV IoU : {compute_iou(preds_pp, gt_matches) :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def post_process_matches(matches, mode=\"append\"):\n",
    "    new_matches = copy.deepcopy(matches)\n",
    "    for k in matches:\n",
    "        for m in matches[k]:\n",
    "            if k not in new_matches[m]:\n",
    "                if mode == \"remove\":\n",
    "                    new_matches[k].remove(m)\n",
    "                elif mode == \"append\":\n",
    "                    new_matches[m].append(k)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "    return new_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pp = post_process_matches(preds, mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CV IoU : {compute_iou(preds_pp, gt_matches) :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found_prop, missed = compute_found_prop(preds_pp, gt_matches)\n",
    "\n",
    "n_matches = sum([len(preds_pp[k]) for k in preds_pp])\n",
    "\n",
    "print(f\"Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
