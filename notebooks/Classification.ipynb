{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to train classification models\n",
    "**TODO**:\n",
    "- Missing values ?\n",
    "- Categorical features\n",
    "- Use it to filter out obvious non-matches\n",
    "- geopy for missing address\n",
    "\n",
    "- better retrieval of TPs using lgbm model\n",
    "- https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\n",
    "- catboost, lgbm\n",
    "- Voting & thresh tuning ? https://www.kaggle.com/code/nlztrk/public-0-861-pykakasi-radian-coordinates/notebook\n",
    "- Fts & nan from https://www.kaggle.com/code/theoviel/4sq-tar-k-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import cudf\n",
    "import lofo\n",
    "import torch\n",
    "import pickle\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pandarallel.initialize(progress_bar=False, use_memory_fs=False)\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.features import *\n",
    "from data.preparation import *\n",
    "from data.post_processing import *\n",
    "\n",
    "from utils.logger import prepare_log_folder, create_logger, save_config\n",
    "from utils.metrics import *\n",
    "\n",
    "from model_zoo.xgb import train_xgb, objective_xgb, lofo_xgb\n",
    "from model_zoo.catboost import train_catboost, objective_catboost, lofo_catboost\n",
    "from training.main_boosting import k_fold\n",
    "from utils.plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = LOG_PATH + \"2022-05-19/4/\"  # 1 ep, d=256, large\n",
    "N_NEIGHBORS = 20\n",
    "\n",
    "EXP_FOLDER_2 = None \n",
    "THRESHOLD = None\n",
    "\n",
    "# EXP_FOLDER_2 = LOG_PATH + \"lvl_2/\" + \"2022-05-25/6/\"  # 50 Neighbors\n",
    "# THRESHOLD = 0.01\n",
    "\n",
    "# EXP_FOLDER_2 = LOG_PATH + \"lvl_2/\" + \"2022-05-30/1/\"  # 20 Neighbors\n",
    "# THRESHOLD = 0.01  # 0.001\n",
    "\n",
    "# EXP_FOLDER_2 = LOG_PATH + \"lvl_2/\" + \"2022-06-06/3/\"  # 20 Neighbors\n",
    "# THRESHOLD = 0.01  # 0.001\n",
    "\n",
    "FOLD = 0\n",
    "\n",
    "PRECOMPUTED = True\n",
    "\n",
    "# MAX_DIST_POS = 0.015\n",
    "# MAX_DIST_NN = 0.3\n",
    "\n",
    "MAX_DIST_POS = None  # 0.02\n",
    "MAX_DIST_NN = None  # 0.35\n",
    "\n",
    "CONVERT_JAP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRECOMPUTED:\n",
    "    if EXP_FOLDER_2 is None:  # lvl 2\n",
    "        assert os.path.exists(EXP_FOLDER + f'df_p_{N_NEIGHBORS}.csv'), \"Level 1 precomputed pairs not found\"\n",
    "    else:  # lvl 3\n",
    "        assert os.path.exists(EXP_FOLDER_2 + f'df_p_{THRESHOLD}.csv'), \"Level 2 precomputed pairs not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_matches = json.load(open(DATA_PATH + \"gt.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    df = pd.read_csv(DATA_PATH + \"df_train.csv\").set_index('id')\n",
    "    \n",
    "    if CONVERT_JAP:\n",
    "        df = convert_japanese_alphabet(df)\n",
    "\n",
    "    folds = pd.read_csv(DATA_PATH + \"folds_2.csv\")[['id', 'fold']]\n",
    "    df = df.merge(folds, how=\"left\", on=\"id\").set_index(\"id\")\n",
    "\n",
    "    df = df[df['fold'] == FOLD]\n",
    "    df['idx'] = np.array(range(len(df)))\n",
    "\n",
    "    try:\n",
    "        df = cudf.from_pandas(df)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if THRESHOLD is None or EXP_FOLDER_2 is None:  # lvl 2\n",
    "        df_p = cudf.read_csv(EXP_FOLDER + f'df_pairs_{N_NEIGHBORS}.csv')\n",
    "    else:  # lvl 3\n",
    "        df_p = cudf.read_csv(EXP_FOLDER_2 + f'kept_pairs_{THRESHOLD}.csv')\n",
    "\n",
    "    df_p = df_p.merge(df, how=\"left\", left_on=\"id_1\", right_on=\"id\")\n",
    "    df_p = df_p.merge(df, how=\"left\", left_on=\"id_2\", right_on=\"id\", suffixes=(\"_1\", \"_2\"))\n",
    "\n",
    "    print(f'Retrieved {numerize(len(df_p))} pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "- Add features based on model preds\n",
    "- Categories clusters using pois (leaky ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAN_COLS = ['address', 'city', 'state', 'zip', 'url', 'phone']\n",
    "\n",
    "TF_IDF_COLS = ['name', 'categories', 'address', 'url']\n",
    "\n",
    "TF_IDF_PARAMS = [\n",
    "    ((1, 1), 'word'),  # word unigrams\n",
    "    ((3, 3), 'char_wb'),  # char trigrams\n",
    "]\n",
    "\n",
    "STRING_DIST_COLS = ['name', \"categories\", 'address', 'url', 'phone']\n",
    "\n",
    "FEATURES_SAME = [\n",
    "    ('country', is_equal),\n",
    "    ('state', is_equal),\n",
    "    ('zip', is_included),\n",
    "    ('phone', is_included),\n",
    "    ('city', is_included),\n",
    "    ('categories', is_included),\n",
    "]\n",
    "\n",
    "FEATURES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_FT_FOLDERS = [\n",
    "    (\"xlm-large\", LOG_PATH + \"2022-05-19/4/\"),            # 1 ep, d=256, large\n",
    "#     (\"roberta\", LOG_PATH + \"2022-05-20/1/\"),              # roberta-large\n",
    "#     (\"xlm-base+url\", LOG_PATH + \"2022-05-20/2/\"),         # base + url\n",
    "    (\"xlm-large+noaddress\", LOG_PATH + \"2022-05-20/3/\"),  # large + no address\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GROUPS = [\n",
    "    \"nn_dist_l1_*\",\n",
    "    \"nn_dist_l2_*\",\n",
    "    \"nn_cosine_sim_*\",\n",
    "#     \".*_tf_idf_11_word_sim\",\n",
    "#     \".*_tf_idf_33_char_wb_sim\",\n",
    "#     \".*_lcs$\",\n",
    "#     \".*_gesh\", \n",
    "#     \".*_jaro\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.logger import upload_to_kaggle\n",
    "\n",
    "# upload_to_kaggle([f[0] for f in NN_FT_FOLDERS], LOG_PATH + \"lvl_1/dataset_1\", \"Foursquare Weights 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    df_p.loc[df_p['rank'] == -1, 'rank'] = np.nan\n",
    "    df_p.loc[df_p['rank_nn'] == -1, 'rank_nn'] = np.nan\n",
    "\n",
    "    df_p['rank_nan'] = df_p[\"rank\"].isna().astype(np.uint8)\n",
    "    df_p['rank_nn_nan'] = df_p[\"rank_nn\"].isna().astype(np.uint8)\n",
    "    df_p[\"rank_both_nan\"] = df_p[[\"rank_nan\", \"rank_nn_nan\"]].min(axis=1)\n",
    "    df_p[\"rank_any_nan\"] = df_p[[\"rank_nan\", \"rank_nn_nan\"]].max(axis=1)\n",
    "\n",
    "#     df_p[\"rank_min\"] = df_p[[\"rank_nn\", \"rank\"]].min(axis=1)\n",
    "#     df_p[\"rank_max\"] = - (- df_p[[\"rank_nn\", \"rank\"]]).max(axis=1)\n",
    "#     df_p[\"rank_diff\"] = df_p[\"rank_nn\"] - df_p[\"rank\"]\n",
    "\n",
    "    FEATURES += [\n",
    "        \"rank\", \"rank_nn\", \n",
    "        \"rank_nan\", \"rank_nn_nan\", \n",
    "        \"rank_both_nan\", \"rank_any_nan\",\n",
    "#         \"rank_min\", \"rank_max\", \n",
    "#         \"rank_diff\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    FEATURES += compute_nan_features(df_p, NAN_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    for name, folder in tqdm(NN_FT_FOLDERS):\n",
    "        print(f'-> Adding features using model {name}')\n",
    "        nn_preds = np.load(folder + f\"fts_val_{FOLD}.npy\").astype(np.float16)\n",
    "        nn_preds = torch.from_numpy(nn_preds).cuda()\n",
    "\n",
    "        FEATURES += compute_nn_distances(df_p, nn_preds, suffix=\"_\" + name)\n",
    "        \n",
    "        del nn_preds\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    FEATURES += compute_position_distances(df_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_distance(row, min_dist_pos=1, min_dist_nn=0.5):\n",
    "    to_keep = 1\n",
    "\n",
    "    if row['rank'] >= 1000 and row['rank_nn'] < 1000:\n",
    "        # Only NN found\n",
    "        if row['nn_dist_l2'] > min_dist_nn:\n",
    "            to_keep = 0\n",
    "    \n",
    "    elif row['rank'] < 1000 and row['rank_nn'] >= 1000:\n",
    "        if row['angular_distance_min'] > min_dist_pos:\n",
    "            to_keep = 0\n",
    "\n",
    "    return to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED and MAX_DIST_POS is not None and MAX_DIST_NN is not None:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(y=\"angular_distance_min\", x=\"match\", data=df_p[df_p[\"rank\"] < 1000].sample(100000).to_pandas())\n",
    "    plt.axhline(MAX_DIST_POS, c=\"salmon\")\n",
    "    plt.yscale('log')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=\"nn_dist_l2\", x=\"match\", data=df_p[df_p[\"rank_nn\"] < 1000].sample(100000).to_pandas())\n",
    "    plt.axhline(MAX_DIST_NN, c=\"salmon\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED and MAX_DIST_POS is not None and MAX_DIST_NN is not None:\n",
    "    to_keep = ~(\n",
    "        (df_p['rank'].isna() & ~df_p['rank_nn'].isna() & (df_p['nn_dist_l2'] > MAX_DIST_NN)) |\n",
    "        (~df_p['rank'].isna() & df_p['rank_nn'].isna() & (df_p['angular_distance_min'] > MAX_DIST_POS))\n",
    "    )\n",
    "\n",
    "    plot_confusion_matrix(to_keep.to_numpy(), df_p['match'].to_numpy(), display_labels=[\"0\", \"1\"])\n",
    "    \n",
    "    ids = get_ids(DATA_PATH, fold=FOLD)\n",
    "    preds, scores = preds_to_matches(df_p['match'], df_p, threshold=0.5, ids=ids)\n",
    "    s = compute_iou(preds, gt_matches)\n",
    "    print(f\"Before : Best reachable IoU : {s:.3f} with {numerize(len(df_p))} candidates\")\n",
    "    \n",
    "    df_p = df_p[to_keep].reset_index(drop=True)\n",
    "\n",
    "    ids = get_ids(DATA_PATH, fold=FOLD)\n",
    "    preds, scores = preds_to_matches(df_p['match'], df_p, threshold=0.5, ids=ids)\n",
    "    s = compute_iou(preds, gt_matches)\n",
    "    print(f\"After  : Best reachable IoU : {s:.3f} with {numerize(len(df_p))} candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    for col in TF_IDF_COLS:\n",
    "        for ngram_range, analyzer in TF_IDF_PARAMS:\n",
    "            print(f'Computing feature \"{col}_tf_idf_{ngram_range[0]}{ngram_range[1]}_{analyzer}_sim\"')\n",
    "\n",
    "            tf_idf = TfidfVectorizer(use_idf=False, ngram_range=ngram_range, analyzer=analyzer)\n",
    "            tf_idf_mat = tf_idf.fit_transform(df[col].fillna('nan'))\n",
    "\n",
    "            df_p[f\"{col}_tf_idf_{ngram_range[0]}{ngram_range[1]}_{analyzer}_sim\"] = tf_idf_similarity(df_p, tf_idf_mat)\n",
    "            FEATURES.append(f\"{col}_tf_idf_{ngram_range[0]}{ngram_range[1]}_{analyzer}_sim\")\n",
    "        \n",
    "            # TODO : If one string is nan, put ft to nan ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion / equality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED and not isinstance(df_p, pd.DataFrame):\n",
    "    df_p = df_p.to_pandas()\n",
    "#     df_p = reduce_mem_usage(df_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    for col, fct in FEATURES_SAME:\n",
    "        print(f'Computing feature same_{col}')\n",
    "        df_p[f\"same_{col}\"] = df_p[[f\"{col}_1\", f\"{col}_2\"]].fillna('').parallel_apply(\n",
    "            lambda x: fct(x[0], x[1]), axis=1\n",
    "        ).astype(float)\n",
    "\n",
    "        FEATURES.append(f\"same_{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not PRECOMPUTED:\n",
    "    FEATURES += compute_string_distances(df_p, STRING_DIST_COLS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load / remove useless stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_KEEP = ['id_1', 'id_2', 'point_of_interest_1', 'point_of_interest_2', 'match'] + FEATURES\n",
    "\n",
    "if not PRECOMPUTED:\n",
    "    df_p.drop([c for c in df_p.columns if c not in TO_KEEP], axis=1, inplace=True)\n",
    "    # df_p.to_csv(EXP_FOLDER + f'df_p_{N_NEIGHBORS}.csv', index=False)\n",
    "else:\n",
    "    if EXP_FOLDER_2 is None:  # lvl 2\n",
    "        df_p = cudf.read_csv(EXP_FOLDER + f'df_p_{N_NEIGHBORS}.csv')\n",
    "        print('Retrieved precomputed level 1 features')\n",
    "    else:  # lvl 3\n",
    "        df_p = cudf.read_csv(EXP_FOLDER_2 + f'df_p_{THRESHOLD}.csv')\n",
    "        print('Retrieved precomputed level 2 features')\n",
    "        \n",
    "    FEATURES = [col for col in df_p.columns[3:] if \"fold\" not in col and \"point_of_interest\" not in col]\n",
    "\n",
    "df_p['match'] = df_p['match'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds / Save computations\n",
    "\n",
    "TODO : non-leaky splits by considering pairs: \n",
    "- gkf on poi, val set considers left and right pois -> (x1, x2) if x1 is in fold 1 val, x2 can be seen during training but not with any element of same poi as x1. This should not be leaky ?\n",
    "- split before looking for pairs ? No bc it's important to have a 600k set to look for pairs in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "\n",
    "if \"fold_1\" not in df_p.columns:\n",
    "    if not os.path.exists(DATA_PATH + f\"folds_{FOLD}_{N_SPLITS}.csv\"):\n",
    "        gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "        splits = gkf.split(df, groups=df['point_of_interest'])\n",
    "\n",
    "        df_split = df.reset_index()[['id', 'point_of_interest']]\n",
    "        df_split['fold'] = -1\n",
    "\n",
    "        for i, (_, val_idx) in enumerate(splits):\n",
    "            df_split.loc[val_idx, 'fold'] = i\n",
    "\n",
    "        df_split.to_csv(DATA_PATH + f\"folds_{FOLD}_{N_SPLITS}.csv\", index=False)\n",
    "        \n",
    "    df_split = pd.read_csv(DATA_PATH + f\"folds_{FOLD}_{N_SPLITS}.csv\")\n",
    "\n",
    "#     df_split = cudf.read_csv(DATA_PATH + f\"folds_{FOLD}_{N_SPLITS}.csv\")  \n",
    "#     try:\n",
    "#         df_p = cudf.from_pandas(df_p)\n",
    "#     except TypeError:\n",
    "#         print('df_p already in pandas')\n",
    "\n",
    "    df_p = df_p.merge(df_split[['id', 'fold']], how=\"left\", left_on=\"id_1\", right_on=\"id\")\n",
    "    df_p.drop('id', axis=1, inplace=True)\n",
    "    df_p = df_p.merge(df_split[['id', 'fold']], how=\"left\", left_on=\"id_2\", right_on=\"id\", suffixes=(\"_1\", \"_2\"))\n",
    "    df_p.drop('id', axis=1, inplace=True)\n",
    "    df_p.drop([c for c in df_p.columns if c not in TO_KEEP + ['fold_1', 'fold_2']], axis=1, inplace=True)\n",
    "\n",
    "    # Save\n",
    "    if EXP_FOLDER_2 is None:\n",
    "        save_path = EXP_FOLDER + f'df_p_{N_NEIGHBORS}.csv'\n",
    "    else:\n",
    "        save_path = EXP_FOLDER_2 + f'df_p_{THRESHOLD}.csv'\n",
    "\n",
    "    if isinstance(df_p, pd.DataFrame):\n",
    "        df_p.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        df_p.to_pandas().to_csv(save_path, index=False)\n",
    "        \n",
    "    print(f'-> Saved precomputed features to \"{save_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if isinstance(df_p, pd.DataFrame):\n",
    "#     df_p = cudf.from_pandas(df_p)\n",
    "    \n",
    "# for group in tqdm(FT_GROUPS):\n",
    "#     cols = list(filter(re.compile(group).match, FEATURES))\n",
    "    \n",
    "#     try:\n",
    "#         assert len(cols), f\"Group {group}, matches no columns\"\n",
    "#         assert len(cols) > 2, \"Group is too small\"\n",
    "#     except AssertionError:\n",
    "#         continue\n",
    "\n",
    "#     df_p[f\"{group}_mean\"] = np.nanmean(df_p[cols].fillna(np.nan).values, axis=1)\n",
    "#     df_p[f\"{group}_min\"] = np.nanmin(df_p[cols].fillna(np.nan).values, axis=1)\n",
    "#     df_p[f\"{group}_sum\"] = np.nansum(df_p[cols].fillna(np.nan).values, axis=1)\n",
    "# #     df_p[f\"{group}_max\"] = np.nanmax(df_p[cols].fillna(np.nan).values, axis=1)\n",
    "# #     df_p[f\"{group}_std\"] = np.nanstd(df_p[cols].fillna(np.nan).values, axis=1)\n",
    "\n",
    "#     FEATURES += [\n",
    "#         f\"{group}_mean\",\n",
    "#         f\"{group}_min\",\n",
    "\n",
    "#         f\"{group}_sum\"\n",
    "# #         f\"{group}_max\",\n",
    "# #         f\"{group}_std\",\n",
    "#     ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_p = df_p[(df_p['rank'].fillna(999) < 15) | (df_p['rank_nn'].fillna(999) < 15)].reset_index(drop=True)\n",
    "\n",
    "# df_p = df_p.fillna(-1)  ## ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not isinstance(df_p, pd.DataFrame):\n",
    "    df_p = df_p.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE = False\n",
    "TRAIN = False\n",
    "DEBUG = False\n",
    "LOFO = False\n",
    "\n",
    "OPT_FOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_p = df_p[df_p['rank_nan'] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_p['preds'] = pred_oof\n",
    "\n",
    "# df_p.loc[(df_p['rank_nn'] > 20) & (df_p['rank_nn_nan'] == 1) , 'preds'] = 0\n",
    "# # df_p.loc[(df_p['rank'] > 10) & (df_p['rank_nn'] > 10) , 'preds'] = 0\n",
    "# df_p.loc[(df_p['rank_nan'] == 1) , 'preds'] = 0\n",
    "\n",
    "# (df_p['preds'] == pred_oof).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"xgb\":\n",
    "        {\n",
    "#             'max_depth': 10,\n",
    "#             'min_child_weight': 1,\n",
    "#             'reg_alpha': 0.1,\n",
    "#             'reg_lambda': 0.1,\n",
    "#             \"colsample_bytree\": 0.85,\n",
    "#             \"subsample\": 0.75,\n",
    "            'max_depth': 14, \n",
    "            'gamma': 0.02, \n",
    "            'min_child_weight': 9, \n",
    "            'colsample_bytree': 0.864,\n",
    "            'subsample': 0.614,\n",
    "            'reg_alpha': 0.198,\n",
    "            'reg_lambda': 0.27,\n",
    "        },\n",
    "    \"catboost\":\n",
    "        {\n",
    "            'depth': 10,\n",
    "#             'reg_lambda': 0.1,\n",
    "#             \"model_size_reg\": 0.5,\n",
    "#             'min_data_in_leaf': 1,\n",
    "#             \"bagging_temperature\": 1,\n",
    "#             \"leaf_estimation_iterations\": 10,\n",
    "#             \"subsample\": 0.75,\n",
    "#             \"colsample_bylevel\": 0.85\n",
    "        }\n",
    "}\n",
    "\n",
    "OBJECTIVES = {\n",
    "    \"xgb\": objective_xgb,\n",
    "    \"catboost\" : objective_catboost\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"xgb\"  # \"catboost\", \"xgb\"\n",
    "params = PARAMS[NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZE:\n",
    "    df_train_opt = df_p[(df_p['fold_1'] != OPT_FOLD) & (df_p['fold_2'] != OPT_FOLD)].reset_index(drop=True)\n",
    "    df_val_opt = df_p[\n",
    "        (df_p['fold_1'] == OPT_FOLD) | (df_p['fold_2'] == OPT_FOLD)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    objective = OBJ_FCTS[NAME]\n",
    "    objective = lambda x: objective(x, df_train_opt, df_val_opt, FEATURES, \"match\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    params = study.best_params\n",
    "    print(\"Final params :\\n\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    exp_folder = EXP_FOLDER\n",
    "    n_neighbors = N_NEIGHBORS\n",
    "    \n",
    "    exp_folder_2 = EXP_FOLDER_2\n",
    "    threshold = THRESHOLD\n",
    "\n",
    "    fold = FOLD\n",
    "    n_folds = N_SPLITS\n",
    "    \n",
    "    max_dist_pos = MAX_DIST_POS\n",
    "    max_dist_nn = MAX_DIST_NN\n",
    "\n",
    "    features = FEATURES\n",
    "#     cat_features = [ft for ft in FEATURES if \"nan\" in ft or \"same\" in ft]\n",
    "    cat_features = []\n",
    "\n",
    "    target = \"match\"\n",
    "    model = NAME\n",
    "    params = params\n",
    "    selected_folds = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Config.cat_features):\n",
    "    df_p[Config.cat_features] = df_p[Config.cat_features].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoFo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOFO:\n",
    "    imp_df = lofo_xgb(\n",
    "        df_p.sample(1000000).reset_index(drop=True),\n",
    "        Config,\n",
    "        folds=[0], #, 1, 2, 3, 4],  # [OPT_FOLD],\n",
    "        auto_group_threshold=0.95\n",
    "    )\n",
    "\n",
    "    lofo.plot_importance(imp_df)\n",
    "    plt.savefig(EXP_FOLDER + f\"lofo_imp_{N_NEIGHBORS}.png\", facecolor='white', transparent=False)\n",
    "    imp_df.to_csv(EXP_FOLDER + f\"lofo_imp_{N_NEIGHBORS}.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    if os.path.exists(EXP_FOLDER + f\"lofo_imp_{N_NEIGHBORS}.csv\"):\n",
    "        imp_df = pd.read_csv(EXP_FOLDER + f\"lofo_imp_{N_NEIGHBORS}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fts = []\n",
    "# for ft in imp_df[imp_df['importance_mean'] > 0]['feature'].values.tolist():\n",
    "#     fts += ft.split(' & ')\n",
    "\n",
    "# Config.features = fts\n",
    "\n",
    "# # Config.features = FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    log_folder = None\n",
    "    if not DEBUG:\n",
    "        log_folder = prepare_log_folder(LOG_PATH + \"lvl_2/\")\n",
    "        print(f'Logging results to {log_folder}')\n",
    "        save_config(Config, log_folder + 'config')\n",
    "        create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "#     df_p = reduce_mem_usage(df_p)\n",
    "    pred_oof, models, ft_imp = k_fold(df_p, Config, log_folder=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-05-25/6/\"  # lvl 2 - 50 neighbors\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-05-30/1/\"  # lvl 2 - 20 neighbors\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-06/3/\"  # lvl 2 - 20 neighbors more fts\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-07/0/\"  # lvl 2 - 30 neighbors more fts\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-07/1/\"  # lvl 2 - 10 neighbors + rank fts + max_dist\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-07/3/\"  # lvl 2 - 20 neighbors + max_dist\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-07/5/\"  # lvl 2 - 10 neighbors + max_dist\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-09/1/\"  # 0.869+ - 20 neighbors + rank fts\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-09/2/\"  # lvl 2 - 10 neighbors + rank fts\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-09/3/\"  # 0.8709 - 20 neighbors + rank fts & more nn fts\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-09/6/\"  # 0 - 20 neighbors + rank fts & more nn fts + groups\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-15/0/\"  # 0.87 - 20 neighbors + rank fts & more nn fts + more tf-idf\n",
    "# EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-15/9/\"  # 0.8702 - 20 neighbors + rank fts & more nn fts + more tf-idf\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-16/0/\"  # 0.8708 - 20 neighbors + rank fts & more nn fts + more tf-idf + agg\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-16/1/\"  # 0.8707 - 20 neighbors + rank fts & more nn fts + more tf-idf\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-20/1/\"  # 0.8720 - xgb 20 neighbors fix\n",
    "\n",
    "# EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-20/3/\"  # 0.8681 - catboost 15 neighbors\n",
    "# EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-20/10/\"  # 0.8712 - xgb 15 neighbors\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"lvl_2/\" + \"2022-06-21/2/\"  # 0.8714 - xgb 20 neighbors fix less nn, reproduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN:\n",
    "    pred_oof = np.load(EXP_FOLDER + \"pred_oof.npy\")\n",
    "    ft_imp = pd.read_csv(EXP_FOLDER + \"ft_imp.csv\").set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p['pred'] = pred_oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_p[Config.target].values if isinstance(df_p, pd.DataFrame) else df_p[Config.target].get()\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    pred_oof > 0.5,\n",
    "    y,\n",
    "    display_labels=['No Match', 'Match'],\n",
    "#     normalize=\"pred\"\n",
    ")\n",
    "\n",
    "plt.title(f\"AUC = {roc_auc_score(y, pred_oof) :.4f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = get_ids(DATA_PATH, fold=Config.fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds, scores = preds_to_matches(pred_oof, df_p, threshold=0.55, ids=ids)\n",
    "\n",
    "# preds, scores = preds_to_matches(df_p['preds'], df_p, threshold=0.6, ids=ids)\n",
    "\n",
    "# preds, scores = preds_to_matches(df_p['match'].values, df_p.copy(), threshold=0.5)  # Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"CV IoU : {compute_iou(preds, gt_matches) :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found_prop, missed = compute_found_prop(preds, gt_matches)\n",
    "\n",
    "n_matches = sum([len(preds[k]) - 1 for k in preds])\n",
    "\n",
    "print(f\"Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if EXP_FOLDER_2 is None:\n",
    "#     for thresh in [0.001, 0.005, 0.01, 0.02]:\n",
    "#         print(f'\\n -> Threshold={thresh}\\n')\n",
    "\n",
    "#         kept_pairs = df_p[df_p['pred'] > thresh].reset_index(drop=True)\n",
    "\n",
    "#         preds, _ = preds_to_matches(kept_pairs['pred'].values, kept_pairs, threshold=thresh, ids=ids)\n",
    "#         found_prop, _ = compute_found_prop(preds, gt_matches)\n",
    "\n",
    "#         n_matches = sum([len(preds[k]) - 1 for k in preds])\n",
    "#         print(f\"Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.\")\n",
    "\n",
    "#         kept_pairs = kept_pairs[['id_1', 'id_2', 'match']]\n",
    "#         save_path = EXP_FOLDER + f\"kept_pairs_{thresh}.csv\"\n",
    "#         kept_pairs.to_csv(save_path, index=False)\n",
    "#         print(f'Saved pairs to \"{save_path}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=[min(len(gt_matches[k]), 10) for k in gt_matches if k in preds.keys()])\n",
    "plt.title('Number of pred matches per id')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=[min(len(preds[k]), 10) for k in preds])\n",
    "plt.title('Number of gt matches per id')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_importances(ft_imp)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = prepare_train_data(root=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, id_ in enumerate(preds):\n",
    "#     if not len(list(missed[i])):\n",
    "#         continue\n",
    "\n",
    "#     print('Query')\n",
    "#     display(df.loc[[id_]])\n",
    "\n",
    "#     print('Target')\n",
    "#     display(df.loc[[g for g in gt_matches[id_] if g != id_]])\n",
    "\n",
    "#     print('Missed')\n",
    "#     display(df.loc[list(missed[i])])\n",
    "\n",
    "# #     print('Preds')\n",
    "# #     display(df.loc[preds_matches[df.index[i]]].head(5))\n",
    "\n",
    "# #     break\n",
    "#     print('-' * 50)\n",
    "\n",
    "#     if i > 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.post_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_pp = limit_numbers(preds, scores, 20)\n",
    "# print(f\"CV IoU : {compute_iou(preds_pp, gt_matches) :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_pp = post_process_matches(preds, mode=\"remove\")\n",
    "# print(f\"CV IoU : {compute_iou(preds_pp, gt_matches) :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_pp = post_process_matches(preds, mode=\"append\")\n",
    "print(f\"CV IoU : {compute_iou(preds_pp, gt_matches) :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found_prop, missed = compute_found_prop(preds_pp, gt_matches)\n",
    "\n",
    "n_matches = sum([len(preds_pp[k]) - 1 for k in preds_pp])\n",
    "\n",
    "print(f\"Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
