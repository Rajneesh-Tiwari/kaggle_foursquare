{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code to create canadidate pairs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113095,
     "end_time": "2022-06-20T12:41:41.215410",
     "exception": false,
     "start_time": "2022-06-20T12:41:41.102315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-20T12:42:48.194197Z",
     "iopub.status.busy": "2022-06-20T12:42:48.193938Z",
     "iopub.status.idle": "2022-06-20T12:42:50.735561Z",
     "shell.execute_reply": "2022-06-20T12:42:50.734610Z"
    },
    "papermill": {
     "duration": 2.672311,
     "end_time": "2022-06-20T12:42:50.737847",
     "exception": false,
     "start_time": "2022-06-20T12:42:48.065536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from params import OUT_PATH, IS_TEST\n",
    "from ressources import COUNTRIES\n",
    "from matching import (\n",
    "    load_cleaned_data,\n",
    "    print_infos,\n",
    "    lcs,\n",
    "    lcs2,\n",
    "    distance,\n",
    "    pi1,\n",
    "    substring_ratio,\n",
    "    subseq_ratio,\n",
    "    ll_lcs,\n",
    "    get_CV,\n",
    "    Compute_Mdist_Mindex,\n",
    "    vectorisation_similarite,\n",
    "    haversine,\n",
    "    create_address,\n",
    "    find_potential_matchs,\n",
    ")\n",
    "\n",
    "random.seed(13)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.128224,
     "end_time": "2022-06-20T12:42:52.888296",
     "exception": false,
     "start_time": "2022-06-20T12:42:52.760072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_TEST:\n",
    "    train = load_cleaned_data(OUT_PATH + \"cleaned_data_test.csv\")\n",
    "else:\n",
    "    train = load_cleaned_data(OUT_PATH + \"cleaned_data_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train = train.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_TEST:\n",
    "    clusts = (\n",
    "        train[[\"id\", \"point_of_interest\"]]\n",
    "        .groupby(\"point_of_interest\")\n",
    "        .agg(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "    clusts = clusts[clusts[\"id\"].apply(lambda x: len(x) > 1)]\n",
    "\n",
    "    N_TO_FIND = clusts[\"id\"].apply(lambda x: len(x)).sum()\n",
    "    print(N_TO_FIND)\n",
    "\n",
    "    example = clusts.explode(\"id\").sort_values(\"id\")\n",
    "    example[\"y\"] = 1\n",
    "\n",
    "    print_infos(example, None, N_TO_FIND)\n",
    "\n",
    "    p1 = example.sample(len(example) // 2)\n",
    "\n",
    "    print_infos(p1, None, N_TO_FIND)\n",
    "\n",
    "else:\n",
    "    N_TO_FIND = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"phone\", \"lon2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"phone\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"phone\", \"lon2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"phone\"].to_numpy()\n",
    "\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 11):  # 11 = add 10 sets\n",
    "        if i + j < p3.shape[0] and lcs(d[i], d[i + j]) >= 7:  # accept <=3 digits off\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "\n",
    "p1 = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2 = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lat / lon 22mÂ² square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat/lon, rounded to 2 x 4 digits = 22* meters square; there should not be too many false positives this close to each other\n",
    "# do this in 4 blocks, shifted by 1/2 size, to avoid cut-offs\n",
    "for s1 in [0, 5e-5]:\n",
    "    for s2 in [0, 5e-5]:\n",
    "        p3 = train[\n",
    "            [\"country\", \"id\", \"point_of_interest\", \"latitude\", \"longitude\"]\n",
    "        ].copy()\n",
    "        p3[\"latitude\"] = np.round(s1 + 0.5 * p3[\"latitude\"], 4)  # rounded to 4 digits\n",
    "        p3[\"longitude\"] = np.round(\n",
    "            s2 + 0.5 * p3[\"longitude\"] / np.cos(p3[\"latitude\"] * 3.14 / 180.0), 4\n",
    "        )  # rounded to 4 digits\n",
    "        p3 = p3.sort_values(by=[\"country\", \"latitude\", \"longitude\", \"id\"]).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        idx1 = []\n",
    "        idx2 = []\n",
    "        lat, lon = p3[\"latitude\"].to_numpy(), p3[\"longitude\"].to_numpy()\n",
    "        for i in range(p3.shape[0] - 1):\n",
    "            for j in range(1, 5):  # 5 = add 4 sets\n",
    "                if (\n",
    "                    i + j < p3.shape[0]\n",
    "                    and lat[i] == lat[i + j]\n",
    "                    and lon[i] == lon[i + j]\n",
    "                ):\n",
    "                    idx1.append(i)\n",
    "                    idx2.append(i + j)\n",
    "        p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "        p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "        # append\n",
    "        p1 = p1.append(p1a, ignore_index=True)\n",
    "        p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"url\", \"lon2\", \"lat2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"url\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"url\", \"lon2\", \"lat2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"url\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 2):  # 2 = add 1 set\n",
    "        if i + j < p3.shape[0] and ll_lcs(d[i], d[i + j], 3) >= 7:  # ll_lcs(3) >= 7\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"categories\", \"lon2\", \"lat2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"categories\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"categories\", \"lon2\", \"lat2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"categories\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 2):  # 2 = add 1 set\n",
    "        if i + j < p3.shape[0] and d[i][:4] == d[i + j][:4]:  # match on first 4 leters\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"address\", \"lon2\", \"lat2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"address\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"address\", \"lon2\", \"lat2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"address\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 7):  # 7 = add 6 sets\n",
    "        if i + j < p3.shape[0] and lcs2(d[i], d[i + j]) >= 6:  # lcs2 >= 6\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name\n",
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"name\", \"lon2\", \"lat2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"name\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"name\", \"lon2\", \"lat2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"name\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 4):  # 4 = add 3 sets\n",
    "        if i + j < p3.shape[0] and lcs2(d[i], d[i + j]) >= 5:  # lcs2 >= 5\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"name\", \"latitude\"]].copy()\n",
    "p3 = p3.sort_values(by=[\"country\", \"latitude\", \"id\"]).reset_index(drop=True)\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"latitude\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 21):  # 21 = add 20 sets\n",
    "        if i + j < p3.shape[0] and d[i] == d[i + j]:  # exact match\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"name\", \"longitude\"]].copy()\n",
    "p3 = p3.sort_values(by=[\"country\", \"longitude\", \"id\"]).reset_index(drop=True)\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"longitude\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 21):  # 21 = add 20 sets\n",
    "        if i + j < p3.shape[0] and d[i] == d[i + j]:  # exact match\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1[\"y\"] = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "a = p1.groupby(\"id\")[\"y\"].sum().reset_index()\n",
    "print(\n",
    "    \"Added\",\n",
    "    p1a.shape[0],\n",
    "    p1[\"y\"].sum(),\n",
    "    np.minimum(1, a[\"y\"]).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name lcs\n",
    "- Slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = train[\n",
    "    [\n",
    "        \"country\",\n",
    "        \"id\",\n",
    "        \"point_of_interest\",\n",
    "        \"name\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"categories\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# rounded coordinates\n",
    "p3[\"latitude\"] = np.round(p3[\"latitude\"], 1).astype(\n",
    "    \"float32\"\n",
    ")  # rounding: 1=10Km, 2=1Km\n",
    "p3[\"longitude\"] = np.round(p3[\"longitude\"], 1).astype(\"float32\")\n",
    "\n",
    "p3 = p3.sort_values(\n",
    "    by=[\"country\", \"latitude\", \"longitude\", \"categories\", \"id\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "names = p3[\"name\"].to_numpy()\n",
    "lon2 = p3[\"longitude\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = []\n",
    "idx2 = []\n",
    "\n",
    "for i in tqdm(range(p3.shape[0] - 1)):\n",
    "    li = lon2[i]\n",
    "    for j in range(\n",
    "        1, min(300, p3.shape[0] - 1 - i)\n",
    "    ):  # put a limit here - look at no more than X items\n",
    "        if (\n",
    "            li != lon2[i + j]\n",
    "        ):  # if lon matches, lat and country also match - b/c of sorting order\n",
    "            break\n",
    "        if lcs2(names[i], names[i + j]) >= 5:  # lcs2 >= 5\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1[\"y\"] = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "a = p1.groupby(\"id\")[\"y\"].sum().reset_index()\n",
    "print(\n",
    "    \"Added\",\n",
    "    p1a.shape[0],\n",
    "    p1[\"y\"].sum(),\n",
    "    np.minimum(1, a[\"y\"]).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1[\"y\"] = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "a = p1.groupby(\"id\")[\"y\"].sum().reset_index()\n",
    "print(\n",
    "    \"Added\",\n",
    "    p1a.shape[0],\n",
    "    p1[\"y\"].sum(),\n",
    "    np.minimum(1, a[\"y\"]).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del d, names, lon2, p3, idx1, idx2, p1a, p2a, lat, lon\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort to put similar points next to each other - for constructing pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sort = [\n",
    "    \"lat2\",\n",
    "    \"lon2\",\n",
    "    \"name2\",\n",
    "    \"latitude\",\n",
    "    \"city\",\n",
    "    \"cat2\",\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"country\",\n",
    "    \"id\",\n",
    "]\n",
    "\n",
    "train = train.sort_values(by=sort).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"id\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"point_of_interest\",\n",
    "    \"name\",\n",
    "    \"category_simpl\",\n",
    "    \"name_initial_decode\",\n",
    "]\n",
    "colsa = [\"id\", \"point_of_interest\"]\n",
    "\n",
    "p1a = train[colsa].copy()\n",
    "p2a = train[colsa].iloc[1:, :].reset_index(drop=True).copy()\n",
    "p2a = p2a.append(train[colsa].iloc[0], ignore_index=True)\n",
    "\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_svg = p1.copy()\n",
    "p2_svg = p2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more shifts\n",
    "- Slow (15min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = p1_svg.copy()\n",
    "p2 = p2_svg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, s in enumerate(tqdm(range(2, 121))):  # 121\n",
    "    if s == 15:  # resort by closer location after 15 shifts\n",
    "        train[\"lat2\"] = np.round(train[\"latitude\"], 2).astype(\"float32\")  # 2 = 1 Km\n",
    "        train[\"lon2\"] = np.round(train[\"longitude\"], 2).astype(\"float32\")\n",
    "        train = train.sort_values(\n",
    "            by=[\"country\", \"lat2\", \"lon2\", \"categories\", \"city\", \"id\"]\n",
    "        ).reset_index(drop=True)\n",
    "        train.drop([\"lat2\", \"lon2\"], axis=1, inplace=True)\n",
    "\n",
    "    if s < 4:\n",
    "        maxdist = 500000\n",
    "    elif s < 8:\n",
    "        maxdist = 10000\n",
    "    elif s < 12:\n",
    "        maxdist = 5000\n",
    "    elif s < 15:\n",
    "        maxdist = 2000\n",
    "    else:\n",
    "        maxdist = max(100, 200 - (s - 16) * 1)\n",
    "\n",
    "    s2 = s  # shift\n",
    "    if i >= 13:  # resorted data\n",
    "        s2 = i - 12\n",
    "\n",
    "    p2a = train[cols].iloc[s2:, :]\n",
    "    p2a = p2a.append(train[cols].iloc[:s2, :], ignore_index=True)\n",
    "\n",
    "    # drop pairs with large distances\n",
    "    dist = distance(\n",
    "        np.array(train[\"latitude\"]),\n",
    "        np.array(train[\"longitude\"]),\n",
    "        np.array(p2a[\"latitude\"]),\n",
    "        np.array(p2a[\"longitude\"]),\n",
    "    )\n",
    "    same_cat_simpl = (train[\"category_simpl\"] == p2a[\"category_simpl\"]) & (\n",
    "        train[\"category_simpl\"] > 0\n",
    "    )\n",
    "\n",
    "    ii = np.zeros(train.shape[0], dtype=np.int8)\n",
    "    x1, x2 = train[\"name\"].to_numpy(), p2a[\"name\"].to_numpy()\n",
    "    for j in range(train.shape[0]):\n",
    "        if pi1(x1[j], x2[j]):\n",
    "            ii[j] = 1\n",
    "        elif substring_ratio(x1[j], x2[j]) >= 0.65:\n",
    "            ii[j] = 1\n",
    "        elif subseq_ratio(x1[j], x2[j]) >= 0.75:\n",
    "            ii[j] = 1\n",
    "        elif len(x1[j]) >= 7 and len(x2[j]) >= 7 and x1[j].endswith(x2[j][-7:]):\n",
    "            ii[j] = 1\n",
    "    # keep if dist < maxdist, or names partially match\n",
    "    # idx = (dist < maxdist) | (ii > 0)\n",
    "    idx = (\n",
    "        (dist < maxdist)\n",
    "        | (ii > 0)\n",
    "        | np.logical_and(same_cat_simpl, dist < train[\"q90\"] * 900)\n",
    "    )\n",
    "\n",
    "    p1 = p1.append(train[colsa].loc[idx], ignore_index=True)\n",
    "    p2 = p2.append(p2a[colsa].loc[idx], ignore_index=True)\n",
    "\n",
    "    if not (s % 10):\n",
    "        # get stats; overstated b/c dups are not excluded yet\n",
    "        print(f\"{i}, s={s}\")\n",
    "        print_infos(p1, p2, N_TO_FIND)\n",
    "        print()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add close candidates \n",
    "- Slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_found = {\n",
    "    tuple(sorted([idx1, idx2])): 1 for idx1, idx2 in zip(p1[\"id\"], p2[\"id\"])\n",
    "}\n",
    "\n",
    "New_candidates = []\n",
    "\n",
    "# for country_ in range(2, len(countries)+1):\n",
    "for country_ in [2, 3]:\n",
    "\n",
    "    nb_max_candidates = 400\n",
    "    new_cand = set()\n",
    "\n",
    "    # Create matrix\n",
    "    matrix = train[train[\"country\"] == country_].copy()\n",
    "    if len(matrix) <= 1:\n",
    "        break\n",
    "    Original_idx = {i: idx for i, idx in enumerate(matrix.index)}\n",
    "\n",
    "    # Find closest neighbours\n",
    "    M_dist, M_index = Compute_Mdist_Mindex(\n",
    "        matrix[[\"latitude\", \"longitude\"]].to_numpy(),\n",
    "        nb_max_candidates=nb_max_candidates,\n",
    "    )\n",
    "\n",
    "    # Select candidates\n",
    "    new_true_match = 0\n",
    "    infos = matrix[[\"id\", \"name\", \"point_of_interest\"]].to_numpy()\n",
    "\n",
    "    for idx1, (Liste_idx, Liste_val) in enumerate(zip(M_index, M_dist)):\n",
    "        for idx2, dist in zip(Liste_idx, Liste_val):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "\n",
    "            # Too far candidates\n",
    "            if dist > 0.12:\n",
    "                break\n",
    "\n",
    "            id1, id2 = infos[idx1, 0], infos[idx2, 0]\n",
    "            name1, name2 = infos[idx1, 1], infos[idx2, 1]\n",
    "\n",
    "            if pi1(name1, name2) == 1 or substring_ratio(name1, name2) >= 0.5:\n",
    "                key = tuple(sorted([id1, id2]))\n",
    "                if key not in already_found:\n",
    "                    key_idx = tuple(sorted([Original_idx[idx1], Original_idx[idx2]]))\n",
    "                    try:\n",
    "                        if key_idx not in new_cand:\n",
    "                            new_true_match += int(infos[idx1, -1] == infos[idx2, -1])\n",
    "                    except:\n",
    "                        pass\n",
    "                    new_cand.add(key_idx)\n",
    "\n",
    "    # Add new candidates\n",
    "    New_candidates += [list(x) for x in new_cand]\n",
    "    print(\n",
    "        f\"Country {country_} ({COUNTRIES[country_-1]}) : {new_true_match}/{len(new_cand)} new cand added.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add matches\n",
    "size1 = len(p1)\n",
    "Added_p1, Added_p2 = [], []\n",
    "for idx1, idx2 in New_candidates:\n",
    "    id1, id2 = train[\"id\"].iat[idx1], train[\"id\"].iat[idx2]\n",
    "    poi1, poi2 = (\n",
    "        train[\"point_of_interest\"].iat[idx1],\n",
    "        train[\"point_of_interest\"].iat[idx2],\n",
    "    )\n",
    "    Added_p1.append([id1, poi1, 0])\n",
    "    Added_p2.append([id2, poi2])\n",
    "\n",
    "Added_p1 = pd.DataFrame(Added_p1, columns=p1.columns)\n",
    "Added_p2 = pd.DataFrame(Added_p2, columns=p2.columns)\n",
    "for col in Added_p1.columns:\n",
    "    Added_p1[col] = Added_p1[col].astype(p1[col].dtype)\n",
    "for col in Added_p2.columns:\n",
    "    Added_p2[col] = Added_p2[col].astype(p2[col].dtype)\n",
    "\n",
    "p1 = p1.append(Added_p1).reset_index(drop=True).copy()\n",
    "p2 = p2.append(Added_p2).reset_index(drop=True).copy()\n",
    "\n",
    "print(f\"Candidates added : {len(p1) - size1}/{len(p1)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx, matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_svg = p1.copy()\n",
    "p2_svg = p2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add close candidates v2\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1 = p1_svg.copy()\n",
    "p2 = p2_svg.copy()\n",
    "\n",
    "train[\"lat2\"] = np.round(train[\"latitude\"], 0).astype(\"int8\")\n",
    "train[\"lon2\"] = np.round(train[\"longitude\"], 0).astype(\"int8\")\n",
    "\n",
    "# sort to put similar points next to each other - for constructing pairs\n",
    "sort = [\n",
    "    \"category_simpl\",\n",
    "    \"lat2\",\n",
    "    \"lon2\",\n",
    "    \"name2\",\n",
    "    \"latitude\",\n",
    "    \"city\",\n",
    "    \"cat2\",\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"country\",\n",
    "    \"id\",\n",
    "]\n",
    "train = train.sort_values(by=sort).reset_index(drop=True)\n",
    "\n",
    "maxdist = (train[\"q90\"] * 400 + train[\"q99\"] * 400).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more shifts, only for short distances or for partial name matches\n",
    "for i, s in enumerate(tqdm(range(1, 50))): # 121\n",
    "\n",
    "    s2 = s  # shift\n",
    "    p2a = train[cols].iloc[s2:, :]\n",
    "    p2a = p2a.append(train[cols].iloc[:s2, :], ignore_index=True)\n",
    "\n",
    "    # drop pairs with large distances\n",
    "    same_cat_simpl = (train['category_simpl'] == p2a['category_simpl']).to_numpy()\n",
    "\n",
    "    dist = distance(\n",
    "        np.array(train['latitude']),\n",
    "        np.array(train['longitude']),\n",
    "        np.array(p2a['latitude']),\n",
    "        np.array(p2a['longitude'])\n",
    "    )\n",
    "\n",
    "    ii = np.zeros(train.shape[0], dtype=np.int8)\n",
    "    x1 = train[['name', 'name_initial_decode']].to_numpy()\n",
    "    x2 = p2a[['name', 'name_initial_decode']].to_numpy()\n",
    "\n",
    "    for j in range(train.shape[0]):  # pi1 adds 14K matches\n",
    "\n",
    "        if same_cat_simpl[j] and dist[j] < maxdist[j]:\n",
    "\n",
    "            name1, name2 = x1[j][0], x2[j][0]\n",
    "            name_ini1, name_ini2 = x1[j][1], x2[j][1]\n",
    "\n",
    "            if pi1(name1, name2) == 1:\n",
    "                ii[j] = 1\n",
    "            elif substring_ratio(name1, name2) >= 0.6:\n",
    "                ii[j] = 1\n",
    "            elif subseq_ratio(name1, name2) >= 0.7:\n",
    "                ii[j] = 1\n",
    "            elif len(name1) >= 6 and len(name2) >= 6 and name1.endswith(name2[-6:]) :\n",
    "                ii[j] = 1\n",
    "\n",
    "            # elif has_common_word(name_ini1, name_ini2, min_len=6) :\n",
    "            #    ii[j] = 1\n",
    "            # elif word_in_common(name_ini1, name_ini2, min_len_word=6):\n",
    "            #    ii[j] = 1\n",
    "            # elif subword_in_common(name_ini1, name_ini2, min_len_word=6) :\n",
    "            #    ii[j]=1\n",
    "\n",
    "    idx = (ii > 0)\n",
    "\n",
    "    p1 = p1.append(train[colsa].loc[idx], ignore_index=True)\n",
    "    p2 = p2.append(p2a[colsa].loc[idx], ignore_index=True)\n",
    "\n",
    "    if s < 10 or s % 10 == 0:\n",
    "        # get stats; overstated b/c dups are not excluded yet\n",
    "        print(f\"{i}, s={s}\")\n",
    "        print_infos(p1, p2, N_TO_FIND)\n",
    "        print()\n",
    "\n",
    "del p2a, dist, idx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_svg = p1.copy()\n",
    "p2_svg = p2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates in initial Youri's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1[\"y\"] = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "\n",
    "ID_to_POI = dict(zip(train[\"id\"], train[\"point_of_interest\"]))\n",
    "nb_true_matchs_initial = 0\n",
    "Cand = {}\n",
    "for i, (id1, id2) in enumerate(zip(p1[\"id\"], p2[\"id\"])):\n",
    "    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "    Cand[key] = p1[\"y\"].iloc[i]\n",
    "    nb_true_matchs_initial += int(ID_to_POI[id1] == ID_to_POI[id2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF nÂ°1 : airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = p1_svg.copy()\n",
    "p2 = p2_svg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "far_cat_simpl = [1, 2]\n",
    "thr_tfidf = 0.45\n",
    "\n",
    "for col_name in [\"name_initial_decode\"]:\n",
    "\n",
    "    Names = train[train[\"category_simpl\"].isin(far_cat_simpl + [-1])][\n",
    "        col_name\n",
    "    ].copy()  # add unknown categories\n",
    "\n",
    "    def process_terminal(text):\n",
    "        for i in range(0, 30):\n",
    "            text = text.replace(f\"terminal {i}\", \"\")\n",
    "            text = text.replace(f\"terminal{i}\", \"\")\n",
    "            text = text.replace(f\"t{i}\", \"\")\n",
    "        return text\n",
    "\n",
    "    Names = Names.apply(process_terminal)\n",
    "\n",
    "    # Drop stop words\n",
    "    Names = Names.apply(lambda x: x.replace(\"airpord\", \"airport\"))\n",
    "    Names = Names.apply(lambda x: x.replace(\"internasional\", \"international\"))\n",
    "    Names = Names.apply(lambda x: x.replace(\"internacional\", \"international\"))\n",
    "    for stopword in [\n",
    "        \"terminal\",\n",
    "        \"airport\",\n",
    "        \"arrival\",\n",
    "        \"hall\",\n",
    "        \"departure\",\n",
    "        \"bus stop\",\n",
    "        \"airways\",\n",
    "        \"checkin\",\n",
    "    ]:\n",
    "        Names = Names.apply(lambda x: x.replace(stopword + \"s\", \"\"))\n",
    "        Names = Names.apply(lambda x: x.replace(stopword, \"\"))\n",
    "    Names = Names.apply(lambda x: x.strip())\n",
    "    Names = Names[Names.str.len() >= 2]\n",
    "\n",
    "    Names_numrow = {\n",
    "        i: idx for i, idx in enumerate(Names.index)\n",
    "    }  # Keep initial row number\n",
    "    Names = Names.to_list()\n",
    "\n",
    "    print(f\"Len names : {len(Names)}.\")\n",
    "\n",
    "    # Tf-idf\n",
    "    if 1 < len(Names) < 400000:\n",
    "        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=thr_tfidf)\n",
    "\n",
    "        # no self-matchs and retrieve the initial row number\n",
    "        Tfidf_no_selfmatch = [\n",
    "            [Names_numrow[i], [Names_numrow[x] for x in L if x != i]]\n",
    "            for i, L in enumerate(Tfidf_idx)\n",
    "        ]\n",
    "        Tfidf_no_selfmatch = [x for x in Tfidf_no_selfmatch if len(x[-1]) > 0]\n",
    "        print(\"Nb cand tf-idf :\", sum([len(L) for idx, L in Tfidf_no_selfmatch]))\n",
    "\n",
    "        # Add matches\n",
    "        size1 = len(p1)\n",
    "        Added_p1, Added_p2 = [], []\n",
    "        for idx1, Liste_idx in Tfidf_no_selfmatch:\n",
    "            id1, name1, lat1, lon1 = (\n",
    "                train[\"id\"].iat[idx1],\n",
    "                train[\"name\"].iat[idx1],\n",
    "                train[\"latitude\"].iat[idx1],\n",
    "                train[\"longitude\"].iat[idx1],\n",
    "            )\n",
    "            cat1, country1, cat_simpl1 = (\n",
    "                train[\"categories\"].iat[idx1],\n",
    "                train[\"country\"].iat[idx1],\n",
    "                train[\"category_simpl\"].iat[idx1],\n",
    "            )\n",
    "            for idx2 in Liste_idx:\n",
    "                # if len(Liste_idx)>30 : continue\n",
    "                if idx1 < idx2:\n",
    "                    id2, lat2, lon2 = (\n",
    "                        train[\"id\"].iat[idx2],\n",
    "                        train[\"latitude\"].iat[idx2],\n",
    "                        train[\"longitude\"].iat[idx2],\n",
    "                    )\n",
    "                    cat2, country2, cat_simpl2 = (\n",
    "                        train[\"categories\"].iat[idx2],\n",
    "                        train[\"country\"].iat[idx2],\n",
    "                        train[\"category_simpl\"].iat[idx2],\n",
    "                    )\n",
    "                    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "                    # same_cat = (cat_simpl1==cat_simpl2 and cat_simpl1>0) or (cat1==cat2 and cat1!='')\n",
    "                    if (\n",
    "                        key not in Cand\n",
    "                        and (cat_simpl1 == 1 or cat_simpl2 == 1)\n",
    "                        and (\n",
    "                            haversine(lat1, lon1, lat2, lon2) <= 100\n",
    "                            or \"kualalumpur\" in name1\n",
    "                        )\n",
    "                    ):\n",
    "                        poi1, poi2 = (\n",
    "                            train[\"point_of_interest\"].iat[idx1],\n",
    "                            train[\"point_of_interest\"].iat[idx2],\n",
    "                        )\n",
    "                        Cand[key] = int(poi1 == poi2)\n",
    "                        Added_p1.append([id1, poi1])\n",
    "                        Added_p2.append([id2, poi2])\n",
    "                        nb_true_matchs_initial += int(poi1 == poi2)\n",
    "\n",
    "        p1 = (\n",
    "            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        p2 = (\n",
    "            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        print(f\"Candidates added for tfidf nÂ°1 (airports) : {len(p1)-size1}/{len(p1)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF nÂ°2 : metro stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "far_cat_simpl = [4]\n",
    "thr_tfidf = 0.45\n",
    "thr_distance = 100\n",
    "\n",
    "for col_name in [\"name_initial\", \"name_initial_decode\"]:\n",
    "\n",
    "    Names = train[train[\"category_simpl\"].isin(far_cat_simpl)][\n",
    "        col_name\n",
    "    ].copy()  # add unknown categories\n",
    "\n",
    "    # Drop stop words\n",
    "    for stopword in [\"stasiun\", \"station\", \"metro\", \"åæ¹æ­\", \"bei gai zha\", \"stasiun\"]:\n",
    "        Names = Names.apply(lambda x: x.replace(stopword + \"s\", \"\"))\n",
    "        Names = Names.apply(lambda x: x.replace(stopword, \"\"))\n",
    "    Names = Names.apply(lambda x: x.strip())\n",
    "    Names = Names[Names.str.len() > 2]\n",
    "\n",
    "    Names_numrow = {\n",
    "        i: idx for i, idx in enumerate(Names.index)\n",
    "    }  # Keep initial row number\n",
    "    Names = Names.to_list()\n",
    "\n",
    "    print(f\"Len names : {len(Names)}.\")\n",
    "\n",
    "    # Tf-idf\n",
    "    if 1 < len(Names) < 400000:\n",
    "        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=thr_tfidf)\n",
    "\n",
    "        # no self-matchs and retrieve the initial row number\n",
    "        Tfidf_no_selfmatch = [\n",
    "            [Names_numrow[i], [Names_numrow[x] for x in L if x != i]]\n",
    "            for i, L in enumerate(Tfidf_idx)\n",
    "        ]\n",
    "        Tfidf_no_selfmatch = [x for x in Tfidf_no_selfmatch if len(x[-1]) > 0]\n",
    "        print(\"Nb cand tf-idf :\", sum([len(L) for idx, L in Tfidf_no_selfmatch]))\n",
    "\n",
    "        # Add matches\n",
    "        size1 = len(p1)\n",
    "        Added_p1, Added_p2 = [], []\n",
    "        for idx1, Liste_idx in Tfidf_no_selfmatch:\n",
    "            id1, lat1, lon1, cat1, cat_simpl1 = (\n",
    "                train[\"id\"].iat[idx1],\n",
    "                train[\"latitude\"].iat[idx1],\n",
    "                train[\"longitude\"].iat[idx1],\n",
    "                train[\"categories\"].iat[idx1],\n",
    "                train[\"category_simpl\"].iat[idx1],\n",
    "            )\n",
    "            for idx2 in Liste_idx:\n",
    "                if idx1 < idx2:\n",
    "                    id2, lat2, lon2, cat2, cat_simpl2 = (\n",
    "                        train[\"id\"].iat[idx2],\n",
    "                        train[\"latitude\"].iat[idx2],\n",
    "                        train[\"longitude\"].iat[idx2],\n",
    "                        train[\"categories\"].iat[idx2],\n",
    "                        train[\"category_simpl\"].iat[idx2],\n",
    "                    )\n",
    "                    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "                    if (\n",
    "                        key not in Cand\n",
    "                        and haversine(lat1, lon1, lat2, lon2) <= thr_distance\n",
    "                        and (cat_simpl1 in far_cat_simpl or cat_simpl2 in far_cat_simpl)\n",
    "                    ):\n",
    "                        poi1, poi2 = (\n",
    "                            train[\"point_of_interest\"].iat[idx1],\n",
    "                            train[\"point_of_interest\"].iat[idx2],\n",
    "                        )\n",
    "                        Cand[key] = int(poi1 == poi2)\n",
    "                        Added_p1.append([id1, poi1])\n",
    "                        Added_p2.append([id2, poi2])\n",
    "                        nb_true_matchs_initial += int(poi1 == poi2)\n",
    "\n",
    "        p1 = (\n",
    "            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        p2 = (\n",
    "            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        print(\n",
    "            f\"Candidates added for tfidf nÂ°2 (metro stations) : {len(p1)-size1}/{len(p1)}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF nÂ°3a : for each countries (with initial unprocessed name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thr_tfidf_ = 0.5\n",
    "thr_distance_ = 20\n",
    "thr_distance_or_same_cat_ = 2\n",
    "\n",
    "size = len(p1)\n",
    "\n",
    "for country in [2, 3, 32]:  # range(1, 30)\n",
    "\n",
    "    # Reset parameters\n",
    "    thr_tfidf = thr_tfidf_\n",
    "    thr_distance = thr_distance_\n",
    "    thr_distance_or_same_cat = thr_distance_or_same_cat_\n",
    "\n",
    "    # Tune parameters for each country\n",
    "    if country == 2:\n",
    "        thr_tfidf = 1.1  # will impose to have same category\n",
    "        thr_distance = 20\n",
    "        thr_distance_or_same_cat = -1  # will impose to have same category\n",
    "    elif country == 3:\n",
    "        thr_tfidf = 0.6\n",
    "        thr_distance = 10\n",
    "    elif country == 32:\n",
    "        thr_tfidf = 0.4\n",
    "        thr_distance = 100  # no limit\n",
    "        thr_distance_or_same_cat = 100  # no limit\n",
    "\n",
    "    # List of names\n",
    "    Names = train[train[\"country\"] == country][\"name_initial\"].copy()\n",
    "    if len(Names) == 0:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    print(\"#\" * 20)\n",
    "    print(f\"# Country nÂ°{country} : {COUNTRIES[country-1]}.\")\n",
    "\n",
    "    Names_numrow = {\n",
    "        i: idx for i, idx in enumerate(Names.index)\n",
    "    }  # Keep initial row number\n",
    "    Names = Names.to_list()\n",
    "\n",
    "    print(f\"Len names : {len(Names)}.\")\n",
    "\n",
    "    # Tf-idf\n",
    "    if 1 < len(Names) < 400000:\n",
    "        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=min(0.45, thr_tfidf))\n",
    "\n",
    "        # no self-matchs and retrieve the initial row number\n",
    "        Tfidf_idx = [[Names_numrow[x] for x in L] for L in Tfidf_idx]\n",
    "\n",
    "        # Add matches : /!\\ ONLY IF THERE IS A CATEGORY MATCH AND THE DISTANCE IS NOT TOO BIG\n",
    "        size1 = len(p1)\n",
    "        Added_p1, Added_p2 = [], []\n",
    "        for idx1, (Liste_idx, Liste_val) in enumerate(zip(Tfidf_idx, Tfidf_val)):\n",
    "            idx1 = Names_numrow[idx1]\n",
    "            id1, lat1, lon1, cat1, cat_simpl1 = (\n",
    "                train[\"id\"].iat[idx1],\n",
    "                train[\"latitude\"].iat[idx1],\n",
    "                train[\"longitude\"].iat[idx1],\n",
    "                train[\"categories\"].iat[idx1],\n",
    "                train[\"category_simpl\"].iat[idx1],\n",
    "            )\n",
    "            for idx2, val in zip(Liste_idx, Liste_val):\n",
    "                if idx1 < idx2:\n",
    "                    id2, lat2, lon2, cat2, cat_simpl2 = (\n",
    "                        train[\"id\"].iat[idx2],\n",
    "                        train[\"latitude\"].iat[idx2],\n",
    "                        train[\"longitude\"].iat[idx2],\n",
    "                        train[\"categories\"].iat[idx2],\n",
    "                        train[\"category_simpl\"].iat[idx2],\n",
    "                    )\n",
    "                    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "                    dist = haversine(lat1, lon1, lat2, lon2)\n",
    "                    same_cat = (cat_simpl1 == cat_simpl2 and cat_simpl1 > 0) or (\n",
    "                        cat1 == cat2 and cat1 != \"\" and cat1 != \"nan\"\n",
    "                    )\n",
    "                    if (\n",
    "                        key not in Cand\n",
    "                        and dist <= thr_distance\n",
    "                        and (same_cat or dist <= thr_distance_or_same_cat)\n",
    "                        and (same_cat or val >= thr_tfidf)\n",
    "                    ):\n",
    "                        poi1, poi2 = (\n",
    "                            train[\"point_of_interest\"].iat[idx1],\n",
    "                            train[\"point_of_interest\"].iat[idx2],\n",
    "                        )\n",
    "                        Cand[key] = int(poi1 == poi2)\n",
    "                        Added_p1.append([id1, poi1])\n",
    "                        Added_p2.append([id2, poi2])\n",
    "                        nb_true_matchs_initial += int(poi1 == poi2)\n",
    "\n",
    "        p1 = (\n",
    "            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        p2 = (\n",
    "            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        print(f\"Candidates added : {len(p1)-size1}/{len(p1)}.\")\n",
    "\n",
    "print(\"\\n-> TF-IDF for contries finished.\")\n",
    "print(f\"Candidates added : {len(p1)-size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF nÂ°3b : for each countries (with few processed name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_tfidf_ = 0.45\n",
    "thr_distance_ = 25\n",
    "thr_distance_or_same_cat_ = 10\n",
    "\n",
    "size = len(p1)\n",
    "\n",
    "for country in [32]:  # range(1, 30)\n",
    "\n",
    "    # Reset parameter\n",
    "    thr_tfidf = thr_tfidf_\n",
    "    thr_distance = thr_distance_\n",
    "    thr_distance_or_same_cat = thr_distance_or_same_cat_\n",
    "\n",
    "    # Tune parameters for each country\n",
    "    if country == 32:\n",
    "        thr_tfidf_ = 0.4\n",
    "        thr_distance = 100  # no limit\n",
    "        thr_distance_or_same_cat = 100  # no limit\n",
    "\n",
    "    Names = train[train[\"country\"] == country][\"name_initial_decode\"].copy()\n",
    "    if len(Names) == 0:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    print(\"#\" * 20)\n",
    "    print(f\"# Country nÂ°{country} : {COUNTRIES[country-1]}.\")\n",
    "\n",
    "    Names_numrow = {\n",
    "        i: idx for i, idx in enumerate(Names.index)\n",
    "    }  # Keep initial row number\n",
    "    Names = Names.to_list()\n",
    "\n",
    "    print(f\"Len names : {len(Names)}.\")\n",
    "\n",
    "    # Tf-idf\n",
    "    if 1 < len(Names) < 400000:\n",
    "        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=min(0.4, thr_tfidf))\n",
    "\n",
    "        # no self-matchs and retrieve the initial row number\n",
    "        Tfidf_idx = [[Names_numrow[x] for x in L] for L in Tfidf_idx]\n",
    "\n",
    "        # Add matches : /!\\ ONLY IF THERE IS A CATEGORY MATCH AND THE DISTANCE IS NOT TOO BIG\n",
    "        size1 = len(p1)\n",
    "        Added_p1, Added_p2 = [], []\n",
    "        for idx1, (Liste_idx, Liste_val) in enumerate(zip(Tfidf_idx, Tfidf_val)):\n",
    "            idx1 = Names_numrow[idx1]\n",
    "            id1, lat1, lon1, cat1, cat_simpl1 = (\n",
    "                train[\"id\"].iat[idx1],\n",
    "                train[\"latitude\"].iat[idx1],\n",
    "                train[\"longitude\"].iat[idx1],\n",
    "                train[\"categories\"].iat[idx1],\n",
    "                train[\"category_simpl\"].iat[idx1],\n",
    "            )\n",
    "            for idx2, val in zip(Liste_idx, Liste_val):\n",
    "                if idx1 < idx2:\n",
    "                    id2, lat2, lon2, cat2, cat_simpl2 = (\n",
    "                        train[\"id\"].iat[idx2],\n",
    "                        train[\"latitude\"].iat[idx2],\n",
    "                        train[\"longitude\"].iat[idx2],\n",
    "                        train[\"categories\"].iat[idx2],\n",
    "                        train[\"category_simpl\"].iat[idx2],\n",
    "                    )\n",
    "                    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "                    dist = haversine(lat1, lon1, lat2, lon2)\n",
    "                    same_cat = (cat_simpl1 == cat_simpl2 and cat_simpl1 > 0) or (\n",
    "                        cat1 == cat2 and cat1 != \"\" and cat1 != \"nan\"\n",
    "                    )\n",
    "                    if (\n",
    "                        key not in Cand\n",
    "                        and dist <= thr_distance\n",
    "                        and (same_cat or dist <= thr_distance_or_same_cat)\n",
    "                        and (same_cat or val >= thr_tfidf)\n",
    "                    ):\n",
    "                        poi1, poi2 = (\n",
    "                            train[\"point_of_interest\"].iat[idx1],\n",
    "                            train[\"point_of_interest\"].iat[idx2],\n",
    "                        )\n",
    "                        Cand[key] = int(poi1 == poi2)\n",
    "                        Added_p1.append([id1, poi1])\n",
    "                        Added_p2.append([id2, poi2])\n",
    "                        nb_true_matchs_initial += int(poi1 == poi2)\n",
    "\n",
    "        p1 = (\n",
    "            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        p2 = (\n",
    "            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        print(f\"Candidates added : {len(p1)-size1}.\")\n",
    "\n",
    "print(\"\\n-> TF-IDF for contries finished.\")\n",
    "print(f\"Candidates added : {len(p1)-size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add candidates based on same name/phone/address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation d'un df de travail\n",
    "work = train.copy()\n",
    "\n",
    "# Prepare columns\n",
    "for c in [\"name\", \"address\", \"city\"]:\n",
    "    work[c] = work[c].astype(str).str.lower()\n",
    "work[\"index\"] = work.index\n",
    "\n",
    "\n",
    "work_names = work.groupby(\"name\")[\"index\"].apply(list).to_frame().reset_index()\n",
    "work_names = dict(zip(work_names[\"name\"], work_names[\"index\"]))\n",
    "work_names = {\n",
    "    name: Liste_idx\n",
    "    for name, Liste_idx in work_names.items()\n",
    "    if len(name) >= 2 and len(Liste_idx) <= 25\n",
    "}  # Don't consider too widespread names\n",
    "\n",
    "\n",
    "work_phones = work.groupby(\"phone\")[\"index\"].apply(list).to_frame().reset_index()\n",
    "work_phones = dict(zip(work_phones[\"phone\"], work_phones[\"index\"]))\n",
    "work_phones = {\n",
    "    phone: Liste_idx\n",
    "    for phone, Liste_idx in work_phones.items()\n",
    "    if len(phone) >= 3 and len(Liste_idx) <= 10\n",
    "}  # Don't consider too widespread phone\n",
    "\n",
    "\n",
    "work[\"address_complet\"] = work.apply(\n",
    "    lambda row: create_address(row[\"address\"], row[\"city\"]), axis=1\n",
    ")\n",
    "work_address = (\n",
    "    work.groupby(\"address_complet\")[\"index\"].apply(list).to_frame().reset_index()\n",
    ")\n",
    "work_address = dict(zip(work_address[\"address_complet\"], work_address[\"index\"]))\n",
    "work_address = {\n",
    "    address: Liste_idx\n",
    "    for address, Liste_idx in work_address.items()\n",
    "    if len(address) >= 3 and len(Liste_idx) <= 10\n",
    "}  # Don't consider too widespread address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process\n",
    "# tqdm.pandas()\n",
    "# Potential_on_NamePhone = work.progress_apply(find_potential_matchs, axis=1).to_list()\n",
    "Potential_on_NamePhone = work.apply(\n",
    "    lambda row: find_potential_matchs(row, work_names, work_phones, work_address),\n",
    "    axis=1,\n",
    ").to_list()\n",
    "\n",
    "# Don't keep pairs too far from each other\n",
    "Potential_on_NamePhone_new = []\n",
    "\n",
    "# Numpy for faster process\n",
    "train_numpy = train[[\"name\", \"latitude\", \"longitude\", \"category_simpl\"]].to_numpy()\n",
    "\n",
    "# Filtre on dist\n",
    "for i, Liste_idx in enumerate(Potential_on_NamePhone):\n",
    "    new = []\n",
    "    name1, lat1, lon1, cat_simpl1 = (\n",
    "        train_numpy[i][0],\n",
    "        train_numpy[i][1],\n",
    "        train_numpy[i][2],\n",
    "        train_numpy[i][3],\n",
    "    )\n",
    "    for j, row in enumerate(train_numpy[Liste_idx]):\n",
    "        name2, lat2, lon2, cat_simpl2 = row[0], row[1], row[2], row[3]\n",
    "\n",
    "        # if rare name, we are more tolerant\n",
    "        if name1 == name2 and len(work_names[name1]) <= 5:\n",
    "            thr_distance = 100\n",
    "        else:\n",
    "            thr_distance = 26\n",
    "\n",
    "        # if the category is usually far even for matchs\n",
    "        if (cat_simpl1 in far_cat_simpl) or (cat_simpl2 in far_cat_simpl):\n",
    "            thr_distance = 350\n",
    "            if (cat_simpl1 == 1) or (cat_simpl2 == 1):\n",
    "                thr_distance = 100000  # no limit\n",
    "\n",
    "        # Add distance if long names (not a coincidence if they are equal)\n",
    "        if name1 == name2 and len(name1) >= 10:\n",
    "            thr_distance += 15\n",
    "\n",
    "        # Process\n",
    "        if haversine(lat1, lon1, lat2, lon2) > thr_distance:\n",
    "            continue\n",
    "        else:\n",
    "            new.append(Liste_idx[j])\n",
    "    Potential_on_NamePhone_new.append(new.copy())\n",
    "\n",
    "Potential_on_NamePhone = Potential_on_NamePhone_new.copy()\n",
    "\n",
    "del Potential_on_NamePhone_new, train_numpy\n",
    "gc.collect()\n",
    "\n",
    "# Number of potential matchs\n",
    "print(\n",
    "    f\"Potential match on name/phone/address : {sum(len(x) for x in Potential_on_NamePhone)}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add matches\n",
    "size1 = len(p1)\n",
    "Added_p1, Added_p2 = [], []\n",
    "try:\n",
    "    seen\n",
    "except:\n",
    "    seen = set()\n",
    "for idx1, Liste_idx in enumerate(Potential_on_NamePhone):\n",
    "\n",
    "    id1, lat1, lon1, cat1, cat_simpl1 = (\n",
    "        train[\"id\"].iat[idx1],\n",
    "        train[\"latitude\"].iat[idx1],\n",
    "        train[\"longitude\"].iat[idx1],\n",
    "        train[\"categories\"].iat[idx1],\n",
    "        train[\"category_simpl\"].iat[idx1],\n",
    "    )\n",
    "    for idx2 in Liste_idx:\n",
    "        if idx1 != idx2:\n",
    "            id2, lat2, lon2, cat2, cat_simpl2 = (\n",
    "                train[\"id\"].iat[idx2],\n",
    "                train[\"latitude\"].iat[idx2],\n",
    "                train[\"longitude\"].iat[idx2],\n",
    "                train[\"categories\"].iat[idx2],\n",
    "                train[\"category_simpl\"].iat[idx2],\n",
    "            )\n",
    "            key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "            # same_cat = (cat_simpl1==cat_simpl2 and cat_simpl1>0) or (cat1==cat2 and cat1!='')\n",
    "            if key not in Cand and key not in seen:\n",
    "                seen.add(key)\n",
    "                poi1, poi2 = (\n",
    "                    train[\"point_of_interest\"].iat[idx1],\n",
    "                    train[\"point_of_interest\"].iat[idx2],\n",
    "                )\n",
    "                Added_p1.append([id1, poi1])\n",
    "                Added_p2.append([id2, poi2])\n",
    "\n",
    "p1 = (\n",
    "    p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "    .reset_index(drop=True)\n",
    "    .copy()\n",
    ")\n",
    "p2 = (\n",
    "    p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "    .reset_index(drop=True)\n",
    "    .copy()\n",
    ")\n",
    "print(f\"Candidates added with name/phone/address similarity : {len(p1)-size1}.\")\n",
    "\n",
    "del (\n",
    "    Tfidf_idx,\n",
    "    Tfidf_val,\n",
    "    Cand,\n",
    "    ID_to_POI,\n",
    "    Potential_on_NamePhone,\n",
    "    work,\n",
    "    work_names,\n",
    "    work_phones,\n",
    "    work_address,\n",
    "    Added_p1,\n",
    "    Added_p2,\n",
    "    seen,\n",
    "    Names_numrow,\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index after Vincent's candidate addition\n",
    "p1 = p1.reset_index(drop=True)\n",
    "p2 = p2.reset_index(drop=True)\n",
    "\n",
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    if IS_TEST:\n",
    "        p1.to_csv(OUT_PATH + \"p1_yv_test.csv\", index=False)\n",
    "        p2.to_csv(OUT_PATH + \"p2_yv_test.csv\", index=False)\n",
    "    else:\n",
    "        p1.to_csv(OUT_PATH + \"p1_yv_train.csv\", index=False)\n",
    "        p2.to_csv(OUT_PATH + \"p2_yv_train.csv\", index=False)\n",
    "\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_yv = p1.copy()\n",
    "p2_yv = p2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from numerize.numerize import numerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.preparation import prepare_train_data, prepare_triplet_data\n",
    "from data.dataset import SingleDataset\n",
    "from data.tokenization import get_tokenizer\n",
    "\n",
    "from model_zoo.models import SingleTransformer\n",
    "\n",
    "from utils.logger import Config\n",
    "from utils.torch import load_model_weights\n",
    "from utils.metrics import *\n",
    "\n",
    "from inference.predict import predict\n",
    "from inference.knn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = prepare_train_data(root=DATA_PATH)\n",
    "folds = pd.read_csv(DATA_PATH + \"folds_2.csv\")[[\"id\", \"fold\"]]\n",
    "df = df.merge(folds, how=\"left\", on=\"id\").set_index(\"id\")\n",
    "\n",
    "FOLD = 0\n",
    "df = df[df[\"fold\"] == FOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tmp = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "\n",
    "# poi_mapping = {\n",
    "#     p: m\n",
    "#     for p, m in zip(\n",
    "#         train_tmp[\"point_of_interest\"],\n",
    "#         train_tmp[\"point_of_interest\"].astype(\"category\").cat.codes,\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# del train_tmp\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = LOG_PATH + \"2022-05-19/2/\"  # 1 ep, d=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", \"r\")))\n",
    "\n",
    "tokenizer = get_tokenizer(config.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(EXP_FOLDER + f\"fts_val_{FOLD}.npy\"):\n",
    "    preds = np.load(EXP_FOLDER + f\"fts_val_{FOLD}.npy\")\n",
    "else:\n",
    "    dataset = SingleDataset(df, tokenizer, config.max_len, use_url=False)\n",
    "\n",
    "    weights = sorted(glob.glob(EXP_FOLDER + \"*.pt\"))\n",
    "\n",
    "    model = SingleTransformer(\n",
    "        config.name,\n",
    "        nb_layers=config.nb_layers,\n",
    "        no_dropout=config.no_dropout,\n",
    "        embed_dim=config.embed_dim,\n",
    "        nb_features=config.nb_features,\n",
    "    ).cuda()\n",
    "    model.zero_grad()\n",
    "\n",
    "    model = load_model_weights(model, weights[FOLD])\n",
    "\n",
    "    preds = predict(model, dataset, config.data_config)\n",
    "    np.save(EXP_FOLDER + f\"fts_val_{FOLD}.npy\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_matches = json.load(open(DATA_PATH + \"gt.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [100]:\n",
    "    # for n_neighbors in [50, 100]:\n",
    "    print(f\"\\n- -> n_neighbors={n_neighbors}\\n\")\n",
    "\n",
    "    nn_matches = find_matches(preds, df, n_neighbors)\n",
    "\n",
    "    naive_matches = json.load(\n",
    "        open(OUT_PATH + f\"dist_matches_{n_neighbors}_0.json\", \"r\")\n",
    "    )\n",
    "\n",
    "    # UNION\n",
    "    merged_matches = {\n",
    "        k: list(set(naive_matches[k] + nn_matches[k])) for k in nn_matches\n",
    "    }\n",
    "\n",
    "    df_pairs = create_pairs(nn_matches, naive_matches, n_neighbors, gt_matches)\n",
    "    prop = df_pairs[\"match\"].sum() / len(df_pairs) * 100\n",
    "    save_path = EXP_FOLDER + f\"df_pairs_{n_neighbors}.csv\"\n",
    "\n",
    "    if SAVE:\n",
    "        df_pairs.to_csv(save_path, index=False)\n",
    "        print(f\"-> Saved pairs to {save_path} - Positive proportion  {prop:.2f}%\\n\")\n",
    "    else:\n",
    "        print(f\"Positive proportion  {prop:.2f}%\\n\")\n",
    "\n",
    "    # INTERSECTION\n",
    "    merged_matches = {\n",
    "        k: list(set(naive_matches[k]).intersection(nn_matches[k])) for k in nn_matches\n",
    "    }\n",
    "    found_prop, missed = compute_found_prop(merged_matches, gt_matches)\n",
    "    n_matches = sum([len(merged_matches[k]) for k in merged_matches])\n",
    "    print(\"Merged matches - Intersection :\")\n",
    "    print(\n",
    "        f\"Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.\"\n",
    "    )\n",
    "    print(f\"Best reachable IoU : {compute_best_iou(merged_matches, gt_matches) :.3f}\")\n",
    "\n",
    "    df_pairs_i = df_pairs[\n",
    "        (df_pairs[\"rank\"] >= -0.5) & (df_pairs[\"rank_nn\"] >= -0.5)\n",
    "    ].reset_index(drop=True)\n",
    "    prop = df_pairs_i[\"match\"].sum() / len(df_pairs_i) * 100\n",
    "\n",
    "    if SAVE:\n",
    "        df_pairs_i.to_csv(save_path, index=False)\n",
    "        print(f\"-> Saved pairs to {save_path} - Positive proportion  {prop:.2f}%\\n\")\n",
    "    else:\n",
    "        print(f\"Positive proportion  {prop:.2f}%\\n\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pairs_used = df_pairs[df_pairs[\"rank_nn\"] >= 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs_used = df_pairs_i.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1_theo = (\n",
    "    (\n",
    "        df_pairs_used[[\"id_1\"]]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"id_1\": \"id\"})\n",
    "        .merge(df.reset_index()[[\"id\", \"point_of_interest\"]])\n",
    "    )\n",
    "    .sort_values(\"index\")\n",
    "    .set_index(\"index\")\n",
    ")\n",
    "\n",
    "p2_theo = (\n",
    "    (\n",
    "        df_pairs_used[[\"id_2\"]]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"id_2\": \"id\"})\n",
    "        .merge(df.reset_index()[[\"id\", \"point_of_interest\"]])\n",
    "    )\n",
    "    .sort_values(\"index\")\n",
    "    .set_index(\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_theo[\"point_of_interest\"] = p1_theo[\"point_of_interest\"].map(poi_mapping)\n",
    "p2_theo[\"point_of_interest\"] = p2_theo[\"point_of_interest\"].map(poi_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1_theo, p2_theo, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_CV(\n",
    "    p1_theo,\n",
    "    p2_theo,\n",
    "    np.array(p1_theo[\"point_of_interest\"] == p2_theo[\"point_of_interest\"]).astype(\n",
    "        np.int8\n",
    "    ),\n",
    "    np.array(p1_theo[\"point_of_interest\"] == p2_theo[\"point_of_interest\"]).astype(\n",
    "        np.int8\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index after Vincent's candidate addition\n",
    "p1 = pd.concat([p1_final, p1_theo]).reset_index(drop=True)\n",
    "p2 = pd.concat([p2_final, p2_theo]).reset_index(drop=True)\n",
    "\n",
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx\n",
    "gc.collect()\n",
    "\n",
    "y = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "print(\n",
    "    \"removed duplicates\",\n",
    "    p1.shape[0],\n",
    "    (p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).sum(),\n",
    "    int(time.time() - start_time),\n",
    "    \"sec\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter n=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
