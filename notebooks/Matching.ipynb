{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/theo/kaggle/foursquare/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113095,
     "end_time": "2022-06-20T12:41:41.215410",
     "exception": false,
     "start_time": "2022-06-20T12:41:41.102315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-20T12:42:48.194197Z",
     "iopub.status.busy": "2022-06-20T12:42:48.193938Z",
     "iopub.status.idle": "2022-06-20T12:42:50.735561Z",
     "shell.execute_reply": "2022-06-20T12:42:50.734610Z"
    },
    "papermill": {
     "duration": 2.672311,
     "end_time": "2022-06-20T12:42:50.737847",
     "exception": false,
     "start_time": "2022-06-20T12:42:48.065536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import gc\\nimport random\\nimport warnings\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm.auto import tqdm\\n\\nfrom params import DEBUG, OUT_PATH, IS_TEST\\nfrom ressources import *\\nfrom matching import *\\n\\nrandom.seed(13)\\n# warnings.simplefilter(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"import gc\\nimport random\\nimport warnings\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm.auto import tqdm\\n\\nfrom params import DEBUG, OUT_PATH, IS_TEST\\nfrom ressources import *\\nfrom matching import *\\n\\nrandom.seed(13)\\n# warnings.simplefilter(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from params import DEBUG, OUT_PATH, IS_TEST\n",
    "from ressources import *\n",
    "from matching import *\n",
    "\n",
    "random.seed(13)\n",
    "# warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.128224,
     "end_time": "2022-06-20T12:42:52.888296",
     "exception": false,
     "start_time": "2022-06-20T12:42:52.760072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"if IS_TEST:\\n    train = load_cleaned_data(OUT_PATH + \\\"cleaned_data_test.csv\\\")\\nelse:\\n    train = load_cleaned_data(OUT_PATH + \\\"cleaned_data_train.csv\\\")\";\n",
       "                var nbb_formatted_code = \"if IS_TEST:\\n    train = load_cleaned_data(OUT_PATH + \\\"cleaned_data_test.csv\\\")\\nelse:\\n    train = load_cleaned_data(OUT_PATH + \\\"cleaned_data_train.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if IS_TEST:\n",
    "    train = load_cleaned_data(OUT_PATH + \"cleaned_data_test.csv\")\n",
    "else:\n",
    "    train = load_cleaned_data(OUT_PATH + \"cleaned_data_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"if DEBUG:\\n    train = train.head(10000)\";\n",
       "                var nbb_formatted_code = \"if DEBUG:\\n    train = train.head(10000)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    train = train.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713788\n",
      "\n",
      "Number of candidates : 713.79K\n",
      "Proportion of positive candidates: 100.00%\n",
      "Proportion of found matches: 100.00%\n",
      "\n",
      "\n",
      "Number of candidates : 356.89K\n",
      "Proportion of positive candidates: 100.00%\n",
      "Proportion of found matches: 50.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"if not IS_TEST:\\n    clusts = (\\n        train[[\\\"id\\\", \\\"point_of_interest\\\"]]\\n        .groupby(\\\"point_of_interest\\\")\\n        .agg(list)\\n        .reset_index()\\n    )\\n    clusts = clusts[clusts[\\\"id\\\"].apply(lambda x: len(x) > 1)]\\n\\n    N_TO_FIND = clusts[\\\"id\\\"].apply(lambda x: len(x)).sum()\\n    print(N_TO_FIND)\\n\\n    example = clusts.explode(\\\"id\\\").sort_values(\\\"id\\\")\\n    example[\\\"y\\\"] = 1\\n\\n    print_infos(example, None, N_TO_FIND)\\n\\n    p1 = example.sample(len(example) // 2)\\n\\n    print_infos(p1, None, N_TO_FIND)\\n\\nelse:\\n    N_TO_FIND = -1\";\n",
       "                var nbb_formatted_code = \"if not IS_TEST:\\n    clusts = (\\n        train[[\\\"id\\\", \\\"point_of_interest\\\"]]\\n        .groupby(\\\"point_of_interest\\\")\\n        .agg(list)\\n        .reset_index()\\n    )\\n    clusts = clusts[clusts[\\\"id\\\"].apply(lambda x: len(x) > 1)]\\n\\n    N_TO_FIND = clusts[\\\"id\\\"].apply(lambda x: len(x)).sum()\\n    print(N_TO_FIND)\\n\\n    example = clusts.explode(\\\"id\\\").sort_values(\\\"id\\\")\\n    example[\\\"y\\\"] = 1\\n\\n    print_infos(example, None, N_TO_FIND)\\n\\n    p1 = example.sample(len(example) // 2)\\n\\n    print_infos(p1, None, N_TO_FIND)\\n\\nelse:\\n    N_TO_FIND = -1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not IS_TEST:\n",
    "    clusts = (\n",
    "        train[[\"id\", \"point_of_interest\"]]\n",
    "        .groupby(\"point_of_interest\")\n",
    "        .agg(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "    clusts = clusts[clusts[\"id\"].apply(lambda x: len(x) > 1)]\n",
    "\n",
    "    N_TO_FIND = clusts[\"id\"].apply(lambda x: len(x)).sum()\n",
    "    print(N_TO_FIND)\n",
    "\n",
    "    example = clusts.explode(\"id\").sort_values(\"id\")\n",
    "    example[\"y\"] = 1\n",
    "\n",
    "    print_infos(example, None, N_TO_FIND)\n",
    "\n",
    "    p1 = example.sample(len(example) // 2)\n",
    "\n",
    "    print_infos(p1, None, N_TO_FIND)\n",
    "\n",
    "else:\n",
    "    N_TO_FIND = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"phone\\\", \\\"lon2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"phone\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"phone\\\", \\\"lon2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\";\n",
       "                var nbb_formatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"phone\\\", \\\"lon2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"phone\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"phone\\\", \\\"lon2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"phone\", \"lon2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"phone\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"phone\", \"lon2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"idx1 = []\\nidx2 = []\\nd = p3[\\\"phone\\\"].to_numpy()\\n\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 11):  # 11 = add 10 sets\\n        if i + j < p3.shape[0] and lcs(d[i], d[i + j]) >= 7:  # accept <=3 digits off\\n            idx1.append(i)\\n            idx2.append(i + j)\\n\\np1 = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2 = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\";\n",
       "                var nbb_formatted_code = \"idx1 = []\\nidx2 = []\\nd = p3[\\\"phone\\\"].to_numpy()\\n\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 11):  # 11 = add 10 sets\\n        if i + j < p3.shape[0] and lcs(d[i], d[i + j]) >= 7:  # accept <=3 digits off\\n            idx1.append(i)\\n            idx2.append(i + j)\\n\\np1 = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2 = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"phone\"].to_numpy()\n",
    "\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 11):  # 11 = add 10 sets\n",
    "        if i + j < p3.shape[0] and lcs(d[i], d[i + j]) >= 7:  # accept <=3 digits off\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "\n",
    "p1 = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2 = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 499.22K\n",
      "Proportion of positive candidates: 9.90%\n",
      "Proportion of found matches: 13.20%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lat / lon 22m² square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# lat/lon, rounded to 2 x 4 digits = 22* meters square; there should not be too many false positives this close to each other\\n# do this in 4 blocks, shifted by 1/2 size, to avoid cut-offs\\nfor s1 in [0, 5e-5]:\\n    for s2 in [0, 5e-5]:\\n        p3 = train[\\n            [\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"latitude\\\", \\\"longitude\\\"]\\n        ].copy()\\n        p3[\\\"latitude\\\"] = np.round(s1 + 0.5 * p3[\\\"latitude\\\"], 4)  # rounded to 4 digits\\n        p3[\\\"longitude\\\"] = np.round(\\n            s2 + 0.5 * p3[\\\"longitude\\\"] / np.cos(p3[\\\"latitude\\\"] * 3.14 / 180.0), 4\\n        )  # rounded to 4 digits\\n        p3 = p3.sort_values(by=[\\\"country\\\", \\\"latitude\\\", \\\"longitude\\\", \\\"id\\\"]).reset_index(\\n            drop=True\\n        )\\n        idx1 = []\\n        idx2 = []\\n        lat, lon = p3[\\\"latitude\\\"].to_numpy(), p3[\\\"longitude\\\"].to_numpy()\\n        for i in range(p3.shape[0] - 1):\\n            for j in range(1, 5):  # 5 = add 4 sets\\n                if (\\n                    i + j < p3.shape[0]\\n                    and lat[i] == lat[i + j]\\n                    and lon[i] == lon[i + j]\\n                ):\\n                    idx1.append(i)\\n                    idx2.append(i + j)\\n        p1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\n        p2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n        # append\\n        p1 = p1.append(p1a, ignore_index=True)\\n        p2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_formatted_code = \"# lat/lon, rounded to 2 x 4 digits = 22* meters square; there should not be too many false positives this close to each other\\n# do this in 4 blocks, shifted by 1/2 size, to avoid cut-offs\\nfor s1 in [0, 5e-5]:\\n    for s2 in [0, 5e-5]:\\n        p3 = train[\\n            [\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"latitude\\\", \\\"longitude\\\"]\\n        ].copy()\\n        p3[\\\"latitude\\\"] = np.round(s1 + 0.5 * p3[\\\"latitude\\\"], 4)  # rounded to 4 digits\\n        p3[\\\"longitude\\\"] = np.round(\\n            s2 + 0.5 * p3[\\\"longitude\\\"] / np.cos(p3[\\\"latitude\\\"] * 3.14 / 180.0), 4\\n        )  # rounded to 4 digits\\n        p3 = p3.sort_values(by=[\\\"country\\\", \\\"latitude\\\", \\\"longitude\\\", \\\"id\\\"]).reset_index(\\n            drop=True\\n        )\\n        idx1 = []\\n        idx2 = []\\n        lat, lon = p3[\\\"latitude\\\"].to_numpy(), p3[\\\"longitude\\\"].to_numpy()\\n        for i in range(p3.shape[0] - 1):\\n            for j in range(1, 5):  # 5 = add 4 sets\\n                if (\\n                    i + j < p3.shape[0]\\n                    and lat[i] == lat[i + j]\\n                    and lon[i] == lon[i + j]\\n                ):\\n                    idx1.append(i)\\n                    idx2.append(i + j)\\n        p1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\n        p2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n        # append\\n        p1 = p1.append(p1a, ignore_index=True)\\n        p2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lat/lon, rounded to 2 x 4 digits = 22* meters square; there should not be too many false positives this close to each other\n",
    "# do this in 4 blocks, shifted by 1/2 size, to avoid cut-offs\n",
    "for s1 in [0, 5e-5]:\n",
    "    for s2 in [0, 5e-5]:\n",
    "        p3 = train[\n",
    "            [\"country\", \"id\", \"point_of_interest\", \"latitude\", \"longitude\"]\n",
    "        ].copy()\n",
    "        p3[\"latitude\"] = np.round(s1 + 0.5 * p3[\"latitude\"], 4)  # rounded to 4 digits\n",
    "        p3[\"longitude\"] = np.round(\n",
    "            s2 + 0.5 * p3[\"longitude\"] / np.cos(p3[\"latitude\"] * 3.14 / 180.0), 4\n",
    "        )  # rounded to 4 digits\n",
    "        p3 = p3.sort_values(by=[\"country\", \"latitude\", \"longitude\", \"id\"]).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        idx1 = []\n",
    "        idx2 = []\n",
    "        lat, lon = p3[\"latitude\"].to_numpy(), p3[\"longitude\"].to_numpy()\n",
    "        for i in range(p3.shape[0] - 1):\n",
    "            for j in range(1, 5):  # 5 = add 4 sets\n",
    "                if (\n",
    "                    i + j < p3.shape[0]\n",
    "                    and lat[i] == lat[i + j]\n",
    "                    and lon[i] == lon[i + j]\n",
    "                ):\n",
    "                    idx1.append(i)\n",
    "                    idx2.append(i + j)\n",
    "        p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "        p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "        # append\n",
    "        p1 = p1.append(p1a, ignore_index=True)\n",
    "        p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 1.09M\n",
      "Proportion of positive candidates: 18.47%\n",
      "Proportion of found matches: 27.03%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"url\\\", \\\"lon2\\\", \\\"lat2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"url\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"url\\\", \\\"lon2\\\", \\\"lat2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"url\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 2):  # 2 = add 1 set\\n        if i + j < p3.shape[0] and ll_lcs(d[i], d[i + j], 3) >= 7:  # ll_lcs(3) >= 7\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_formatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"url\\\", \\\"lon2\\\", \\\"lat2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"url\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"url\\\", \\\"lon2\\\", \\\"lat2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"url\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 2):  # 2 = add 1 set\\n        if i + j < p3.shape[0] and ll_lcs(d[i], d[i + j], 3) >= 7:  # ll_lcs(3) >= 7\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"url\", \"lon2\", \"lat2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"url\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"url\", \"lon2\", \"lat2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"url\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 2):  # 2 = add 1 set\n",
    "        if i + j < p3.shape[0] and ll_lcs(d[i], d[i + j], 3) >= 7:  # ll_lcs(3) >= 7\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 1.28M\n",
      "Proportion of positive candidates: 17.31%\n",
      "Proportion of found matches: 28.22%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"categories\\\", \\\"lon2\\\", \\\"lat2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"categories\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"categories\\\", \\\"lon2\\\", \\\"lat2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"categories\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 2):  # 2 = add 1 set\\n        if i + j < p3.shape[0] and d[i][:4] == d[i + j][:4]:  # match on first 4 leters\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_formatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"categories\\\", \\\"lon2\\\", \\\"lat2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"categories\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"categories\\\", \\\"lon2\\\", \\\"lat2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"categories\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 2):  # 2 = add 1 set\\n        if i + j < p3.shape[0] and d[i][:4] == d[i + j][:4]:  # match on first 4 leters\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"categories\", \"lon2\", \"lat2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"categories\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"categories\", \"lon2\", \"lat2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"categories\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 2):  # 2 = add 1 set\n",
    "        if i + j < p3.shape[0] and d[i][:4] == d[i + j][:4]:  # match on first 4 leters\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 2.33M\n",
      "Proportion of positive candidates: 11.86%\n",
      "Proportion of found matches: 38.25%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"address\\\", \\\"lon2\\\", \\\"lat2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"address\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"address\\\", \\\"lon2\\\", \\\"lat2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"address\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 7):  # 7 = add 6 sets\\n        if i + j < p3.shape[0] and lcs2(d[i], d[i + j]) >= 6:  # lcs2 >= 6\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_formatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"address\\\", \\\"lon2\\\", \\\"lat2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"address\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"address\\\", \\\"lon2\\\", \\\"lat2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"address\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 7):  # 7 = add 6 sets\\n        if i + j < p3.shape[0] and lcs2(d[i], d[i + j]) >= 6:  # lcs2 >= 6\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"address\", \"lon2\", \"lat2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"address\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"address\", \"lon2\", \"lat2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"address\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 7):  # 7 = add 6 sets\n",
    "        if i + j < p3.shape[0] and lcs2(d[i], d[i + j]) >= 6:  # lcs2 >= 6\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 4.28M\n",
      "Proportion of positive candidates: 8.79%\n",
      "Proportion of found matches: 47.90%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# name\\np3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"name\\\", \\\"lon2\\\", \\\"lat2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"name\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"name\\\", \\\"lon2\\\", \\\"lat2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"name\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 4):  # 4 = add 3 sets\\n        if i + j < p3.shape[0] and lcs2(d[i], d[i + j]) >= 5:  # lcs2 >= 5\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_formatted_code = \"# name\\np3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"name\\\", \\\"lon2\\\", \\\"lat2\\\"]].copy()\\np3 = (\\n    p3.loc[p3[\\\"name\\\"] != \\\"\\\"]\\n    .sort_values(by=[\\\"country\\\", \\\"name\\\", \\\"lon2\\\", \\\"lat2\\\", \\\"id\\\"])\\n    .reset_index(drop=True)\\n)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"name\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 4):  # 4 = add 3 sets\\n        if i + j < p3.shape[0] and lcs2(d[i], d[i + j]) >= 5:  # lcs2 >= 5\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# name\n",
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"name\", \"lon2\", \"lat2\"]].copy()\n",
    "p3 = (\n",
    "    p3.loc[p3[\"name\"] != \"\"]\n",
    "    .sort_values(by=[\"country\", \"name\", \"lon2\", \"lat2\", \"id\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"name\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 4):  # 4 = add 3 sets\n",
    "        if i + j < p3.shape[0] and lcs2(d[i], d[i + j]) >= 5:  # lcs2 >= 5\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 6.48M\n",
      "Proportion of positive candidates: 9.95%\n",
      "Proportion of found matches: 75.50%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"name\\\", \\\"latitude\\\"]].copy()\\np3 = p3.sort_values(by=[\\\"country\\\", \\\"latitude\\\", \\\"id\\\"]).reset_index(drop=True)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"latitude\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 21):  # 21 = add 20 sets\\n        if i + j < p3.shape[0] and d[i] == d[i + j]:  # exact match\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_formatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"name\\\", \\\"latitude\\\"]].copy()\\np3 = p3.sort_values(by=[\\\"country\\\", \\\"latitude\\\", \\\"id\\\"]).reset_index(drop=True)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"latitude\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 21):  # 21 = add 20 sets\\n        if i + j < p3.shape[0] and d[i] == d[i + j]:  # exact match\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"name\", \"latitude\"]].copy()\n",
    "p3 = p3.sort_values(by=[\"country\", \"latitude\", \"id\"]).reset_index(drop=True)\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"latitude\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 21):  # 21 = add 20 sets\n",
    "        if i + j < p3.shape[0] and d[i] == d[i + j]:  # exact match\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 6.88M\n",
      "Proportion of positive candidates: 9.59%\n",
      "Proportion of found matches: 75.74%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"name\\\", \\\"longitude\\\"]].copy()\\np3 = p3.sort_values(by=[\\\"country\\\", \\\"longitude\\\", \\\"id\\\"]).reset_index(drop=True)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"longitude\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 21):  # 21 = add 20 sets\\n        if i + j < p3.shape[0] and d[i] == d[i + j]:  # exact match\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_formatted_code = \"p3 = train[[\\\"country\\\", \\\"id\\\", \\\"point_of_interest\\\", \\\"name\\\", \\\"longitude\\\"]].copy()\\np3 = p3.sort_values(by=[\\\"country\\\", \\\"longitude\\\", \\\"id\\\"]).reset_index(drop=True)\\nidx1 = []\\nidx2 = []\\nd = p3[\\\"longitude\\\"].to_numpy()\\nfor i in range(p3.shape[0] - 1):\\n    for j in range(1, 21):  # 21 = add 20 sets\\n        if i + j < p3.shape[0] and d[i] == d[i + j]:  # exact match\\n            idx1.append(i)\\n            idx2.append(i + j)\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n# append\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p3 = train[[\"country\", \"id\", \"point_of_interest\", \"name\", \"longitude\"]].copy()\n",
    "p3 = p3.sort_values(by=[\"country\", \"longitude\", \"id\"]).reset_index(drop=True)\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "d = p3[\"longitude\"].to_numpy()\n",
    "for i in range(p3.shape[0] - 1):\n",
    "    for j in range(1, 21):  # 21 = add 20 sets\n",
    "        if i + j < p3.shape[0] and d[i] == d[i + j]:  # exact match\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "# append\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 7.26M\n",
      "Proportion of positive candidates: 9.30%\n",
      "Proportion of found matches: 75.92%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 376643 674852 316187\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"p1[\\\"y\\\"] = np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8)\\na = p1.groupby(\\\"id\\\")[\\\"y\\\"].sum().reset_index()\\nprint(\\n    \\\"Added\\\",\\n    p1a.shape[0],\\n    p1[\\\"y\\\"].sum(),\\n    np.minimum(1, a[\\\"y\\\"]).sum(),\\n)\";\n",
       "                var nbb_formatted_code = \"p1[\\\"y\\\"] = np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8)\\na = p1.groupby(\\\"id\\\")[\\\"y\\\"].sum().reset_index()\\nprint(\\n    \\\"Added\\\",\\n    p1a.shape[0],\\n    p1[\\\"y\\\"].sum(),\\n    np.minimum(1, a[\\\"y\\\"]).sum(),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1[\"y\"] = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "a = p1.groupby(\"id\")[\"y\"].sum().reset_index()\n",
    "print(\n",
    "    \"Added\",\n",
    "    p1a.shape[0],\n",
    "    p1[\"y\"].sum(),\n",
    "    np.minimum(1, a[\"y\"]).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name lcs\n",
    "- Slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"p3 = train[\\n    [\\n        \\\"country\\\",\\n        \\\"id\\\",\\n        \\\"point_of_interest\\\",\\n        \\\"name\\\",\\n        \\\"latitude\\\",\\n        \\\"longitude\\\",\\n        \\\"categories\\\",\\n    ]\\n].copy()\\n\\n# rounded coordinates\\np3[\\\"latitude\\\"] = np.round(p3[\\\"latitude\\\"], 1).astype(\\n    \\\"float32\\\"\\n)  # rounding: 1=10Km, 2=1Km\\np3[\\\"longitude\\\"] = np.round(p3[\\\"longitude\\\"], 1).astype(\\\"float32\\\")\\n\\np3 = p3.sort_values(\\n    by=[\\\"country\\\", \\\"latitude\\\", \\\"longitude\\\", \\\"categories\\\", \\\"id\\\"]\\n).reset_index(drop=True)\\n\\nidx1 = []\\nidx2 = []\\nnames = p3[\\\"name\\\"].to_numpy()\\nlon2 = p3[\\\"longitude\\\"].to_numpy()\";\n",
       "                var nbb_formatted_code = \"p3 = train[\\n    [\\n        \\\"country\\\",\\n        \\\"id\\\",\\n        \\\"point_of_interest\\\",\\n        \\\"name\\\",\\n        \\\"latitude\\\",\\n        \\\"longitude\\\",\\n        \\\"categories\\\",\\n    ]\\n].copy()\\n\\n# rounded coordinates\\np3[\\\"latitude\\\"] = np.round(p3[\\\"latitude\\\"], 1).astype(\\n    \\\"float32\\\"\\n)  # rounding: 1=10Km, 2=1Km\\np3[\\\"longitude\\\"] = np.round(p3[\\\"longitude\\\"], 1).astype(\\\"float32\\\")\\n\\np3 = p3.sort_values(\\n    by=[\\\"country\\\", \\\"latitude\\\", \\\"longitude\\\", \\\"categories\\\", \\\"id\\\"]\\n).reset_index(drop=True)\\n\\nidx1 = []\\nidx2 = []\\nnames = p3[\\\"name\\\"].to_numpy()\\nlon2 = p3[\\\"longitude\\\"].to_numpy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p3 = train[\n",
    "    [\n",
    "        \"country\",\n",
    "        \"id\",\n",
    "        \"point_of_interest\",\n",
    "        \"name\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"categories\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# rounded coordinates\n",
    "p3[\"latitude\"] = np.round(p3[\"latitude\"], 1).astype(\n",
    "    \"float32\"\n",
    ")  # rounding: 1=10Km, 2=1Km\n",
    "p3[\"longitude\"] = np.round(p3[\"longitude\"], 1).astype(\"float32\")\n",
    "\n",
    "p3 = p3.sort_values(\n",
    "    by=[\"country\", \"latitude\", \"longitude\", \"categories\", \"id\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "idx1 = []\n",
    "idx2 = []\n",
    "names = p3[\"name\"].to_numpy()\n",
    "lon2 = p3[\"longitude\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9867f069e41425cb0cc9574b37f805d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1138811.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"idx1 = []\\nidx2 = []\\n\\nfor i in tqdm(range(p3.shape[0] - 1)):\\n    li = lon2[i]\\n    for j in range(\\n        1, min(300, p3.shape[0] - 1 - i)\\n    ):  # put a limit here - look at no more than X items\\n        if (\\n            li != lon2[i + j]\\n        ):  # if lon matches, lat and country also match - b/c of sorting order\\n            break\\n        if lcs2(names[i], names[i + j]) >= 5:  # lcs2 >= 5\\n            idx1.append(i)\\n            idx2.append(i + j)\\n\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_formatted_code = \"idx1 = []\\nidx2 = []\\n\\nfor i in tqdm(range(p3.shape[0] - 1)):\\n    li = lon2[i]\\n    for j in range(\\n        1, min(300, p3.shape[0] - 1 - i)\\n    ):  # put a limit here - look at no more than X items\\n        if (\\n            li != lon2[i + j]\\n        ):  # if lon matches, lat and country also match - b/c of sorting order\\n            break\\n        if lcs2(names[i], names[i + j]) >= 5:  # lcs2 >= 5\\n            idx1.append(i)\\n            idx2.append(i + j)\\n\\np1a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx1].reset_index(drop=True)\\np2a = p3[[\\\"id\\\", \\\"point_of_interest\\\"]].loc[idx2].reset_index(drop=True)\\n\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx1 = []\n",
    "idx2 = []\n",
    "\n",
    "for i in tqdm(range(p3.shape[0] - 1)):\n",
    "    li = lon2[i]\n",
    "    for j in range(\n",
    "        1, min(300, p3.shape[0] - 1 - i)\n",
    "    ):  # put a limit here - look at no more than X items\n",
    "        if (\n",
    "            li != lon2[i + j]\n",
    "        ):  # if lon matches, lat and country also match - b/c of sorting order\n",
    "            break\n",
    "        if lcs2(names[i], names[i + j]) >= 5:  # lcs2 >= 5\n",
    "            idx1.append(i)\n",
    "            idx2.append(i + j)\n",
    "\n",
    "p1a = p3[[\"id\", \"point_of_interest\"]].loc[idx1].reset_index(drop=True)\n",
    "p2a = p3[[\"id\", \"point_of_interest\"]].loc[idx2].reset_index(drop=True)\n",
    "\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 13.55M\n",
      "Proportion of positive candidates: 7.86%\n",
      "Proportion of found matches: 89.23%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6294650 1065420 418955\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"p1[\\\"y\\\"] = np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8)\\na = p1.groupby(\\\"id\\\")[\\\"y\\\"].sum().reset_index()\\nprint(\\n    \\\"Added\\\",\\n    p1a.shape[0],\\n    p1[\\\"y\\\"].sum(),\\n    np.minimum(1, a[\\\"y\\\"]).sum(),\\n)\";\n",
       "                var nbb_formatted_code = \"p1[\\\"y\\\"] = np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8)\\na = p1.groupby(\\\"id\\\")[\\\"y\\\"].sum().reset_index()\\nprint(\\n    \\\"Added\\\",\\n    p1a.shape[0],\\n    p1[\\\"y\\\"].sum(),\\n    np.minimum(1, a[\\\"y\\\"]).sum(),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1[\"y\"] = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "a = p1.groupby(\"id\")[\"y\"].sum().reset_index()\n",
    "print(\n",
    "    \"Added\",\n",
    "    p1a.shape[0],\n",
    "    p1[\"y\"].sum(),\n",
    "    np.minimum(1, a[\"y\"]).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/.local/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "631"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"# remove duplicate pairs\\np12 = pd.concat([p1[\\\"id\\\"], p2[\\\"id\\\"]], axis=1)\\np12.columns = [\\\"id\\\", \\\"id2\\\"]\\np12 = p12.reset_index()\\n\\n# flip - only keep one of the flipped pairs, the other one is truly redundant\\nidx = p12[\\\"id\\\"] > p12[\\\"id2\\\"]\\np12[\\\"t\\\"] = p12[\\\"id\\\"]\\np12[\\\"id\\\"].loc[idx] = p12[\\\"id2\\\"].loc[idx]\\np12[\\\"id2\\\"].loc[idx] = p12[\\\"t\\\"].loc[idx]\\n\\np12 = p12.sort_values(by=[\\\"id\\\", \\\"id2\\\"]).reset_index(drop=True)\\np12 = p12.drop_duplicates(subset=[\\\"id\\\", \\\"id2\\\"])\\n\\n# also drop id == id2 - it may happen\\np12 = p12.loc[p12[\\\"id\\\"] != p12[\\\"id2\\\"]]\\np1 = p1.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\np2 = p2.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\n\\ndel p12, idx\\ngc.collect()\";\n",
       "                var nbb_formatted_code = \"# remove duplicate pairs\\np12 = pd.concat([p1[\\\"id\\\"], p2[\\\"id\\\"]], axis=1)\\np12.columns = [\\\"id\\\", \\\"id2\\\"]\\np12 = p12.reset_index()\\n\\n# flip - only keep one of the flipped pairs, the other one is truly redundant\\nidx = p12[\\\"id\\\"] > p12[\\\"id2\\\"]\\np12[\\\"t\\\"] = p12[\\\"id\\\"]\\np12[\\\"id\\\"].loc[idx] = p12[\\\"id2\\\"].loc[idx]\\np12[\\\"id2\\\"].loc[idx] = p12[\\\"t\\\"].loc[idx]\\n\\np12 = p12.sort_values(by=[\\\"id\\\", \\\"id2\\\"]).reset_index(drop=True)\\np12 = p12.drop_duplicates(subset=[\\\"id\\\", \\\"id2\\\"])\\n\\n# also drop id == id2 - it may happen\\np12 = p12.loc[p12[\\\"id\\\"] != p12[\\\"id2\\\"]]\\np1 = p1.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\np2 = p2.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\n\\ndel p12, idx\\ngc.collect()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 12.1M\n",
      "Proportion of positive candidates: 4.17%\n",
      "Proportion of found matches: 89.23%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6294650 504039 351737\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"p1[\\\"y\\\"] = np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8)\\na = p1.groupby(\\\"id\\\")[\\\"y\\\"].sum().reset_index()\\nprint(\\n    \\\"Added\\\",\\n    p1a.shape[0],\\n    p1[\\\"y\\\"].sum(),\\n    np.minimum(1, a[\\\"y\\\"]).sum(),\\n)\";\n",
       "                var nbb_formatted_code = \"p1[\\\"y\\\"] = np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8)\\na = p1.groupby(\\\"id\\\")[\\\"y\\\"].sum().reset_index()\\nprint(\\n    \\\"Added\\\",\\n    p1a.shape[0],\\n    p1[\\\"y\\\"].sum(),\\n    np.minimum(1, a[\\\"y\\\"]).sum(),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1[\"y\"] = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "a = p1.groupby(\"id\")[\"y\"].sum().reset_index()\n",
    "print(\n",
    "    \"Added\",\n",
    "    p1a.shape[0],\n",
    "    p1[\"y\"].sum(),\n",
    "    np.minimum(1, a[\"y\"]).sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"del d, names, lon2, p3, idx1, idx2, p1a, p2a, lat, lon\\ngc.collect()\";\n",
       "                var nbb_formatted_code = \"del d, names, lon2, p3, idx1, idx2, p1a, p2a, lat, lon\\ngc.collect()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del d, names, lon2, p3, idx1, idx2, p1a, p2a, lat, lon\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort to put similar points next to each other - for constructing pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"sort = [\\n    \\\"lat2\\\",\\n    \\\"lon2\\\",\\n    \\\"name2\\\",\\n    \\\"latitude\\\",\\n    \\\"city\\\",\\n    \\\"cat2\\\",\\n    \\\"name\\\",\\n    \\\"address\\\",\\n    \\\"country\\\",\\n    \\\"id\\\",\\n]\\n\\ntrain = train.sort_values(by=sort).reset_index(drop=True)\";\n",
       "                var nbb_formatted_code = \"sort = [\\n    \\\"lat2\\\",\\n    \\\"lon2\\\",\\n    \\\"name2\\\",\\n    \\\"latitude\\\",\\n    \\\"city\\\",\\n    \\\"cat2\\\",\\n    \\\"name\\\",\\n    \\\"address\\\",\\n    \\\"country\\\",\\n    \\\"id\\\",\\n]\\n\\ntrain = train.sort_values(by=sort).reset_index(drop=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sort = [\n",
    "    \"lat2\",\n",
    "    \"lon2\",\n",
    "    \"name2\",\n",
    "    \"latitude\",\n",
    "    \"city\",\n",
    "    \"cat2\",\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"country\",\n",
    "    \"id\",\n",
    "]\n",
    "\n",
    "train = train.sort_values(by=sort).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"cols = [\\n    \\\"id\\\",\\n    \\\"latitude\\\",\\n    \\\"longitude\\\",\\n    \\\"point_of_interest\\\",\\n    \\\"name\\\",\\n    \\\"category_simpl\\\",\\n    \\\"name_initial_decode\\\",\\n]\\ncolsa = [\\\"id\\\", \\\"point_of_interest\\\"]\\n\\np1a = train[colsa].copy()\\np2a = train[colsa].iloc[1:, :].reset_index(drop=True).copy()\\np2a = p2a.append(train[colsa].iloc[0], ignore_index=True)\\n\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_formatted_code = \"cols = [\\n    \\\"id\\\",\\n    \\\"latitude\\\",\\n    \\\"longitude\\\",\\n    \\\"point_of_interest\\\",\\n    \\\"name\\\",\\n    \\\"category_simpl\\\",\\n    \\\"name_initial_decode\\\",\\n]\\ncolsa = [\\\"id\\\", \\\"point_of_interest\\\"]\\n\\np1a = train[colsa].copy()\\np2a = train[colsa].iloc[1:, :].reset_index(drop=True).copy()\\np2a = p2a.append(train[colsa].iloc[0], ignore_index=True)\\n\\np1 = p1.append(p1a, ignore_index=True)\\np2 = p2.append(p2a, ignore_index=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = [\n",
    "    \"id\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"point_of_interest\",\n",
    "    \"name\",\n",
    "    \"category_simpl\",\n",
    "    \"name_initial_decode\",\n",
    "]\n",
    "colsa = [\"id\", \"point_of_interest\"]\n",
    "\n",
    "p1a = train[colsa].copy()\n",
    "p2a = train[colsa].iloc[1:, :].reset_index(drop=True).copy()\n",
    "p2a = p2a.append(train[colsa].iloc[0], ignore_index=True)\n",
    "\n",
    "p1 = p1.append(p1a, ignore_index=True)\n",
    "p2 = p2.append(p2a, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 13.24M\n",
      "Proportion of positive candidates: 5.71%\n",
      "Proportion of found matches: 91.64%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 37;\n",
       "                var nbb_unformatted_code = \"p1_svg = p1.copy()\\np2_svg = p2.copy()\";\n",
       "                var nbb_formatted_code = \"p1_svg = p1.copy()\\np2_svg = p2.copy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1_svg = p1.copy()\n",
    "p2_svg = p2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more shifts\n",
    "- Slow (15min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"p1 = p1_svg.copy()\\np2 = p2_svg.copy()\";\n",
       "                var nbb_formatted_code = \"p1 = p1_svg.copy()\\np2 = p2_svg.copy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1 = p1_svg.copy()\n",
    "p2 = p2_svg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ed81c953d34f66ae4dc18a90add73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8, s=10\n",
      "\n",
      "Number of candidates : 19.36M\n",
      "Proportion of positive candidates: 4.61%\n",
      "Proportion of found matches: 93.03%\n",
      "\n",
      "\n",
      "18, s=20\n",
      "\n",
      "Number of candidates : 22.54M\n",
      "Proportion of positive candidates: 5.29%\n",
      "Proportion of found matches: 95.47%\n",
      "\n",
      "\n",
      "28, s=30\n",
      "\n",
      "Number of candidates : 24.14M\n",
      "Proportion of positive candidates: 5.19%\n",
      "Proportion of found matches: 96.10%\n",
      "\n",
      "\n",
      "38, s=40\n",
      "\n",
      "Number of candidates : 25.24M\n",
      "Proportion of positive candidates: 5.08%\n",
      "Proportion of found matches: 96.43%\n",
      "\n",
      "\n",
      "48, s=50\n",
      "\n",
      "Number of candidates : 26.1M\n",
      "Proportion of positive candidates: 4.99%\n",
      "Proportion of found matches: 96.65%\n",
      "\n",
      "\n",
      "58, s=60\n",
      "\n",
      "Number of candidates : 26.82M\n",
      "Proportion of positive candidates: 4.91%\n",
      "Proportion of found matches: 96.82%\n",
      "\n",
      "\n",
      "68, s=70\n",
      "\n",
      "Number of candidates : 27.44M\n",
      "Proportion of positive candidates: 4.84%\n",
      "Proportion of found matches: 96.95%\n",
      "\n",
      "\n",
      "78, s=80\n",
      "\n",
      "Number of candidates : 27.98M\n",
      "Proportion of positive candidates: 4.79%\n",
      "Proportion of found matches: 97.05%\n",
      "\n",
      "\n",
      "88, s=90\n",
      "\n",
      "Number of candidates : 28.46M\n",
      "Proportion of positive candidates: 4.74%\n",
      "Proportion of found matches: 97.14%\n",
      "\n",
      "\n",
      "98, s=100\n",
      "\n",
      "Number of candidates : 28.89M\n",
      "Proportion of positive candidates: 4.69%\n",
      "Proportion of found matches: 97.21%\n",
      "\n",
      "\n",
      "108, s=110\n",
      "\n",
      "Number of candidates : 29.29M\n",
      "Proportion of positive candidates: 4.65%\n",
      "Proportion of found matches: 97.27%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(tqdm(range(2, 121))):  # 121\n",
    "    if s == 15:  # resort by closer location after 15 shifts\n",
    "        train[\"lat2\"] = np.round(train[\"latitude\"], 2).astype(\"float32\")  # 2 = 1 Km\n",
    "        train[\"lon2\"] = np.round(train[\"longitude\"], 2).astype(\"float32\")\n",
    "        train = train.sort_values(\n",
    "            by=[\"country\", \"lat2\", \"lon2\", \"categories\", \"city\", \"id\"]\n",
    "        ).reset_index(drop=True)\n",
    "        train.drop([\"lat2\", \"lon2\"], axis=1, inplace=True)\n",
    "\n",
    "    if s < 4:\n",
    "        maxdist = 500000\n",
    "    elif s < 8:\n",
    "        maxdist = 10000\n",
    "    elif s < 12:\n",
    "        maxdist = 5000\n",
    "    elif s < 15:\n",
    "        maxdist = 2000\n",
    "    else:\n",
    "        maxdist = max(100, 200 - (s - 16) * 1)\n",
    "\n",
    "    s2 = s  # shift\n",
    "    if i >= 13:  # resorted data\n",
    "        s2 = i - 12\n",
    "\n",
    "    p2a = train[cols].iloc[s2:, :]\n",
    "    p2a = p2a.append(train[cols].iloc[:s2, :], ignore_index=True)\n",
    "\n",
    "    # drop pairs with large distances\n",
    "    dist = distance(\n",
    "        np.array(train[\"latitude\"]),\n",
    "        np.array(train[\"longitude\"]),\n",
    "        np.array(p2a[\"latitude\"]),\n",
    "        np.array(p2a[\"longitude\"]),\n",
    "    )\n",
    "    same_cat_simpl = (train[\"category_simpl\"] == p2a[\"category_simpl\"]) & (\n",
    "        train[\"category_simpl\"] > 0\n",
    "    )\n",
    "\n",
    "    ii = np.zeros(train.shape[0], dtype=np.int8)\n",
    "    x1, x2 = train[\"name\"].to_numpy(), p2a[\"name\"].to_numpy()\n",
    "    for j in range(train.shape[0]):\n",
    "        if pi1(x1[j], x2[j]):\n",
    "            ii[j] = 1\n",
    "        elif substring_ratio(x1[j], x2[j]) >= 0.65:\n",
    "            ii[j] = 1\n",
    "        elif subseq_ratio(x1[j], x2[j]) >= 0.75:\n",
    "            ii[j] = 1\n",
    "        elif len(x1[j]) >= 7 and len(x2[j]) >= 7 and x1[j].endswith(x2[j][-7:]):\n",
    "            ii[j] = 1\n",
    "    # keep if dist < maxdist, or names partially match\n",
    "    # idx = (dist < maxdist) | (ii > 0)\n",
    "    idx = (\n",
    "        (dist < maxdist)\n",
    "        | (ii > 0)\n",
    "        | np.logical_and(same_cat_simpl, dist < train[\"q90\"] * 900)\n",
    "    )\n",
    "\n",
    "    p1 = p1.append(train[colsa].loc[idx], ignore_index=True)\n",
    "    p2 = p2.append(p2a[colsa].loc[idx], ignore_index=True)\n",
    "\n",
    "    if not (s % 10):\n",
    "        # get stats; overstated b/c dups are not excluded yet\n",
    "        print(f\"{i}, s={s}\")\n",
    "        print_infos(p1, p2, N_TO_FIND)\n",
    "        print()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add close candidates \n",
    "- Slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_found = {\n",
    "    tuple(sorted([idx1, idx2])): 1 for idx1, idx2 in zip(p1[\"id\"], p2[\"id\"])\n",
    "}\n",
    "\n",
    "New_candidates = []\n",
    "\n",
    "# for country_ in range(2, len(countries)+1):\n",
    "for country_ in [2, 3]:\n",
    "\n",
    "    nb_max_candidates = 400\n",
    "    new_cand = set()\n",
    "\n",
    "    # Create matrix\n",
    "    matrix = train[train[\"country\"] == country_].copy()\n",
    "    if len(matrix) <= 1:\n",
    "        break\n",
    "    Original_idx = {i: idx for i, idx in enumerate(matrix.index)}\n",
    "\n",
    "    # Find closest neighbours\n",
    "    M_dist, M_index = Compute_Mdist_Mindex(\n",
    "        matrix[[\"latitude\", \"longitude\"]].to_numpy(),\n",
    "        nb_max_candidates=nb_max_candidates,\n",
    "    )\n",
    "\n",
    "    # Select candidates\n",
    "    new_true_match = 0\n",
    "    infos = matrix[[\"id\", \"name\", \"point_of_interest\"]].to_numpy()\n",
    "\n",
    "    for idx1, (Liste_idx, Liste_val) in enumerate(zip(M_index, M_dist)):\n",
    "        for idx2, dist in zip(Liste_idx, Liste_val):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "\n",
    "            # Too far candidates\n",
    "            if dist > 0.12:\n",
    "                break\n",
    "\n",
    "            id1, id2 = infos[idx1, 0], infos[idx2, 0]\n",
    "            name1, name2 = infos[idx1, 1], infos[idx2, 1]\n",
    "\n",
    "            if pi1(name1, name2) == 1 or substring_ratio(name1, name2) >= 0.5:\n",
    "                key = tuple(sorted([id1, id2]))\n",
    "                if key not in already_found:\n",
    "                    key_idx = tuple(sorted([Original_idx[idx1], Original_idx[idx2]]))\n",
    "                    try:\n",
    "                        if key_idx not in new_cand:\n",
    "                            new_true_match += int(infos[idx1, -1] == infos[idx2, -1])\n",
    "                    except:\n",
    "                        pass\n",
    "                    new_cand.add(key_idx)\n",
    "\n",
    "    # Add new candidates\n",
    "    New_candidates += [list(x) for x in new_cand]\n",
    "    print(\n",
    "        f\"Country {country_} ({COUNTRIES[country_-1]}) : {new_true_match}/{len(new_cand)} new cand added.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add matches\n",
    "size1 = len(p1)\n",
    "Added_p1, Added_p2 = [], []\n",
    "for idx1, idx2 in New_candidates:\n",
    "    id1, id2 = train[\"id\"].iat[idx1], train[\"id\"].iat[idx2]\n",
    "    poi1, poi2 = (\n",
    "        train[\"point_of_interest\"].iat[idx1],\n",
    "        train[\"point_of_interest\"].iat[idx2],\n",
    "    )\n",
    "    Added_p1.append([id1, poi1, 0])\n",
    "    Added_p2.append([id2, poi2])\n",
    "\n",
    "Added_p1 = pd.DataFrame(Added_p1, columns=p1.columns)\n",
    "Added_p2 = pd.DataFrame(Added_p2, columns=p2.columns)\n",
    "for col in Added_p1.columns:\n",
    "    Added_p1[col] = Added_p1[col].astype(p1[col].dtype)\n",
    "for col in Added_p2.columns:\n",
    "    Added_p2[col] = Added_p2[col].astype(p2[col].dtype)\n",
    "\n",
    "p1 = p1.append(Added_p1).reset_index(drop=True).copy()\n",
    "p2 = p2.append(Added_p2).reset_index(drop=True).copy()\n",
    "\n",
    "print(f\"Candidates added : {len(p1) - size1}/{len(p1)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx, matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 26.08M\n",
      "Proportion of positive candidates: 2.53%\n",
      "Proportion of found matches: 97.56%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 49;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/kaggle/foursquare/src/matching.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[\"id2\"] = p2[\"id\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Highest reachable IoU : 0.9883\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 50;\n",
       "                var nbb_unformatted_code = \"get_CV(\\n    p1,\\n    p2,\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    train,\\n)\";\n",
       "                var nbb_formatted_code = \"get_CV(\\n    p1,\\n    p2,\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    train,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_svg = p1.copy()\n",
    "p2_svg = p2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add close candidates v2\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 51;\n",
       "                var nbb_unformatted_code = \"p1 = p1_svg.copy()\\np2 = p2_svg.copy()\\n\\ntrain[\\\"lat2\\\"] = np.round(train[\\\"latitude\\\"], 0).astype(\\\"int8\\\")\\ntrain[\\\"lon2\\\"] = np.round(train[\\\"longitude\\\"], 0).astype(\\\"int8\\\")\\n\\n# sort to put similar points next to each other - for constructing pairs\\nsort = [\\n    \\\"category_simpl\\\",\\n    \\\"lat2\\\",\\n    \\\"lon2\\\",\\n    \\\"name2\\\",\\n    \\\"latitude\\\",\\n    \\\"city\\\",\\n    \\\"cat2\\\",\\n    \\\"name\\\",\\n    \\\"address\\\",\\n    \\\"country\\\",\\n    \\\"id\\\",\\n]\\ntrain = train.sort_values(by=sort).reset_index(drop=True)\\n\\nmaxdist = (train[\\\"q90\\\"] * 400 + train[\\\"q99\\\"] * 400).to_numpy()\";\n",
       "                var nbb_formatted_code = \"p1 = p1_svg.copy()\\np2 = p2_svg.copy()\\n\\ntrain[\\\"lat2\\\"] = np.round(train[\\\"latitude\\\"], 0).astype(\\\"int8\\\")\\ntrain[\\\"lon2\\\"] = np.round(train[\\\"longitude\\\"], 0).astype(\\\"int8\\\")\\n\\n# sort to put similar points next to each other - for constructing pairs\\nsort = [\\n    \\\"category_simpl\\\",\\n    \\\"lat2\\\",\\n    \\\"lon2\\\",\\n    \\\"name2\\\",\\n    \\\"latitude\\\",\\n    \\\"city\\\",\\n    \\\"cat2\\\",\\n    \\\"name\\\",\\n    \\\"address\\\",\\n    \\\"country\\\",\\n    \\\"id\\\",\\n]\\ntrain = train.sort_values(by=sort).reset_index(drop=True)\\n\\nmaxdist = (train[\\\"q90\\\"] * 400 + train[\\\"q99\\\"] * 400).to_numpy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1 = p1_svg.copy()\n",
    "p2 = p2_svg.copy()\n",
    "\n",
    "train[\"lat2\"] = np.round(train[\"latitude\"], 0).astype(\"int8\")\n",
    "train[\"lon2\"] = np.round(train[\"longitude\"], 0).astype(\"int8\")\n",
    "\n",
    "# sort to put similar points next to each other - for constructing pairs\n",
    "sort = [\n",
    "    \"category_simpl\",\n",
    "    \"lat2\",\n",
    "    \"lon2\",\n",
    "    \"name2\",\n",
    "    \"latitude\",\n",
    "    \"city\",\n",
    "    \"cat2\",\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"country\",\n",
    "    \"id\",\n",
    "]\n",
    "train = train.sort_values(by=sort).reset_index(drop=True)\n",
    "\n",
    "maxdist = (train[\"q90\"] * 400 + train[\"q99\"] * 400).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd17114f9b1a44d9a04e0e766e433e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=49.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, s=1\n",
      "\n",
      "Number of candidates : 26.56M\n",
      "Proportion of positive candidates: 3.20%\n",
      "Proportion of found matches: 97.58%\n",
      "\n",
      "\n",
      "1, s=2\n",
      "\n",
      "Number of candidates : 26.83M\n",
      "Proportion of positive candidates: 3.28%\n",
      "Proportion of found matches: 97.59%\n",
      "\n",
      "\n",
      "2, s=3\n",
      "\n",
      "Number of candidates : 27.05M\n",
      "Proportion of positive candidates: 3.31%\n",
      "Proportion of found matches: 97.60%\n",
      "\n",
      "\n",
      "3, s=4\n",
      "\n",
      "Number of candidates : 27.24M\n",
      "Proportion of positive candidates: 3.33%\n",
      "Proportion of found matches: 97.61%\n",
      "\n",
      "\n",
      "4, s=5\n",
      "\n",
      "Number of candidates : 27.4M\n",
      "Proportion of positive candidates: 3.34%\n",
      "Proportion of found matches: 97.62%\n",
      "\n",
      "\n",
      "5, s=6\n",
      "\n",
      "Number of candidates : 27.55M\n",
      "Proportion of positive candidates: 3.35%\n",
      "Proportion of found matches: 97.63%\n",
      "\n",
      "\n",
      "6, s=7\n",
      "\n",
      "Number of candidates : 27.69M\n",
      "Proportion of positive candidates: 3.36%\n",
      "Proportion of found matches: 97.64%\n",
      "\n",
      "\n",
      "7, s=8\n",
      "\n",
      "Number of candidates : 27.82M\n",
      "Proportion of positive candidates: 3.36%\n",
      "Proportion of found matches: 97.64%\n",
      "\n",
      "\n",
      "8, s=9\n",
      "\n",
      "Number of candidates : 27.94M\n",
      "Proportion of positive candidates: 3.36%\n",
      "Proportion of found matches: 97.65%\n",
      "\n",
      "\n",
      "9, s=10\n",
      "\n",
      "Number of candidates : 28.05M\n",
      "Proportion of positive candidates: 3.36%\n",
      "Proportion of found matches: 97.65%\n",
      "\n",
      "\n",
      "19, s=20\n",
      "\n",
      "Number of candidates : 28.96M\n",
      "Proportion of positive candidates: 3.36%\n",
      "Proportion of found matches: 97.71%\n",
      "\n",
      "\n",
      "29, s=30\n",
      "\n",
      "Number of candidates : 29.63M\n",
      "Proportion of positive candidates: 3.34%\n",
      "Proportion of found matches: 97.75%\n",
      "\n",
      "\n",
      "39, s=40\n",
      "\n",
      "Number of candidates : 30.19M\n",
      "Proportion of positive candidates: 3.33%\n",
      "Proportion of found matches: 97.77%\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 7min 39s, sys: 19 s, total: 7min 58s\n",
      "Wall time: 7min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 52;\n",
       "                var nbb_unformatted_code = \"%%time\\n\\n# add more shifts, only for short distances or for partial name matches\\nfor i, s in enumerate(tqdm(range(1, 50))): # 121\\n    \\n    s2 = s # shift\\n    p2a = train[cols].iloc[s2:,:]\\n    p2a = p2a.append(train[cols].iloc[:s2,:], ignore_index=True)\\n    \\n    # drop pairs with large distances\\n    same_cat_simpl = (train['category_simpl']==p2a['category_simpl']).to_numpy()\\n    \\n    dist = distance(np.array(train['latitude']), np.array(train['longitude']), np.array(p2a['latitude']), np.array(p2a['longitude']))\\n    \\n    ii = np.zeros(train.shape[0], dtype=np.int8)\\n    x1, x2 = train[['name', 'name_initial_decode']].to_numpy(), p2a[['name', 'name_initial_decode']].to_numpy()\\n        \\n    for j in range(train.shape[0]):# pi1 adds 14K matches\\n        \\n        if same_cat_simpl[j] and dist[j] < maxdist[j] :\\n            \\n            name1, name2 = x1[j][0], x2[j][0]\\n            name_ini1, name_ini2 = x1[j][1], x2[j][1]\\n                            \\n            if pi1(name1, name2)==1 :\\n                ii[j] = 1\\n            elif substring_ratio(name1, name2) >= 0.6 :\\n                ii[j] = 1\\n            elif subseq_ratio(name1, name2) >= 0.7 :\\n                ii[j] = 1\\n            elif len(name1)>=6 and len(name2)>=6 and name1.endswith(name2[-6:]) :\\n                ii[j] = 1\\n            #elif has_common_word(name_ini1, name_ini2, min_len=6) :\\n            #    ii[j] = 1\\n            #elif word_in_common(name_ini1, name_ini2, min_len_word=6):\\n            #    ii[j] = 1\\n            #elif subword_in_common(name_ini1, name_ini2, min_len_word=6) :\\n            #    ii[j]=1\\n            \\n    idx = (ii > 0)\\n    \\n    p1 = p1.append(train[colsa].loc[idx], ignore_index=True)\\n    p2 = p2.append(p2a[colsa].loc[idx], ignore_index=True)\\n    \\n    if s < 10 or s % 10 == 0:\\n        # get stats; overstated b/c dups are not excluded yet\\n        print(f\\\"{i}, s={s}\\\")\\n        print_infos(p1, p2, N_TO_FIND)\\n        print()\\n\\ndel p2a, dist, idx\\ngc.collect()\";\n",
       "                var nbb_formatted_code = \"%%time\\n\\n# add more shifts, only for short distances or for partial name matches\\nfor i, s in enumerate(tqdm(range(1, 50))): # 121\\n    \\n    s2 = s # shift\\n    p2a = train[cols].iloc[s2:,:]\\n    p2a = p2a.append(train[cols].iloc[:s2,:], ignore_index=True)\\n    \\n    # drop pairs with large distances\\n    same_cat_simpl = (train['category_simpl']==p2a['category_simpl']).to_numpy()\\n    \\n    dist = distance(np.array(train['latitude']), np.array(train['longitude']), np.array(p2a['latitude']), np.array(p2a['longitude']))\\n    \\n    ii = np.zeros(train.shape[0], dtype=np.int8)\\n    x1, x2 = train[['name', 'name_initial_decode']].to_numpy(), p2a[['name', 'name_initial_decode']].to_numpy()\\n        \\n    for j in range(train.shape[0]):# pi1 adds 14K matches\\n        \\n        if same_cat_simpl[j] and dist[j] < maxdist[j] :\\n            \\n            name1, name2 = x1[j][0], x2[j][0]\\n            name_ini1, name_ini2 = x1[j][1], x2[j][1]\\n                            \\n            if pi1(name1, name2)==1 :\\n                ii[j] = 1\\n            elif substring_ratio(name1, name2) >= 0.6 :\\n                ii[j] = 1\\n            elif subseq_ratio(name1, name2) >= 0.7 :\\n                ii[j] = 1\\n            elif len(name1)>=6 and len(name2)>=6 and name1.endswith(name2[-6:]) :\\n                ii[j] = 1\\n            #elif has_common_word(name_ini1, name_ini2, min_len=6) :\\n            #    ii[j] = 1\\n            #elif word_in_common(name_ini1, name_ini2, min_len_word=6):\\n            #    ii[j] = 1\\n            #elif subword_in_common(name_ini1, name_ini2, min_len_word=6) :\\n            #    ii[j]=1\\n            \\n    idx = (ii > 0)\\n    \\n    p1 = p1.append(train[colsa].loc[idx], ignore_index=True)\\n    p2 = p2.append(p2a[colsa].loc[idx], ignore_index=True)\\n    \\n    if s < 10 or s % 10 == 0:\\n        # get stats; overstated b/c dups are not excluded yet\\n        print(f\\\"{i}, s={s}\\\")\\n        print_infos(p1, p2, N_TO_FIND)\\n        print()\\n\\ndel p2a, dist, idx\\ngc.collect()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add more shifts, only for short distances or for partial name matches\n",
    "for i, s in enumerate(tqdm(range(1, 50))): # 121\n",
    "\n",
    "    s2 = s  # shift\n",
    "    p2a = train[cols].iloc[s2:, :]\n",
    "    p2a = p2a.append(train[cols].iloc[:s2, :], ignore_index=True)\n",
    "\n",
    "    # drop pairs with large distances\n",
    "    same_cat_simpl = (train['category_simpl'] == p2a['category_simpl']).to_numpy()\n",
    "\n",
    "    dist = distance(\n",
    "        np.array(train['latitude']),\n",
    "        np.array(train['longitude']),\n",
    "        np.array(p2a['latitude']),\n",
    "        np.array(p2a['longitude'])\n",
    "    )\n",
    "\n",
    "    ii = np.zeros(train.shape[0], dtype=np.int8)\n",
    "    x1 = train[['name', 'name_initial_decode']].to_numpy()\n",
    "    x2 = p2a[['name', 'name_initial_decode']].to_numpy()\n",
    "\n",
    "    for j in range(train.shape[0]):  # pi1 adds 14K matches\n",
    "\n",
    "        if same_cat_simpl[j] and dist[j] < maxdist[j]:\n",
    "\n",
    "            name1, name2 = x1[j][0], x2[j][0]\n",
    "            name_ini1, name_ini2 = x1[j][1], x2[j][1]\n",
    "\n",
    "            if pi1(name1, name2) == 1:\n",
    "                ii[j] = 1\n",
    "            elif substring_ratio(name1, name2) >= 0.6:\n",
    "                ii[j] = 1\n",
    "            elif subseq_ratio(name1, name2) >= 0.7:\n",
    "                ii[j] = 1\n",
    "            elif len(name1) >= 6 and len(name2) >= 6 and name1.endswith(name2[-6:]) :\n",
    "                ii[j] = 1\n",
    "\n",
    "            # elif has_common_word(name_ini1, name_ini2, min_len=6) :\n",
    "            #    ii[j] = 1\n",
    "            # elif word_in_common(name_ini1, name_ini2, min_len_word=6):\n",
    "            #    ii[j] = 1\n",
    "            # elif subword_in_common(name_ini1, name_ini2, min_len_word=6) :\n",
    "            #    ii[j]=1\n",
    "\n",
    "    idx = (ii > 0)\n",
    "\n",
    "    p1 = p1.append(train[colsa].loc[idx], ignore_index=True)\n",
    "    p2 = p2.append(p2a[colsa].loc[idx], ignore_index=True)\n",
    "\n",
    "    if s < 10 or s % 10 == 0:\n",
    "        # get stats; overstated b/c dups are not excluded yet\n",
    "        print(f\"{i}, s={s}\")\n",
    "        print_infos(p1, p2, N_TO_FIND)\n",
    "        print()\n",
    "\n",
    "del p2a, dist, idx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/.local/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 53;\n",
       "                var nbb_unformatted_code = \"# remove duplicate pairs\\np12 = pd.concat([p1[\\\"id\\\"], p2[\\\"id\\\"]], axis=1)\\np12.columns = [\\\"id\\\", \\\"id2\\\"]\\np12 = p12.reset_index()\\n\\n# flip - only keep one of the flipped pairs, the other one is truly redundant\\nidx = p12[\\\"id\\\"] > p12[\\\"id2\\\"]\\np12[\\\"t\\\"] = p12[\\\"id\\\"]\\np12[\\\"id\\\"].loc[idx] = p12[\\\"id2\\\"].loc[idx]\\np12[\\\"id2\\\"].loc[idx] = p12[\\\"t\\\"].loc[idx]\\n\\np12 = p12.sort_values(by=[\\\"id\\\", \\\"id2\\\"]).reset_index(drop=True)\\np12 = p12.drop_duplicates(subset=[\\\"id\\\", \\\"id2\\\"])\\n\\n# also drop id == id2 - it may happen\\np12 = p12.loc[p12[\\\"id\\\"] != p12[\\\"id2\\\"]]\\np1 = p1.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\np2 = p2.loc[p12[\\\"index\\\"]].reset_index(drop=True)\";\n",
       "                var nbb_formatted_code = \"# remove duplicate pairs\\np12 = pd.concat([p1[\\\"id\\\"], p2[\\\"id\\\"]], axis=1)\\np12.columns = [\\\"id\\\", \\\"id2\\\"]\\np12 = p12.reset_index()\\n\\n# flip - only keep one of the flipped pairs, the other one is truly redundant\\nidx = p12[\\\"id\\\"] > p12[\\\"id2\\\"]\\np12[\\\"t\\\"] = p12[\\\"id\\\"]\\np12[\\\"id\\\"].loc[idx] = p12[\\\"id2\\\"].loc[idx]\\np12[\\\"id2\\\"].loc[idx] = p12[\\\"t\\\"].loc[idx]\\n\\np12 = p12.sort_values(by=[\\\"id\\\", \\\"id2\\\"]).reset_index(drop=True)\\np12 = p12.drop_duplicates(subset=[\\\"id\\\", \\\"id2\\\"])\\n\\n# also drop id == id2 - it may happen\\np12 = p12.loc[p12[\\\"id\\\"] != p12[\\\"id2\\\"]]\\np1 = p1.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\np2 = p2.loc[p12[\\\"index\\\"]].reset_index(drop=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 28.13M\n",
      "Proportion of positive candidates: 2.45%\n",
      "Proportion of found matches: 97.79%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 54;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/kaggle/foursquare/src/matching.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[\"id2\"] = p2[\"id\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Highest reachable IoU : 0.9894\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 55;\n",
       "                var nbb_unformatted_code = \"get_CV(\\n    p1,\\n    p2,\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    train,\\n)\";\n",
       "                var nbb_formatted_code = \"get_CV(\\n    p1,\\n    p2,\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    train,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 56;\n",
       "                var nbb_unformatted_code = \"p1_svg = p1.copy()\\np2_svg = p2.copy()\";\n",
       "                var nbb_formatted_code = \"p1_svg = p1.copy()\\np2_svg = p2.copy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1_svg = p1.copy()\n",
    "p2_svg = p2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates in initial Youri's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 57;\n",
       "                var nbb_unformatted_code = \"p1[\\\"y\\\"] = np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8)\\n\\nID_to_POI = dict(zip(train[\\\"id\\\"], train[\\\"point_of_interest\\\"]))\\nnb_true_matchs_initial = 0\\nCand = {}\\nfor i, (id1, id2) in enumerate(zip(p1[\\\"id\\\"], p2[\\\"id\\\"])):\\n    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n    Cand[key] = p1[\\\"y\\\"].iloc[i]\\n    nb_true_matchs_initial += int(ID_to_POI[id1] == ID_to_POI[id2])\";\n",
       "                var nbb_formatted_code = \"p1[\\\"y\\\"] = np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8)\\n\\nID_to_POI = dict(zip(train[\\\"id\\\"], train[\\\"point_of_interest\\\"]))\\nnb_true_matchs_initial = 0\\nCand = {}\\nfor i, (id1, id2) in enumerate(zip(p1[\\\"id\\\"], p2[\\\"id\\\"])):\\n    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n    Cand[key] = p1[\\\"y\\\"].iloc[i]\\n    nb_true_matchs_initial += int(ID_to_POI[id1] == ID_to_POI[id2])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1[\"y\"] = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "\n",
    "ID_to_POI = dict(zip(train[\"id\"], train[\"point_of_interest\"]))\n",
    "nb_true_matchs_initial = 0\n",
    "Cand = {}\n",
    "for i, (id1, id2) in enumerate(zip(p1[\"id\"], p2[\"id\"])):\n",
    "    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "    Cand[key] = p1[\"y\"].iloc[i]\n",
    "    nb_true_matchs_initial += int(ID_to_POI[id1] == ID_to_POI[id2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF n°1 : airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 58;\n",
       "                var nbb_unformatted_code = \"p1 = p1_svg.copy()\\np2 = p2_svg.copy()\";\n",
       "                var nbb_formatted_code = \"p1 = p1_svg.copy()\\np2 = p2_svg.copy()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p1 = p1_svg.copy()\n",
    "p2 = p2_svg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len names : 86363.\n",
      "Nb cand tf-idf : 894784\n",
      "Candidates added for tfidf n°1 (airports) : 23944/28152218.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 59;\n",
       "                var nbb_unformatted_code = \"far_cat_simpl = [1, 2]\\nthr_tfidf = 0.45\\n\\nfor col_name in [\\\"name_initial_decode\\\"]:\\n\\n    Names = train[train[\\\"category_simpl\\\"].isin(far_cat_simpl + [-1])][\\n        col_name\\n    ].copy()  # add unknown categories\\n\\n    def process_terminal(text):\\n        for i in range(0, 30):\\n            text = text.replace(f\\\"terminal {i}\\\", \\\"\\\")\\n            text = text.replace(f\\\"terminal{i}\\\", \\\"\\\")\\n            text = text.replace(f\\\"t{i}\\\", \\\"\\\")\\n        return text\\n\\n    Names = Names.apply(process_terminal)\\n\\n    # Drop stop words\\n    Names = Names.apply(lambda x: x.replace(\\\"airpord\\\", \\\"airport\\\"))\\n    Names = Names.apply(lambda x: x.replace(\\\"internasional\\\", \\\"international\\\"))\\n    Names = Names.apply(lambda x: x.replace(\\\"internacional\\\", \\\"international\\\"))\\n    for stopword in [\\n        \\\"terminal\\\",\\n        \\\"airport\\\",\\n        \\\"arrival\\\",\\n        \\\"hall\\\",\\n        \\\"departure\\\",\\n        \\\"bus stop\\\",\\n        \\\"airways\\\",\\n        \\\"checkin\\\",\\n    ]:\\n        Names = Names.apply(lambda x: x.replace(stopword + \\\"s\\\", \\\"\\\"))\\n        Names = Names.apply(lambda x: x.replace(stopword, \\\"\\\"))\\n    Names = Names.apply(lambda x: x.strip())\\n    Names = Names[Names.str.len() >= 2]\\n\\n    Names_numrow = {\\n        i: idx for i, idx in enumerate(Names.index)\\n    }  # Keep initial row number\\n    Names = Names.to_list()\\n\\n    print(f\\\"Len names : {len(Names)}.\\\")\\n\\n    # Tf-idf\\n    if 1 < len(Names) < 400000:\\n        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=thr_tfidf)\\n\\n        # no self-matchs and retrieve the initial row number\\n        Tfidf_no_selfmatch = [\\n            [Names_numrow[i], [Names_numrow[x] for x in L if x != i]]\\n            for i, L in enumerate(Tfidf_idx)\\n        ]\\n        Tfidf_no_selfmatch = [x for x in Tfidf_no_selfmatch if len(x[-1]) > 0]\\n        print(\\\"Nb cand tf-idf :\\\", sum([len(L) for idx, L in Tfidf_no_selfmatch]))\\n\\n        # Add matches\\n        size1 = len(p1)\\n        Added_p1, Added_p2 = [], []\\n        for idx1, Liste_idx in Tfidf_no_selfmatch:\\n            id1, name1, lat1, lon1 = (\\n                train[\\\"id\\\"].iat[idx1],\\n                train[\\\"name\\\"].iat[idx1],\\n                train[\\\"latitude\\\"].iat[idx1],\\n                train[\\\"longitude\\\"].iat[idx1],\\n            )\\n            cat1, country1, cat_simpl1 = (\\n                train[\\\"categories\\\"].iat[idx1],\\n                train[\\\"country\\\"].iat[idx1],\\n                train[\\\"category_simpl\\\"].iat[idx1],\\n            )\\n            for idx2 in Liste_idx:\\n                # if len(Liste_idx)>30 : continue\\n                if idx1 < idx2:\\n                    id2, lat2, lon2 = (\\n                        train[\\\"id\\\"].iat[idx2],\\n                        train[\\\"latitude\\\"].iat[idx2],\\n                        train[\\\"longitude\\\"].iat[idx2],\\n                    )\\n                    cat2, country2, cat_simpl2 = (\\n                        train[\\\"categories\\\"].iat[idx2],\\n                        train[\\\"country\\\"].iat[idx2],\\n                        train[\\\"category_simpl\\\"].iat[idx2],\\n                    )\\n                    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n                    # same_cat = (cat_simpl1==cat_simpl2 and cat_simpl1>0) or (cat1==cat2 and cat1!='')\\n                    if (\\n                        key not in Cand\\n                        and (cat_simpl1 == 1 or cat_simpl2 == 1)\\n                        and (\\n                            haversine(lat1, lon1, lat2, lon2) <= 100\\n                            or \\\"kualalumpur\\\" in name1\\n                        )\\n                    ):\\n                        poi1, poi2 = (\\n                            train[\\\"point_of_interest\\\"].iat[idx1],\\n                            train[\\\"point_of_interest\\\"].iat[idx2],\\n                        )\\n                        Cand[key] = int(poi1 == poi2)\\n                        Added_p1.append([id1, poi1])\\n                        Added_p2.append([id2, poi2])\\n                        nb_true_matchs_initial += int(poi1 == poi2)\\n\\n        p1 = (\\n            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        p2 = (\\n            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        print(f\\\"Candidates added for tfidf n\\u00b01 (airports) : {len(p1)-size1}/{len(p1)}.\\\")\";\n",
       "                var nbb_formatted_code = \"far_cat_simpl = [1, 2]\\nthr_tfidf = 0.45\\n\\nfor col_name in [\\\"name_initial_decode\\\"]:\\n\\n    Names = train[train[\\\"category_simpl\\\"].isin(far_cat_simpl + [-1])][\\n        col_name\\n    ].copy()  # add unknown categories\\n\\n    def process_terminal(text):\\n        for i in range(0, 30):\\n            text = text.replace(f\\\"terminal {i}\\\", \\\"\\\")\\n            text = text.replace(f\\\"terminal{i}\\\", \\\"\\\")\\n            text = text.replace(f\\\"t{i}\\\", \\\"\\\")\\n        return text\\n\\n    Names = Names.apply(process_terminal)\\n\\n    # Drop stop words\\n    Names = Names.apply(lambda x: x.replace(\\\"airpord\\\", \\\"airport\\\"))\\n    Names = Names.apply(lambda x: x.replace(\\\"internasional\\\", \\\"international\\\"))\\n    Names = Names.apply(lambda x: x.replace(\\\"internacional\\\", \\\"international\\\"))\\n    for stopword in [\\n        \\\"terminal\\\",\\n        \\\"airport\\\",\\n        \\\"arrival\\\",\\n        \\\"hall\\\",\\n        \\\"departure\\\",\\n        \\\"bus stop\\\",\\n        \\\"airways\\\",\\n        \\\"checkin\\\",\\n    ]:\\n        Names = Names.apply(lambda x: x.replace(stopword + \\\"s\\\", \\\"\\\"))\\n        Names = Names.apply(lambda x: x.replace(stopword, \\\"\\\"))\\n    Names = Names.apply(lambda x: x.strip())\\n    Names = Names[Names.str.len() >= 2]\\n\\n    Names_numrow = {\\n        i: idx for i, idx in enumerate(Names.index)\\n    }  # Keep initial row number\\n    Names = Names.to_list()\\n\\n    print(f\\\"Len names : {len(Names)}.\\\")\\n\\n    # Tf-idf\\n    if 1 < len(Names) < 400000:\\n        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=thr_tfidf)\\n\\n        # no self-matchs and retrieve the initial row number\\n        Tfidf_no_selfmatch = [\\n            [Names_numrow[i], [Names_numrow[x] for x in L if x != i]]\\n            for i, L in enumerate(Tfidf_idx)\\n        ]\\n        Tfidf_no_selfmatch = [x for x in Tfidf_no_selfmatch if len(x[-1]) > 0]\\n        print(\\\"Nb cand tf-idf :\\\", sum([len(L) for idx, L in Tfidf_no_selfmatch]))\\n\\n        # Add matches\\n        size1 = len(p1)\\n        Added_p1, Added_p2 = [], []\\n        for idx1, Liste_idx in Tfidf_no_selfmatch:\\n            id1, name1, lat1, lon1 = (\\n                train[\\\"id\\\"].iat[idx1],\\n                train[\\\"name\\\"].iat[idx1],\\n                train[\\\"latitude\\\"].iat[idx1],\\n                train[\\\"longitude\\\"].iat[idx1],\\n            )\\n            cat1, country1, cat_simpl1 = (\\n                train[\\\"categories\\\"].iat[idx1],\\n                train[\\\"country\\\"].iat[idx1],\\n                train[\\\"category_simpl\\\"].iat[idx1],\\n            )\\n            for idx2 in Liste_idx:\\n                # if len(Liste_idx)>30 : continue\\n                if idx1 < idx2:\\n                    id2, lat2, lon2 = (\\n                        train[\\\"id\\\"].iat[idx2],\\n                        train[\\\"latitude\\\"].iat[idx2],\\n                        train[\\\"longitude\\\"].iat[idx2],\\n                    )\\n                    cat2, country2, cat_simpl2 = (\\n                        train[\\\"categories\\\"].iat[idx2],\\n                        train[\\\"country\\\"].iat[idx2],\\n                        train[\\\"category_simpl\\\"].iat[idx2],\\n                    )\\n                    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n                    # same_cat = (cat_simpl1==cat_simpl2 and cat_simpl1>0) or (cat1==cat2 and cat1!='')\\n                    if (\\n                        key not in Cand\\n                        and (cat_simpl1 == 1 or cat_simpl2 == 1)\\n                        and (\\n                            haversine(lat1, lon1, lat2, lon2) <= 100\\n                            or \\\"kualalumpur\\\" in name1\\n                        )\\n                    ):\\n                        poi1, poi2 = (\\n                            train[\\\"point_of_interest\\\"].iat[idx1],\\n                            train[\\\"point_of_interest\\\"].iat[idx2],\\n                        )\\n                        Cand[key] = int(poi1 == poi2)\\n                        Added_p1.append([id1, poi1])\\n                        Added_p2.append([id2, poi2])\\n                        nb_true_matchs_initial += int(poi1 == poi2)\\n\\n        p1 = (\\n            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        p2 = (\\n            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        print(f\\\"Candidates added for tfidf n\\u00b01 (airports) : {len(p1)-size1}/{len(p1)}.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "far_cat_simpl = [1, 2]\n",
    "thr_tfidf = 0.45\n",
    "\n",
    "for col_name in [\"name_initial_decode\"]:\n",
    "\n",
    "    Names = train[train[\"category_simpl\"].isin(far_cat_simpl + [-1])][\n",
    "        col_name\n",
    "    ].copy()  # add unknown categories\n",
    "\n",
    "    def process_terminal(text):\n",
    "        for i in range(0, 30):\n",
    "            text = text.replace(f\"terminal {i}\", \"\")\n",
    "            text = text.replace(f\"terminal{i}\", \"\")\n",
    "            text = text.replace(f\"t{i}\", \"\")\n",
    "        return text\n",
    "\n",
    "    Names = Names.apply(process_terminal)\n",
    "\n",
    "    # Drop stop words\n",
    "    Names = Names.apply(lambda x: x.replace(\"airpord\", \"airport\"))\n",
    "    Names = Names.apply(lambda x: x.replace(\"internasional\", \"international\"))\n",
    "    Names = Names.apply(lambda x: x.replace(\"internacional\", \"international\"))\n",
    "    for stopword in [\n",
    "        \"terminal\",\n",
    "        \"airport\",\n",
    "        \"arrival\",\n",
    "        \"hall\",\n",
    "        \"departure\",\n",
    "        \"bus stop\",\n",
    "        \"airways\",\n",
    "        \"checkin\",\n",
    "    ]:\n",
    "        Names = Names.apply(lambda x: x.replace(stopword + \"s\", \"\"))\n",
    "        Names = Names.apply(lambda x: x.replace(stopword, \"\"))\n",
    "    Names = Names.apply(lambda x: x.strip())\n",
    "    Names = Names[Names.str.len() >= 2]\n",
    "\n",
    "    Names_numrow = {\n",
    "        i: idx for i, idx in enumerate(Names.index)\n",
    "    }  # Keep initial row number\n",
    "    Names = Names.to_list()\n",
    "\n",
    "    print(f\"Len names : {len(Names)}.\")\n",
    "\n",
    "    # Tf-idf\n",
    "    if 1 < len(Names) < 400000:\n",
    "        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=thr_tfidf)\n",
    "\n",
    "        # no self-matchs and retrieve the initial row number\n",
    "        Tfidf_no_selfmatch = [\n",
    "            [Names_numrow[i], [Names_numrow[x] for x in L if x != i]]\n",
    "            for i, L in enumerate(Tfidf_idx)\n",
    "        ]\n",
    "        Tfidf_no_selfmatch = [x for x in Tfidf_no_selfmatch if len(x[-1]) > 0]\n",
    "        print(\"Nb cand tf-idf :\", sum([len(L) for idx, L in Tfidf_no_selfmatch]))\n",
    "\n",
    "        # Add matches\n",
    "        size1 = len(p1)\n",
    "        Added_p1, Added_p2 = [], []\n",
    "        for idx1, Liste_idx in Tfidf_no_selfmatch:\n",
    "            id1, name1, lat1, lon1 = (\n",
    "                train[\"id\"].iat[idx1],\n",
    "                train[\"name\"].iat[idx1],\n",
    "                train[\"latitude\"].iat[idx1],\n",
    "                train[\"longitude\"].iat[idx1],\n",
    "            )\n",
    "            cat1, country1, cat_simpl1 = (\n",
    "                train[\"categories\"].iat[idx1],\n",
    "                train[\"country\"].iat[idx1],\n",
    "                train[\"category_simpl\"].iat[idx1],\n",
    "            )\n",
    "            for idx2 in Liste_idx:\n",
    "                # if len(Liste_idx)>30 : continue\n",
    "                if idx1 < idx2:\n",
    "                    id2, lat2, lon2 = (\n",
    "                        train[\"id\"].iat[idx2],\n",
    "                        train[\"latitude\"].iat[idx2],\n",
    "                        train[\"longitude\"].iat[idx2],\n",
    "                    )\n",
    "                    cat2, country2, cat_simpl2 = (\n",
    "                        train[\"categories\"].iat[idx2],\n",
    "                        train[\"country\"].iat[idx2],\n",
    "                        train[\"category_simpl\"].iat[idx2],\n",
    "                    )\n",
    "                    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "                    # same_cat = (cat_simpl1==cat_simpl2 and cat_simpl1>0) or (cat1==cat2 and cat1!='')\n",
    "                    if (\n",
    "                        key not in Cand\n",
    "                        and (cat_simpl1 == 1 or cat_simpl2 == 1)\n",
    "                        and (\n",
    "                            haversine(lat1, lon1, lat2, lon2) <= 100\n",
    "                            or \"kualalumpur\" in name1\n",
    "                        )\n",
    "                    ):\n",
    "                        poi1, poi2 = (\n",
    "                            train[\"point_of_interest\"].iat[idx1],\n",
    "                            train[\"point_of_interest\"].iat[idx2],\n",
    "                        )\n",
    "                        Cand[key] = int(poi1 == poi2)\n",
    "                        Added_p1.append([id1, poi1])\n",
    "                        Added_p2.append([id2, poi2])\n",
    "                        nb_true_matchs_initial += int(poi1 == poi2)\n",
    "\n",
    "        p1 = (\n",
    "            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        p2 = (\n",
    "            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        print(f\"Candidates added for tfidf n°1 (airports) : {len(p1)-size1}/{len(p1)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 28.15M\n",
      "Proportion of positive candidates: 2.48%\n",
      "Proportion of found matches: 97.79%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 60;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF n°2 : metro stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len names : 12760.\n",
      "Nb cand tf-idf : 54354\n",
      "Candidates added for tfidf n°2 (metro stations) : 3147/28155365.\n",
      "Len names : 12740.\n",
      "Nb cand tf-idf : 54516\n",
      "Candidates added for tfidf n°2 (metro stations) : 422/28155787.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 61;\n",
       "                var nbb_unformatted_code = \"far_cat_simpl = [4]\\nthr_tfidf = 0.45\\nthr_distance = 100\\n\\nfor col_name in [\\\"name_initial\\\", \\\"name_initial_decode\\\"]:\\n\\n    Names = train[train[\\\"category_simpl\\\"].isin(far_cat_simpl)][\\n        col_name\\n    ].copy()  # add unknown categories\\n\\n    # Drop stop words\\n    for stopword in [\\\"stasiun\\\", \\\"station\\\", \\\"metro\\\", \\\"\\u5317\\u6539\\u672d\\\", \\\"bei gai zha\\\", \\\"stasiun\\\"]:\\n        Names = Names.apply(lambda x: x.replace(stopword + \\\"s\\\", \\\"\\\"))\\n        Names = Names.apply(lambda x: x.replace(stopword, \\\"\\\"))\\n    Names = Names.apply(lambda x: x.strip())\\n    Names = Names[Names.str.len() > 2]\\n\\n    Names_numrow = {\\n        i: idx for i, idx in enumerate(Names.index)\\n    }  # Keep initial row number\\n    Names = Names.to_list()\\n\\n    print(f\\\"Len names : {len(Names)}.\\\")\\n\\n    # Tf-idf\\n    if 1 < len(Names) < 400000:\\n        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=thr_tfidf)\\n\\n        # no self-matchs and retrieve the initial row number\\n        Tfidf_no_selfmatch = [\\n            [Names_numrow[i], [Names_numrow[x] for x in L if x != i]]\\n            for i, L in enumerate(Tfidf_idx)\\n        ]\\n        Tfidf_no_selfmatch = [x for x in Tfidf_no_selfmatch if len(x[-1]) > 0]\\n        print(\\\"Nb cand tf-idf :\\\", sum([len(L) for idx, L in Tfidf_no_selfmatch]))\\n\\n        # Add matches\\n        size1 = len(p1)\\n        Added_p1, Added_p2 = [], []\\n        for idx1, Liste_idx in Tfidf_no_selfmatch:\\n            id1, lat1, lon1, cat1, cat_simpl1 = (\\n                train[\\\"id\\\"].iat[idx1],\\n                train[\\\"latitude\\\"].iat[idx1],\\n                train[\\\"longitude\\\"].iat[idx1],\\n                train[\\\"categories\\\"].iat[idx1],\\n                train[\\\"category_simpl\\\"].iat[idx1],\\n            )\\n            for idx2 in Liste_idx:\\n                if idx1 < idx2:\\n                    id2, lat2, lon2, cat2, cat_simpl2 = (\\n                        train[\\\"id\\\"].iat[idx2],\\n                        train[\\\"latitude\\\"].iat[idx2],\\n                        train[\\\"longitude\\\"].iat[idx2],\\n                        train[\\\"categories\\\"].iat[idx2],\\n                        train[\\\"category_simpl\\\"].iat[idx2],\\n                    )\\n                    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n                    if (\\n                        key not in Cand\\n                        and haversine(lat1, lon1, lat2, lon2) <= thr_distance\\n                        and (cat_simpl1 in far_cat_simpl or cat_simpl2 in far_cat_simpl)\\n                    ):\\n                        poi1, poi2 = (\\n                            train[\\\"point_of_interest\\\"].iat[idx1],\\n                            train[\\\"point_of_interest\\\"].iat[idx2],\\n                        )\\n                        Cand[key] = int(poi1 == poi2)\\n                        Added_p1.append([id1, poi1])\\n                        Added_p2.append([id2, poi2])\\n                        nb_true_matchs_initial += int(poi1 == poi2)\\n\\n        p1 = (\\n            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        p2 = (\\n            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        print(\\n            f\\\"Candidates added for tfidf n\\u00b02 (metro stations) : {len(p1)-size1}/{len(p1)}.\\\"\\n        )\";\n",
       "                var nbb_formatted_code = \"far_cat_simpl = [4]\\nthr_tfidf = 0.45\\nthr_distance = 100\\n\\nfor col_name in [\\\"name_initial\\\", \\\"name_initial_decode\\\"]:\\n\\n    Names = train[train[\\\"category_simpl\\\"].isin(far_cat_simpl)][\\n        col_name\\n    ].copy()  # add unknown categories\\n\\n    # Drop stop words\\n    for stopword in [\\\"stasiun\\\", \\\"station\\\", \\\"metro\\\", \\\"\\u5317\\u6539\\u672d\\\", \\\"bei gai zha\\\", \\\"stasiun\\\"]:\\n        Names = Names.apply(lambda x: x.replace(stopword + \\\"s\\\", \\\"\\\"))\\n        Names = Names.apply(lambda x: x.replace(stopword, \\\"\\\"))\\n    Names = Names.apply(lambda x: x.strip())\\n    Names = Names[Names.str.len() > 2]\\n\\n    Names_numrow = {\\n        i: idx for i, idx in enumerate(Names.index)\\n    }  # Keep initial row number\\n    Names = Names.to_list()\\n\\n    print(f\\\"Len names : {len(Names)}.\\\")\\n\\n    # Tf-idf\\n    if 1 < len(Names) < 400000:\\n        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=thr_tfidf)\\n\\n        # no self-matchs and retrieve the initial row number\\n        Tfidf_no_selfmatch = [\\n            [Names_numrow[i], [Names_numrow[x] for x in L if x != i]]\\n            for i, L in enumerate(Tfidf_idx)\\n        ]\\n        Tfidf_no_selfmatch = [x for x in Tfidf_no_selfmatch if len(x[-1]) > 0]\\n        print(\\\"Nb cand tf-idf :\\\", sum([len(L) for idx, L in Tfidf_no_selfmatch]))\\n\\n        # Add matches\\n        size1 = len(p1)\\n        Added_p1, Added_p2 = [], []\\n        for idx1, Liste_idx in Tfidf_no_selfmatch:\\n            id1, lat1, lon1, cat1, cat_simpl1 = (\\n                train[\\\"id\\\"].iat[idx1],\\n                train[\\\"latitude\\\"].iat[idx1],\\n                train[\\\"longitude\\\"].iat[idx1],\\n                train[\\\"categories\\\"].iat[idx1],\\n                train[\\\"category_simpl\\\"].iat[idx1],\\n            )\\n            for idx2 in Liste_idx:\\n                if idx1 < idx2:\\n                    id2, lat2, lon2, cat2, cat_simpl2 = (\\n                        train[\\\"id\\\"].iat[idx2],\\n                        train[\\\"latitude\\\"].iat[idx2],\\n                        train[\\\"longitude\\\"].iat[idx2],\\n                        train[\\\"categories\\\"].iat[idx2],\\n                        train[\\\"category_simpl\\\"].iat[idx2],\\n                    )\\n                    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n                    if (\\n                        key not in Cand\\n                        and haversine(lat1, lon1, lat2, lon2) <= thr_distance\\n                        and (cat_simpl1 in far_cat_simpl or cat_simpl2 in far_cat_simpl)\\n                    ):\\n                        poi1, poi2 = (\\n                            train[\\\"point_of_interest\\\"].iat[idx1],\\n                            train[\\\"point_of_interest\\\"].iat[idx2],\\n                        )\\n                        Cand[key] = int(poi1 == poi2)\\n                        Added_p1.append([id1, poi1])\\n                        Added_p2.append([id2, poi2])\\n                        nb_true_matchs_initial += int(poi1 == poi2)\\n\\n        p1 = (\\n            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        p2 = (\\n            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        print(\\n            f\\\"Candidates added for tfidf n\\u00b02 (metro stations) : {len(p1)-size1}/{len(p1)}.\\\"\\n        )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "far_cat_simpl = [4]\n",
    "thr_tfidf = 0.45\n",
    "thr_distance = 100\n",
    "\n",
    "for col_name in [\"name_initial\", \"name_initial_decode\"]:\n",
    "\n",
    "    Names = train[train[\"category_simpl\"].isin(far_cat_simpl)][\n",
    "        col_name\n",
    "    ].copy()  # add unknown categories\n",
    "\n",
    "    # Drop stop words\n",
    "    for stopword in [\"stasiun\", \"station\", \"metro\", \"北改札\", \"bei gai zha\", \"stasiun\"]:\n",
    "        Names = Names.apply(lambda x: x.replace(stopword + \"s\", \"\"))\n",
    "        Names = Names.apply(lambda x: x.replace(stopword, \"\"))\n",
    "    Names = Names.apply(lambda x: x.strip())\n",
    "    Names = Names[Names.str.len() > 2]\n",
    "\n",
    "    Names_numrow = {\n",
    "        i: idx for i, idx in enumerate(Names.index)\n",
    "    }  # Keep initial row number\n",
    "    Names = Names.to_list()\n",
    "\n",
    "    print(f\"Len names : {len(Names)}.\")\n",
    "\n",
    "    # Tf-idf\n",
    "    if 1 < len(Names) < 400000:\n",
    "        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=thr_tfidf)\n",
    "\n",
    "        # no self-matchs and retrieve the initial row number\n",
    "        Tfidf_no_selfmatch = [\n",
    "            [Names_numrow[i], [Names_numrow[x] for x in L if x != i]]\n",
    "            for i, L in enumerate(Tfidf_idx)\n",
    "        ]\n",
    "        Tfidf_no_selfmatch = [x for x in Tfidf_no_selfmatch if len(x[-1]) > 0]\n",
    "        print(\"Nb cand tf-idf :\", sum([len(L) for idx, L in Tfidf_no_selfmatch]))\n",
    "\n",
    "        # Add matches\n",
    "        size1 = len(p1)\n",
    "        Added_p1, Added_p2 = [], []\n",
    "        for idx1, Liste_idx in Tfidf_no_selfmatch:\n",
    "            id1, lat1, lon1, cat1, cat_simpl1 = (\n",
    "                train[\"id\"].iat[idx1],\n",
    "                train[\"latitude\"].iat[idx1],\n",
    "                train[\"longitude\"].iat[idx1],\n",
    "                train[\"categories\"].iat[idx1],\n",
    "                train[\"category_simpl\"].iat[idx1],\n",
    "            )\n",
    "            for idx2 in Liste_idx:\n",
    "                if idx1 < idx2:\n",
    "                    id2, lat2, lon2, cat2, cat_simpl2 = (\n",
    "                        train[\"id\"].iat[idx2],\n",
    "                        train[\"latitude\"].iat[idx2],\n",
    "                        train[\"longitude\"].iat[idx2],\n",
    "                        train[\"categories\"].iat[idx2],\n",
    "                        train[\"category_simpl\"].iat[idx2],\n",
    "                    )\n",
    "                    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "                    if (\n",
    "                        key not in Cand\n",
    "                        and haversine(lat1, lon1, lat2, lon2) <= thr_distance\n",
    "                        and (cat_simpl1 in far_cat_simpl or cat_simpl2 in far_cat_simpl)\n",
    "                    ):\n",
    "                        poi1, poi2 = (\n",
    "                            train[\"point_of_interest\"].iat[idx1],\n",
    "                            train[\"point_of_interest\"].iat[idx2],\n",
    "                        )\n",
    "                        Cand[key] = int(poi1 == poi2)\n",
    "                        Added_p1.append([id1, poi1])\n",
    "                        Added_p2.append([id2, poi2])\n",
    "                        nb_true_matchs_initial += int(poi1 == poi2)\n",
    "\n",
    "        p1 = (\n",
    "            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        p2 = (\n",
    "            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        print(\n",
    "            f\"Candidates added for tfidf n°2 (metro stations) : {len(p1)-size1}/{len(p1)}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 28.16M\n",
      "Proportion of positive candidates: 2.48%\n",
      "Proportion of found matches: 97.80%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 62;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF n°3a : for each countries (with initial unprocessed name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "# Country n°2 : ID.\n",
      "Len names : 110796.\n",
      "Candidates added : 107804/28263591.\n",
      "\n",
      "####################\n",
      "# Country n°3 : TR.\n",
      "Len names : 115177.\n",
      "Candidates added : 13966/28277557.\n",
      "\n",
      "####################\n",
      "# Country n°32 : KW.\n",
      "Len names : 3793.\n",
      "Candidates added : 5363/28282920.\n",
      "\n",
      "-> TF-IDF for contries finished.\n",
      "Candidates added : 127133.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 63;\n",
       "                var nbb_unformatted_code = \"thr_tfidf_ = 0.5\\nthr_distance_ = 20\\nthr_distance_or_same_cat_ = 2\\n\\nsize = len(p1)\\n\\nfor country in [2, 3, 32]:  # range(1, 30)\\n\\n    # Reset parameters\\n    thr_tfidf = thr_tfidf_\\n    thr_distance = thr_distance_\\n    thr_distance_or_same_cat = thr_distance_or_same_cat_\\n\\n    # Tune parameters for each country\\n    if country == 2:\\n        thr_tfidf = 1.1  # will impose to have same category\\n        thr_distance = 20\\n        thr_distance_or_same_cat = -1  # will impose to have same category\\n    elif country == 3:\\n        thr_tfidf = 0.6\\n        thr_distance = 10\\n    elif country == 32:\\n        thr_tfidf = 0.4\\n        thr_distance = 100  # no limit\\n        thr_distance_or_same_cat = 100  # no limit\\n\\n    # List of names\\n    Names = train[train[\\\"country\\\"] == country][\\\"name_initial\\\"].copy()\\n    if len(Names) == 0:\\n        break\\n\\n    print()\\n    print(\\\"#\\\" * 20)\\n    print(f\\\"# Country n\\u00b0{country} : {COUNTRIES[country-1]}.\\\")\\n\\n    Names_numrow = {\\n        i: idx for i, idx in enumerate(Names.index)\\n    }  # Keep initial row number\\n    Names = Names.to_list()\\n\\n    print(f\\\"Len names : {len(Names)}.\\\")\\n\\n    # Tf-idf\\n    if 1 < len(Names) < 400000:\\n        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=min(0.45, thr_tfidf))\\n\\n        # no self-matchs and retrieve the initial row number\\n        Tfidf_idx = [[Names_numrow[x] for x in L] for L in Tfidf_idx]\\n\\n        # Add matches : /!\\\\ ONLY IF THERE IS A CATEGORY MATCH AND THE DISTANCE IS NOT TOO BIG\\n        size1 = len(p1)\\n        Added_p1, Added_p2 = [], []\\n        for idx1, (Liste_idx, Liste_val) in enumerate(zip(Tfidf_idx, Tfidf_val)):\\n            idx1 = Names_numrow[idx1]\\n            id1, lat1, lon1, cat1, cat_simpl1 = (\\n                train[\\\"id\\\"].iat[idx1],\\n                train[\\\"latitude\\\"].iat[idx1],\\n                train[\\\"longitude\\\"].iat[idx1],\\n                train[\\\"categories\\\"].iat[idx1],\\n                train[\\\"category_simpl\\\"].iat[idx1],\\n            )\\n            for idx2, val in zip(Liste_idx, Liste_val):\\n                if idx1 < idx2:\\n                    id2, lat2, lon2, cat2, cat_simpl2 = (\\n                        train[\\\"id\\\"].iat[idx2],\\n                        train[\\\"latitude\\\"].iat[idx2],\\n                        train[\\\"longitude\\\"].iat[idx2],\\n                        train[\\\"categories\\\"].iat[idx2],\\n                        train[\\\"category_simpl\\\"].iat[idx2],\\n                    )\\n                    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n                    dist = haversine(lat1, lon1, lat2, lon2)\\n                    same_cat = (cat_simpl1 == cat_simpl2 and cat_simpl1 > 0) or (\\n                        cat1 == cat2 and cat1 != \\\"\\\" and cat1 != \\\"nan\\\"\\n                    )\\n                    if (\\n                        key not in Cand\\n                        and dist <= thr_distance\\n                        and (same_cat or dist <= thr_distance_or_same_cat)\\n                        and (same_cat or val >= thr_tfidf)\\n                    ):\\n                        poi1, poi2 = (\\n                            train[\\\"point_of_interest\\\"].iat[idx1],\\n                            train[\\\"point_of_interest\\\"].iat[idx2],\\n                        )\\n                        Cand[key] = int(poi1 == poi2)\\n                        Added_p1.append([id1, poi1])\\n                        Added_p2.append([id2, poi2])\\n                        nb_true_matchs_initial += int(poi1 == poi2)\\n\\n        p1 = (\\n            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        p2 = (\\n            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        print(f\\\"Candidates added : {len(p1)-size1}/{len(p1)}.\\\")\\n\\nprint(\\\"\\\\n-> TF-IDF for contries finished.\\\")\\nprint(f\\\"Candidates added : {len(p1)-size}.\\\")\";\n",
       "                var nbb_formatted_code = \"thr_tfidf_ = 0.5\\nthr_distance_ = 20\\nthr_distance_or_same_cat_ = 2\\n\\nsize = len(p1)\\n\\nfor country in [2, 3, 32]:  # range(1, 30)\\n\\n    # Reset parameters\\n    thr_tfidf = thr_tfidf_\\n    thr_distance = thr_distance_\\n    thr_distance_or_same_cat = thr_distance_or_same_cat_\\n\\n    # Tune parameters for each country\\n    if country == 2:\\n        thr_tfidf = 1.1  # will impose to have same category\\n        thr_distance = 20\\n        thr_distance_or_same_cat = -1  # will impose to have same category\\n    elif country == 3:\\n        thr_tfidf = 0.6\\n        thr_distance = 10\\n    elif country == 32:\\n        thr_tfidf = 0.4\\n        thr_distance = 100  # no limit\\n        thr_distance_or_same_cat = 100  # no limit\\n\\n    # List of names\\n    Names = train[train[\\\"country\\\"] == country][\\\"name_initial\\\"].copy()\\n    if len(Names) == 0:\\n        break\\n\\n    print()\\n    print(\\\"#\\\" * 20)\\n    print(f\\\"# Country n\\u00b0{country} : {COUNTRIES[country-1]}.\\\")\\n\\n    Names_numrow = {\\n        i: idx for i, idx in enumerate(Names.index)\\n    }  # Keep initial row number\\n    Names = Names.to_list()\\n\\n    print(f\\\"Len names : {len(Names)}.\\\")\\n\\n    # Tf-idf\\n    if 1 < len(Names) < 400000:\\n        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=min(0.45, thr_tfidf))\\n\\n        # no self-matchs and retrieve the initial row number\\n        Tfidf_idx = [[Names_numrow[x] for x in L] for L in Tfidf_idx]\\n\\n        # Add matches : /!\\\\ ONLY IF THERE IS A CATEGORY MATCH AND THE DISTANCE IS NOT TOO BIG\\n        size1 = len(p1)\\n        Added_p1, Added_p2 = [], []\\n        for idx1, (Liste_idx, Liste_val) in enumerate(zip(Tfidf_idx, Tfidf_val)):\\n            idx1 = Names_numrow[idx1]\\n            id1, lat1, lon1, cat1, cat_simpl1 = (\\n                train[\\\"id\\\"].iat[idx1],\\n                train[\\\"latitude\\\"].iat[idx1],\\n                train[\\\"longitude\\\"].iat[idx1],\\n                train[\\\"categories\\\"].iat[idx1],\\n                train[\\\"category_simpl\\\"].iat[idx1],\\n            )\\n            for idx2, val in zip(Liste_idx, Liste_val):\\n                if idx1 < idx2:\\n                    id2, lat2, lon2, cat2, cat_simpl2 = (\\n                        train[\\\"id\\\"].iat[idx2],\\n                        train[\\\"latitude\\\"].iat[idx2],\\n                        train[\\\"longitude\\\"].iat[idx2],\\n                        train[\\\"categories\\\"].iat[idx2],\\n                        train[\\\"category_simpl\\\"].iat[idx2],\\n                    )\\n                    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n                    dist = haversine(lat1, lon1, lat2, lon2)\\n                    same_cat = (cat_simpl1 == cat_simpl2 and cat_simpl1 > 0) or (\\n                        cat1 == cat2 and cat1 != \\\"\\\" and cat1 != \\\"nan\\\"\\n                    )\\n                    if (\\n                        key not in Cand\\n                        and dist <= thr_distance\\n                        and (same_cat or dist <= thr_distance_or_same_cat)\\n                        and (same_cat or val >= thr_tfidf)\\n                    ):\\n                        poi1, poi2 = (\\n                            train[\\\"point_of_interest\\\"].iat[idx1],\\n                            train[\\\"point_of_interest\\\"].iat[idx2],\\n                        )\\n                        Cand[key] = int(poi1 == poi2)\\n                        Added_p1.append([id1, poi1])\\n                        Added_p2.append([id2, poi2])\\n                        nb_true_matchs_initial += int(poi1 == poi2)\\n\\n        p1 = (\\n            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        p2 = (\\n            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        print(f\\\"Candidates added : {len(p1)-size1}/{len(p1)}.\\\")\\n\\nprint(\\\"\\\\n-> TF-IDF for contries finished.\\\")\\nprint(f\\\"Candidates added : {len(p1)-size}.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thr_tfidf_ = 0.5\n",
    "thr_distance_ = 20\n",
    "thr_distance_or_same_cat_ = 2\n",
    "\n",
    "size = len(p1)\n",
    "\n",
    "for country in [2, 3, 32]:  # range(1, 30)\n",
    "\n",
    "    # Reset parameters\n",
    "    thr_tfidf = thr_tfidf_\n",
    "    thr_distance = thr_distance_\n",
    "    thr_distance_or_same_cat = thr_distance_or_same_cat_\n",
    "\n",
    "    # Tune parameters for each country\n",
    "    if country == 2:\n",
    "        thr_tfidf = 1.1  # will impose to have same category\n",
    "        thr_distance = 20\n",
    "        thr_distance_or_same_cat = -1  # will impose to have same category\n",
    "    elif country == 3:\n",
    "        thr_tfidf = 0.6\n",
    "        thr_distance = 10\n",
    "    elif country == 32:\n",
    "        thr_tfidf = 0.4\n",
    "        thr_distance = 100  # no limit\n",
    "        thr_distance_or_same_cat = 100  # no limit\n",
    "\n",
    "    # List of names\n",
    "    Names = train[train[\"country\"] == country][\"name_initial\"].copy()\n",
    "    if len(Names) == 0:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    print(\"#\" * 20)\n",
    "    print(f\"# Country n°{country} : {COUNTRIES[country-1]}.\")\n",
    "\n",
    "    Names_numrow = {\n",
    "        i: idx for i, idx in enumerate(Names.index)\n",
    "    }  # Keep initial row number\n",
    "    Names = Names.to_list()\n",
    "\n",
    "    print(f\"Len names : {len(Names)}.\")\n",
    "\n",
    "    # Tf-idf\n",
    "    if 1 < len(Names) < 400000:\n",
    "        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=min(0.45, thr_tfidf))\n",
    "\n",
    "        # no self-matchs and retrieve the initial row number\n",
    "        Tfidf_idx = [[Names_numrow[x] for x in L] for L in Tfidf_idx]\n",
    "\n",
    "        # Add matches : /!\\ ONLY IF THERE IS A CATEGORY MATCH AND THE DISTANCE IS NOT TOO BIG\n",
    "        size1 = len(p1)\n",
    "        Added_p1, Added_p2 = [], []\n",
    "        for idx1, (Liste_idx, Liste_val) in enumerate(zip(Tfidf_idx, Tfidf_val)):\n",
    "            idx1 = Names_numrow[idx1]\n",
    "            id1, lat1, lon1, cat1, cat_simpl1 = (\n",
    "                train[\"id\"].iat[idx1],\n",
    "                train[\"latitude\"].iat[idx1],\n",
    "                train[\"longitude\"].iat[idx1],\n",
    "                train[\"categories\"].iat[idx1],\n",
    "                train[\"category_simpl\"].iat[idx1],\n",
    "            )\n",
    "            for idx2, val in zip(Liste_idx, Liste_val):\n",
    "                if idx1 < idx2:\n",
    "                    id2, lat2, lon2, cat2, cat_simpl2 = (\n",
    "                        train[\"id\"].iat[idx2],\n",
    "                        train[\"latitude\"].iat[idx2],\n",
    "                        train[\"longitude\"].iat[idx2],\n",
    "                        train[\"categories\"].iat[idx2],\n",
    "                        train[\"category_simpl\"].iat[idx2],\n",
    "                    )\n",
    "                    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "                    dist = haversine(lat1, lon1, lat2, lon2)\n",
    "                    same_cat = (cat_simpl1 == cat_simpl2 and cat_simpl1 > 0) or (\n",
    "                        cat1 == cat2 and cat1 != \"\" and cat1 != \"nan\"\n",
    "                    )\n",
    "                    if (\n",
    "                        key not in Cand\n",
    "                        and dist <= thr_distance\n",
    "                        and (same_cat or dist <= thr_distance_or_same_cat)\n",
    "                        and (same_cat or val >= thr_tfidf)\n",
    "                    ):\n",
    "                        poi1, poi2 = (\n",
    "                            train[\"point_of_interest\"].iat[idx1],\n",
    "                            train[\"point_of_interest\"].iat[idx2],\n",
    "                        )\n",
    "                        Cand[key] = int(poi1 == poi2)\n",
    "                        Added_p1.append([id1, poi1])\n",
    "                        Added_p2.append([id2, poi2])\n",
    "                        nb_true_matchs_initial += int(poi1 == poi2)\n",
    "\n",
    "        p1 = (\n",
    "            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        p2 = (\n",
    "            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        print(f\"Candidates added : {len(p1)-size1}/{len(p1)}.\")\n",
    "\n",
    "print(\"\\n-> TF-IDF for contries finished.\")\n",
    "print(f\"Candidates added : {len(p1)-size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 28.28M\n",
      "Proportion of positive candidates: 2.49%\n",
      "Proportion of found matches: 97.83%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 64;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF n°3b : for each countries (with few processed name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "# Country n°32 : KW.\n",
      "Len names : 3793.\n",
      "Candidates added : 187.\n",
      "\n",
      "-> TF-IDF for contries finished.\n",
      "Candidates added : 187.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 65;\n",
       "                var nbb_unformatted_code = \"thr_tfidf_ = 0.45\\nthr_distance_ = 25\\nthr_distance_or_same_cat_ = 10\\n\\nsize = len(p1)\\n\\nfor country in [32]:  # range(1, 30)\\n\\n    # Reset parameter\\n    thr_tfidf = thr_tfidf_\\n    thr_distance = thr_distance_\\n    thr_distance_or_same_cat = thr_distance_or_same_cat_\\n\\n    # Tune parameters for each country\\n    if country == 32:\\n        thr_tfidf_ = 0.4\\n        thr_distance = 100  # no limit\\n        thr_distance_or_same_cat = 100  # no limit\\n\\n    Names = train[train[\\\"country\\\"] == country][\\\"name_initial_decode\\\"].copy()\\n    if len(Names) == 0:\\n        break\\n\\n    print()\\n    print(\\\"#\\\" * 20)\\n    print(f\\\"# Country n\\u00b0{country} : {COUNTRIES[country-1]}.\\\")\\n\\n    Names_numrow = {\\n        i: idx for i, idx in enumerate(Names.index)\\n    }  # Keep initial row number\\n    Names = Names.to_list()\\n\\n    print(f\\\"Len names : {len(Names)}.\\\")\\n\\n    # Tf-idf\\n    if 1 < len(Names) < 400000:\\n        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=min(0.4, thr_tfidf))\\n\\n        # no self-matchs and retrieve the initial row number\\n        Tfidf_idx = [[Names_numrow[x] for x in L] for L in Tfidf_idx]\\n\\n        # Add matches : /!\\\\ ONLY IF THERE IS A CATEGORY MATCH AND THE DISTANCE IS NOT TOO BIG\\n        size1 = len(p1)\\n        Added_p1, Added_p2 = [], []\\n        for idx1, (Liste_idx, Liste_val) in enumerate(zip(Tfidf_idx, Tfidf_val)):\\n            idx1 = Names_numrow[idx1]\\n            id1, lat1, lon1, cat1, cat_simpl1 = (\\n                train[\\\"id\\\"].iat[idx1],\\n                train[\\\"latitude\\\"].iat[idx1],\\n                train[\\\"longitude\\\"].iat[idx1],\\n                train[\\\"categories\\\"].iat[idx1],\\n                train[\\\"category_simpl\\\"].iat[idx1],\\n            )\\n            for idx2, val in zip(Liste_idx, Liste_val):\\n                if idx1 < idx2:\\n                    id2, lat2, lon2, cat2, cat_simpl2 = (\\n                        train[\\\"id\\\"].iat[idx2],\\n                        train[\\\"latitude\\\"].iat[idx2],\\n                        train[\\\"longitude\\\"].iat[idx2],\\n                        train[\\\"categories\\\"].iat[idx2],\\n                        train[\\\"category_simpl\\\"].iat[idx2],\\n                    )\\n                    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n                    dist = haversine(lat1, lon1, lat2, lon2)\\n                    same_cat = (cat_simpl1 == cat_simpl2 and cat_simpl1 > 0) or (\\n                        cat1 == cat2 and cat1 != \\\"\\\" and cat1 != \\\"nan\\\"\\n                    )\\n                    if (\\n                        key not in Cand\\n                        and dist <= thr_distance\\n                        and (same_cat or dist <= thr_distance_or_same_cat)\\n                        and (same_cat or val >= thr_tfidf)\\n                    ):\\n                        poi1, poi2 = (\\n                            train[\\\"point_of_interest\\\"].iat[idx1],\\n                            train[\\\"point_of_interest\\\"].iat[idx2],\\n                        )\\n                        Cand[key] = int(poi1 == poi2)\\n                        Added_p1.append([id1, poi1])\\n                        Added_p2.append([id2, poi2])\\n                        nb_true_matchs_initial += int(poi1 == poi2)\\n\\n        p1 = (\\n            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        p2 = (\\n            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        print(f\\\"Candidates added : {len(p1)-size1}.\\\")\\n\\nprint(\\\"\\\\n-> TF-IDF for contries finished.\\\")\\nprint(f\\\"Candidates added : {len(p1)-size}.\\\")\";\n",
       "                var nbb_formatted_code = \"thr_tfidf_ = 0.45\\nthr_distance_ = 25\\nthr_distance_or_same_cat_ = 10\\n\\nsize = len(p1)\\n\\nfor country in [32]:  # range(1, 30)\\n\\n    # Reset parameter\\n    thr_tfidf = thr_tfidf_\\n    thr_distance = thr_distance_\\n    thr_distance_or_same_cat = thr_distance_or_same_cat_\\n\\n    # Tune parameters for each country\\n    if country == 32:\\n        thr_tfidf_ = 0.4\\n        thr_distance = 100  # no limit\\n        thr_distance_or_same_cat = 100  # no limit\\n\\n    Names = train[train[\\\"country\\\"] == country][\\\"name_initial_decode\\\"].copy()\\n    if len(Names) == 0:\\n        break\\n\\n    print()\\n    print(\\\"#\\\" * 20)\\n    print(f\\\"# Country n\\u00b0{country} : {COUNTRIES[country-1]}.\\\")\\n\\n    Names_numrow = {\\n        i: idx for i, idx in enumerate(Names.index)\\n    }  # Keep initial row number\\n    Names = Names.to_list()\\n\\n    print(f\\\"Len names : {len(Names)}.\\\")\\n\\n    # Tf-idf\\n    if 1 < len(Names) < 400000:\\n        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=min(0.4, thr_tfidf))\\n\\n        # no self-matchs and retrieve the initial row number\\n        Tfidf_idx = [[Names_numrow[x] for x in L] for L in Tfidf_idx]\\n\\n        # Add matches : /!\\\\ ONLY IF THERE IS A CATEGORY MATCH AND THE DISTANCE IS NOT TOO BIG\\n        size1 = len(p1)\\n        Added_p1, Added_p2 = [], []\\n        for idx1, (Liste_idx, Liste_val) in enumerate(zip(Tfidf_idx, Tfidf_val)):\\n            idx1 = Names_numrow[idx1]\\n            id1, lat1, lon1, cat1, cat_simpl1 = (\\n                train[\\\"id\\\"].iat[idx1],\\n                train[\\\"latitude\\\"].iat[idx1],\\n                train[\\\"longitude\\\"].iat[idx1],\\n                train[\\\"categories\\\"].iat[idx1],\\n                train[\\\"category_simpl\\\"].iat[idx1],\\n            )\\n            for idx2, val in zip(Liste_idx, Liste_val):\\n                if idx1 < idx2:\\n                    id2, lat2, lon2, cat2, cat_simpl2 = (\\n                        train[\\\"id\\\"].iat[idx2],\\n                        train[\\\"latitude\\\"].iat[idx2],\\n                        train[\\\"longitude\\\"].iat[idx2],\\n                        train[\\\"categories\\\"].iat[idx2],\\n                        train[\\\"category_simpl\\\"].iat[idx2],\\n                    )\\n                    key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n                    dist = haversine(lat1, lon1, lat2, lon2)\\n                    same_cat = (cat_simpl1 == cat_simpl2 and cat_simpl1 > 0) or (\\n                        cat1 == cat2 and cat1 != \\\"\\\" and cat1 != \\\"nan\\\"\\n                    )\\n                    if (\\n                        key not in Cand\\n                        and dist <= thr_distance\\n                        and (same_cat or dist <= thr_distance_or_same_cat)\\n                        and (same_cat or val >= thr_tfidf)\\n                    ):\\n                        poi1, poi2 = (\\n                            train[\\\"point_of_interest\\\"].iat[idx1],\\n                            train[\\\"point_of_interest\\\"].iat[idx2],\\n                        )\\n                        Cand[key] = int(poi1 == poi2)\\n                        Added_p1.append([id1, poi1])\\n                        Added_p2.append([id2, poi2])\\n                        nb_true_matchs_initial += int(poi1 == poi2)\\n\\n        p1 = (\\n            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        p2 = (\\n            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n            .reset_index(drop=True)\\n            .copy()\\n        )\\n        print(f\\\"Candidates added : {len(p1)-size1}.\\\")\\n\\nprint(\\\"\\\\n-> TF-IDF for contries finished.\\\")\\nprint(f\\\"Candidates added : {len(p1)-size}.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thr_tfidf_ = 0.45\n",
    "thr_distance_ = 25\n",
    "thr_distance_or_same_cat_ = 10\n",
    "\n",
    "size = len(p1)\n",
    "\n",
    "for country in [32]:  # range(1, 30)\n",
    "\n",
    "    # Reset parameter\n",
    "    thr_tfidf = thr_tfidf_\n",
    "    thr_distance = thr_distance_\n",
    "    thr_distance_or_same_cat = thr_distance_or_same_cat_\n",
    "\n",
    "    # Tune parameters for each country\n",
    "    if country == 32:\n",
    "        thr_tfidf_ = 0.4\n",
    "        thr_distance = 100  # no limit\n",
    "        thr_distance_or_same_cat = 100  # no limit\n",
    "\n",
    "    Names = train[train[\"country\"] == country][\"name_initial_decode\"].copy()\n",
    "    if len(Names) == 0:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    print(\"#\" * 20)\n",
    "    print(f\"# Country n°{country} : {COUNTRIES[country-1]}.\")\n",
    "\n",
    "    Names_numrow = {\n",
    "        i: idx for i, idx in enumerate(Names.index)\n",
    "    }  # Keep initial row number\n",
    "    Names = Names.to_list()\n",
    "\n",
    "    print(f\"Len names : {len(Names)}.\")\n",
    "\n",
    "    # Tf-idf\n",
    "    if 1 < len(Names) < 400000:\n",
    "        Tfidf_idx, Tfidf_val = vectorisation_similarite(Names, thr=min(0.4, thr_tfidf))\n",
    "\n",
    "        # no self-matchs and retrieve the initial row number\n",
    "        Tfidf_idx = [[Names_numrow[x] for x in L] for L in Tfidf_idx]\n",
    "\n",
    "        # Add matches : /!\\ ONLY IF THERE IS A CATEGORY MATCH AND THE DISTANCE IS NOT TOO BIG\n",
    "        size1 = len(p1)\n",
    "        Added_p1, Added_p2 = [], []\n",
    "        for idx1, (Liste_idx, Liste_val) in enumerate(zip(Tfidf_idx, Tfidf_val)):\n",
    "            idx1 = Names_numrow[idx1]\n",
    "            id1, lat1, lon1, cat1, cat_simpl1 = (\n",
    "                train[\"id\"].iat[idx1],\n",
    "                train[\"latitude\"].iat[idx1],\n",
    "                train[\"longitude\"].iat[idx1],\n",
    "                train[\"categories\"].iat[idx1],\n",
    "                train[\"category_simpl\"].iat[idx1],\n",
    "            )\n",
    "            for idx2, val in zip(Liste_idx, Liste_val):\n",
    "                if idx1 < idx2:\n",
    "                    id2, lat2, lon2, cat2, cat_simpl2 = (\n",
    "                        train[\"id\"].iat[idx2],\n",
    "                        train[\"latitude\"].iat[idx2],\n",
    "                        train[\"longitude\"].iat[idx2],\n",
    "                        train[\"categories\"].iat[idx2],\n",
    "                        train[\"category_simpl\"].iat[idx2],\n",
    "                    )\n",
    "                    key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "                    dist = haversine(lat1, lon1, lat2, lon2)\n",
    "                    same_cat = (cat_simpl1 == cat_simpl2 and cat_simpl1 > 0) or (\n",
    "                        cat1 == cat2 and cat1 != \"\" and cat1 != \"nan\"\n",
    "                    )\n",
    "                    if (\n",
    "                        key not in Cand\n",
    "                        and dist <= thr_distance\n",
    "                        and (same_cat or dist <= thr_distance_or_same_cat)\n",
    "                        and (same_cat or val >= thr_tfidf)\n",
    "                    ):\n",
    "                        poi1, poi2 = (\n",
    "                            train[\"point_of_interest\"].iat[idx1],\n",
    "                            train[\"point_of_interest\"].iat[idx2],\n",
    "                        )\n",
    "                        Cand[key] = int(poi1 == poi2)\n",
    "                        Added_p1.append([id1, poi1])\n",
    "                        Added_p2.append([id2, poi2])\n",
    "                        nb_true_matchs_initial += int(poi1 == poi2)\n",
    "\n",
    "        p1 = (\n",
    "            p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        p2 = (\n",
    "            p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "            .reset_index(drop=True)\n",
    "            .copy()\n",
    "        )\n",
    "        print(f\"Candidates added : {len(p1)-size1}.\")\n",
    "\n",
    "print(\"\\n-> TF-IDF for contries finished.\")\n",
    "print(f\"Candidates added : {len(p1)-size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 28.28M\n",
      "Proportion of positive candidates: 2.49%\n",
      "Proportion of found matches: 97.83%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 66;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add candidates based on same name/phone/address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 67;\n",
       "                var nbb_unformatted_code = \"# Cr\\u00e9ation d'un df de travail\\nwork = train.copy()\\n\\n# Prepare columns\\nfor c in [\\\"name\\\", \\\"address\\\", \\\"city\\\"]:\\n    work[c] = work[c].astype(str).str.lower()\\nwork[\\\"index\\\"] = work.index\\n\\n\\nwork_names = work.groupby(\\\"name\\\")[\\\"index\\\"].apply(list).to_frame().reset_index()\\nwork_names = dict(zip(work_names[\\\"name\\\"], work_names[\\\"index\\\"]))\\nwork_names = {\\n    name: Liste_idx\\n    for name, Liste_idx in work_names.items()\\n    if len(name) >= 2 and len(Liste_idx) <= 25\\n}  # Don't consider too widespread names\\n\\n\\nwork_phones = work.groupby(\\\"phone\\\")[\\\"index\\\"].apply(list).to_frame().reset_index()\\nwork_phones = dict(zip(work_phones[\\\"phone\\\"], work_phones[\\\"index\\\"]))\\nwork_phones = {\\n    phone: Liste_idx\\n    for phone, Liste_idx in work_phones.items()\\n    if len(phone) >= 3 and len(Liste_idx) <= 10\\n}  # Don't consider too widespread phone\\n\\n\\nwork[\\\"address_complet\\\"] = work.apply(\\n    lambda row: create_address(row[\\\"address\\\"], row[\\\"city\\\"]), axis=1\\n)\\nwork_address = (\\n    work.groupby(\\\"address_complet\\\")[\\\"index\\\"].apply(list).to_frame().reset_index()\\n)\\nwork_address = dict(zip(work_address[\\\"address_complet\\\"], work_address[\\\"index\\\"]))\\nwork_address = {\\n    address: Liste_idx\\n    for address, Liste_idx in work_address.items()\\n    if len(address) >= 3 and len(Liste_idx) <= 10\\n}  # Don't consider too widespread address\";\n",
       "                var nbb_formatted_code = \"# Cr\\u00e9ation d'un df de travail\\nwork = train.copy()\\n\\n# Prepare columns\\nfor c in [\\\"name\\\", \\\"address\\\", \\\"city\\\"]:\\n    work[c] = work[c].astype(str).str.lower()\\nwork[\\\"index\\\"] = work.index\\n\\n\\nwork_names = work.groupby(\\\"name\\\")[\\\"index\\\"].apply(list).to_frame().reset_index()\\nwork_names = dict(zip(work_names[\\\"name\\\"], work_names[\\\"index\\\"]))\\nwork_names = {\\n    name: Liste_idx\\n    for name, Liste_idx in work_names.items()\\n    if len(name) >= 2 and len(Liste_idx) <= 25\\n}  # Don't consider too widespread names\\n\\n\\nwork_phones = work.groupby(\\\"phone\\\")[\\\"index\\\"].apply(list).to_frame().reset_index()\\nwork_phones = dict(zip(work_phones[\\\"phone\\\"], work_phones[\\\"index\\\"]))\\nwork_phones = {\\n    phone: Liste_idx\\n    for phone, Liste_idx in work_phones.items()\\n    if len(phone) >= 3 and len(Liste_idx) <= 10\\n}  # Don't consider too widespread phone\\n\\n\\nwork[\\\"address_complet\\\"] = work.apply(\\n    lambda row: create_address(row[\\\"address\\\"], row[\\\"city\\\"]), axis=1\\n)\\nwork_address = (\\n    work.groupby(\\\"address_complet\\\")[\\\"index\\\"].apply(list).to_frame().reset_index()\\n)\\nwork_address = dict(zip(work_address[\\\"address_complet\\\"], work_address[\\\"index\\\"]))\\nwork_address = {\\n    address: Liste_idx\\n    for address, Liste_idx in work_address.items()\\n    if len(address) >= 3 and len(Liste_idx) <= 10\\n}  # Don't consider too widespread address\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Création d'un df de travail\n",
    "work = train.copy()\n",
    "\n",
    "# Prepare columns\n",
    "for c in [\"name\", \"address\", \"city\"]:\n",
    "    work[c] = work[c].astype(str).str.lower()\n",
    "work[\"index\"] = work.index\n",
    "\n",
    "\n",
    "work_names = work.groupby(\"name\")[\"index\"].apply(list).to_frame().reset_index()\n",
    "work_names = dict(zip(work_names[\"name\"], work_names[\"index\"]))\n",
    "work_names = {\n",
    "    name: Liste_idx\n",
    "    for name, Liste_idx in work_names.items()\n",
    "    if len(name) >= 2 and len(Liste_idx) <= 25\n",
    "}  # Don't consider too widespread names\n",
    "\n",
    "\n",
    "work_phones = work.groupby(\"phone\")[\"index\"].apply(list).to_frame().reset_index()\n",
    "work_phones = dict(zip(work_phones[\"phone\"], work_phones[\"index\"]))\n",
    "work_phones = {\n",
    "    phone: Liste_idx\n",
    "    for phone, Liste_idx in work_phones.items()\n",
    "    if len(phone) >= 3 and len(Liste_idx) <= 10\n",
    "}  # Don't consider too widespread phone\n",
    "\n",
    "\n",
    "work[\"address_complet\"] = work.apply(\n",
    "    lambda row: create_address(row[\"address\"], row[\"city\"]), axis=1\n",
    ")\n",
    "work_address = (\n",
    "    work.groupby(\"address_complet\")[\"index\"].apply(list).to_frame().reset_index()\n",
    ")\n",
    "work_address = dict(zip(work_address[\"address_complet\"], work_address[\"index\"]))\n",
    "work_address = {\n",
    "    address: Liste_idx\n",
    "    for address, Liste_idx in work_address.items()\n",
    "    if len(address) >= 3 and len(Liste_idx) <= 10\n",
    "}  # Don't consider too widespread address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential match on name/phone/address : 1839281.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 68;\n",
       "                var nbb_unformatted_code = \"# Process\\n# tqdm.pandas()\\n# Potential_on_NamePhone = work.progress_apply(find_potential_matchs, axis=1).to_list()\\nPotential_on_NamePhone = work.apply(\\n    lambda row: find_potential_matchs(row, work_names, work_phones, work_address),\\n    axis=1,\\n).to_list()\\n\\n# Don't keep pairs too far from each other\\nPotential_on_NamePhone_new = []\\n\\n# Numpy for faster process\\ntrain_numpy = train[[\\\"name\\\", \\\"latitude\\\", \\\"longitude\\\", \\\"category_simpl\\\"]].to_numpy()\\n\\n# Filtre on dist\\nfor i, Liste_idx in enumerate(Potential_on_NamePhone):\\n    new = []\\n    name1, lat1, lon1, cat_simpl1 = (\\n        train_numpy[i][0],\\n        train_numpy[i][1],\\n        train_numpy[i][2],\\n        train_numpy[i][3],\\n    )\\n    for j, row in enumerate(train_numpy[Liste_idx]):\\n        name2, lat2, lon2, cat_simpl2 = row[0], row[1], row[2], row[3]\\n\\n        # if rare name, we are more tolerant\\n        if name1 == name2 and len(work_names[name1]) <= 5:\\n            thr_distance = 100\\n        else:\\n            thr_distance = 26\\n\\n        # if the category is usually far even for matchs\\n        if (cat_simpl1 in far_cat_simpl) or (cat_simpl2 in far_cat_simpl):\\n            thr_distance = 350\\n            if (cat_simpl1 == 1) or (cat_simpl2 == 1):\\n                thr_distance = 100000  # no limit\\n\\n        # Add distance if long names (not a coincidence if they are equal)\\n        if name1 == name2 and len(name1) >= 10:\\n            thr_distance += 15\\n\\n        # Process\\n        if haversine(lat1, lon1, lat2, lon2) > thr_distance:\\n            continue\\n        else:\\n            new.append(Liste_idx[j])\\n    Potential_on_NamePhone_new.append(new.copy())\\n\\nPotential_on_NamePhone = Potential_on_NamePhone_new.copy()\\n\\ndel Potential_on_NamePhone_new, train_numpy\\ngc.collect()\\n\\n# Number of potential matchs\\nprint(\\n    f\\\"Potential match on name/phone/address : {sum(len(x) for x in Potential_on_NamePhone)}.\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"# Process\\n# tqdm.pandas()\\n# Potential_on_NamePhone = work.progress_apply(find_potential_matchs, axis=1).to_list()\\nPotential_on_NamePhone = work.apply(\\n    lambda row: find_potential_matchs(row, work_names, work_phones, work_address),\\n    axis=1,\\n).to_list()\\n\\n# Don't keep pairs too far from each other\\nPotential_on_NamePhone_new = []\\n\\n# Numpy for faster process\\ntrain_numpy = train[[\\\"name\\\", \\\"latitude\\\", \\\"longitude\\\", \\\"category_simpl\\\"]].to_numpy()\\n\\n# Filtre on dist\\nfor i, Liste_idx in enumerate(Potential_on_NamePhone):\\n    new = []\\n    name1, lat1, lon1, cat_simpl1 = (\\n        train_numpy[i][0],\\n        train_numpy[i][1],\\n        train_numpy[i][2],\\n        train_numpy[i][3],\\n    )\\n    for j, row in enumerate(train_numpy[Liste_idx]):\\n        name2, lat2, lon2, cat_simpl2 = row[0], row[1], row[2], row[3]\\n\\n        # if rare name, we are more tolerant\\n        if name1 == name2 and len(work_names[name1]) <= 5:\\n            thr_distance = 100\\n        else:\\n            thr_distance = 26\\n\\n        # if the category is usually far even for matchs\\n        if (cat_simpl1 in far_cat_simpl) or (cat_simpl2 in far_cat_simpl):\\n            thr_distance = 350\\n            if (cat_simpl1 == 1) or (cat_simpl2 == 1):\\n                thr_distance = 100000  # no limit\\n\\n        # Add distance if long names (not a coincidence if they are equal)\\n        if name1 == name2 and len(name1) >= 10:\\n            thr_distance += 15\\n\\n        # Process\\n        if haversine(lat1, lon1, lat2, lon2) > thr_distance:\\n            continue\\n        else:\\n            new.append(Liste_idx[j])\\n    Potential_on_NamePhone_new.append(new.copy())\\n\\nPotential_on_NamePhone = Potential_on_NamePhone_new.copy()\\n\\ndel Potential_on_NamePhone_new, train_numpy\\ngc.collect()\\n\\n# Number of potential matchs\\nprint(\\n    f\\\"Potential match on name/phone/address : {sum(len(x) for x in Potential_on_NamePhone)}.\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process\n",
    "# tqdm.pandas()\n",
    "# Potential_on_NamePhone = work.progress_apply(find_potential_matchs, axis=1).to_list()\n",
    "Potential_on_NamePhone = work.apply(\n",
    "    lambda row: find_potential_matchs(row, work_names, work_phones, work_address),\n",
    "    axis=1,\n",
    ").to_list()\n",
    "\n",
    "# Don't keep pairs too far from each other\n",
    "Potential_on_NamePhone_new = []\n",
    "\n",
    "# Numpy for faster process\n",
    "train_numpy = train[[\"name\", \"latitude\", \"longitude\", \"category_simpl\"]].to_numpy()\n",
    "\n",
    "# Filtre on dist\n",
    "for i, Liste_idx in enumerate(Potential_on_NamePhone):\n",
    "    new = []\n",
    "    name1, lat1, lon1, cat_simpl1 = (\n",
    "        train_numpy[i][0],\n",
    "        train_numpy[i][1],\n",
    "        train_numpy[i][2],\n",
    "        train_numpy[i][3],\n",
    "    )\n",
    "    for j, row in enumerate(train_numpy[Liste_idx]):\n",
    "        name2, lat2, lon2, cat_simpl2 = row[0], row[1], row[2], row[3]\n",
    "\n",
    "        # if rare name, we are more tolerant\n",
    "        if name1 == name2 and len(work_names[name1]) <= 5:\n",
    "            thr_distance = 100\n",
    "        else:\n",
    "            thr_distance = 26\n",
    "\n",
    "        # if the category is usually far even for matchs\n",
    "        if (cat_simpl1 in far_cat_simpl) or (cat_simpl2 in far_cat_simpl):\n",
    "            thr_distance = 350\n",
    "            if (cat_simpl1 == 1) or (cat_simpl2 == 1):\n",
    "                thr_distance = 100000  # no limit\n",
    "\n",
    "        # Add distance if long names (not a coincidence if they are equal)\n",
    "        if name1 == name2 and len(name1) >= 10:\n",
    "            thr_distance += 15\n",
    "\n",
    "        # Process\n",
    "        if haversine(lat1, lon1, lat2, lon2) > thr_distance:\n",
    "            continue\n",
    "        else:\n",
    "            new.append(Liste_idx[j])\n",
    "    Potential_on_NamePhone_new.append(new.copy())\n",
    "\n",
    "Potential_on_NamePhone = Potential_on_NamePhone_new.copy()\n",
    "\n",
    "del Potential_on_NamePhone_new, train_numpy\n",
    "gc.collect()\n",
    "\n",
    "# Number of potential matchs\n",
    "print(\n",
    "    f\"Potential match on name/phone/address : {sum(len(x) for x in Potential_on_NamePhone)}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates added with name/phone/address similarity : 18774.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 69;\n",
       "                var nbb_unformatted_code = \"# Add matches\\nsize1 = len(p1)\\nAdded_p1, Added_p2 = [], []\\ntry:\\n    seen\\nexcept:\\n    seen = set()\\nfor idx1, Liste_idx in enumerate(Potential_on_NamePhone):\\n\\n    id1, lat1, lon1, cat1, cat_simpl1 = (\\n        train[\\\"id\\\"].iat[idx1],\\n        train[\\\"latitude\\\"].iat[idx1],\\n        train[\\\"longitude\\\"].iat[idx1],\\n        train[\\\"categories\\\"].iat[idx1],\\n        train[\\\"category_simpl\\\"].iat[idx1],\\n    )\\n    for idx2 in Liste_idx:\\n        if idx1 != idx2:\\n            id2, lat2, lon2, cat2, cat_simpl2 = (\\n                train[\\\"id\\\"].iat[idx2],\\n                train[\\\"latitude\\\"].iat[idx2],\\n                train[\\\"longitude\\\"].iat[idx2],\\n                train[\\\"categories\\\"].iat[idx2],\\n                train[\\\"category_simpl\\\"].iat[idx2],\\n            )\\n            key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n            # same_cat = (cat_simpl1==cat_simpl2 and cat_simpl1>0) or (cat1==cat2 and cat1!='')\\n            if key not in Cand and key not in seen:\\n                seen.add(key)\\n                poi1, poi2 = (\\n                    train[\\\"point_of_interest\\\"].iat[idx1],\\n                    train[\\\"point_of_interest\\\"].iat[idx2],\\n                )\\n                Added_p1.append([id1, poi1])\\n                Added_p2.append([id2, poi2])\\n\\np1 = (\\n    p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n    .reset_index(drop=True)\\n    .copy()\\n)\\np2 = (\\n    p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n    .reset_index(drop=True)\\n    .copy()\\n)\\nprint(f\\\"Candidates added with name/phone/address similarity : {len(p1)-size1}.\\\")\\n\\ndel (\\n    Tfidf_idx,\\n    Tfidf_val,\\n    Cand,\\n    ID_to_POI,\\n    Potential_on_NamePhone,\\n    work,\\n    work_names,\\n    work_phones,\\n    work_address,\\n    Added_p1,\\n    Added_p2,\\n    seen,\\n    Names_numrow,\\n)\\ngc.collect()\";\n",
       "                var nbb_formatted_code = \"# Add matches\\nsize1 = len(p1)\\nAdded_p1, Added_p2 = [], []\\ntry:\\n    seen\\nexcept:\\n    seen = set()\\nfor idx1, Liste_idx in enumerate(Potential_on_NamePhone):\\n\\n    id1, lat1, lon1, cat1, cat_simpl1 = (\\n        train[\\\"id\\\"].iat[idx1],\\n        train[\\\"latitude\\\"].iat[idx1],\\n        train[\\\"longitude\\\"].iat[idx1],\\n        train[\\\"categories\\\"].iat[idx1],\\n        train[\\\"category_simpl\\\"].iat[idx1],\\n    )\\n    for idx2 in Liste_idx:\\n        if idx1 != idx2:\\n            id2, lat2, lon2, cat2, cat_simpl2 = (\\n                train[\\\"id\\\"].iat[idx2],\\n                train[\\\"latitude\\\"].iat[idx2],\\n                train[\\\"longitude\\\"].iat[idx2],\\n                train[\\\"categories\\\"].iat[idx2],\\n                train[\\\"category_simpl\\\"].iat[idx2],\\n            )\\n            key = f\\\"{min(id1, id2)}-{max(id1, id2)}\\\"\\n            # same_cat = (cat_simpl1==cat_simpl2 and cat_simpl1>0) or (cat1==cat2 and cat1!='')\\n            if key not in Cand and key not in seen:\\n                seen.add(key)\\n                poi1, poi2 = (\\n                    train[\\\"point_of_interest\\\"].iat[idx1],\\n                    train[\\\"point_of_interest\\\"].iat[idx2],\\n                )\\n                Added_p1.append([id1, poi1])\\n                Added_p2.append([id2, poi2])\\n\\np1 = (\\n    p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\\n    .reset_index(drop=True)\\n    .copy()\\n)\\np2 = (\\n    p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\\n    .reset_index(drop=True)\\n    .copy()\\n)\\nprint(f\\\"Candidates added with name/phone/address similarity : {len(p1)-size1}.\\\")\\n\\ndel (\\n    Tfidf_idx,\\n    Tfidf_val,\\n    Cand,\\n    ID_to_POI,\\n    Potential_on_NamePhone,\\n    work,\\n    work_names,\\n    work_phones,\\n    work_address,\\n    Added_p1,\\n    Added_p2,\\n    seen,\\n    Names_numrow,\\n)\\ngc.collect()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add matches\n",
    "size1 = len(p1)\n",
    "Added_p1, Added_p2 = [], []\n",
    "try:\n",
    "    seen\n",
    "except:\n",
    "    seen = set()\n",
    "for idx1, Liste_idx in enumerate(Potential_on_NamePhone):\n",
    "\n",
    "    id1, lat1, lon1, cat1, cat_simpl1 = (\n",
    "        train[\"id\"].iat[idx1],\n",
    "        train[\"latitude\"].iat[idx1],\n",
    "        train[\"longitude\"].iat[idx1],\n",
    "        train[\"categories\"].iat[idx1],\n",
    "        train[\"category_simpl\"].iat[idx1],\n",
    "    )\n",
    "    for idx2 in Liste_idx:\n",
    "        if idx1 != idx2:\n",
    "            id2, lat2, lon2, cat2, cat_simpl2 = (\n",
    "                train[\"id\"].iat[idx2],\n",
    "                train[\"latitude\"].iat[idx2],\n",
    "                train[\"longitude\"].iat[idx2],\n",
    "                train[\"categories\"].iat[idx2],\n",
    "                train[\"category_simpl\"].iat[idx2],\n",
    "            )\n",
    "            key = f\"{min(id1, id2)}-{max(id1, id2)}\"\n",
    "            # same_cat = (cat_simpl1==cat_simpl2 and cat_simpl1>0) or (cat1==cat2 and cat1!='')\n",
    "            if key not in Cand and key not in seen:\n",
    "                seen.add(key)\n",
    "                poi1, poi2 = (\n",
    "                    train[\"point_of_interest\"].iat[idx1],\n",
    "                    train[\"point_of_interest\"].iat[idx2],\n",
    "                )\n",
    "                Added_p1.append([id1, poi1])\n",
    "                Added_p2.append([id2, poi2])\n",
    "\n",
    "p1 = (\n",
    "    p1.append(pd.DataFrame(Added_p1, columns=p1.columns[:2]))\n",
    "    .reset_index(drop=True)\n",
    "    .copy()\n",
    ")\n",
    "p2 = (\n",
    "    p2.append(pd.DataFrame(Added_p2, columns=p2.columns[:2]))\n",
    "    .reset_index(drop=True)\n",
    "    .copy()\n",
    ")\n",
    "print(f\"Candidates added with name/phone/address similarity : {len(p1)-size1}.\")\n",
    "\n",
    "del (\n",
    "    Tfidf_idx,\n",
    "    Tfidf_val,\n",
    "    Cand,\n",
    "    ID_to_POI,\n",
    "    Potential_on_NamePhone,\n",
    "    work,\n",
    "    work_names,\n",
    "    work_phones,\n",
    "    work_address,\n",
    "    Added_p1,\n",
    "    Added_p2,\n",
    "    seen,\n",
    "    Names_numrow,\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 28.3M\n",
      "Proportion of positive candidates: 2.50%\n",
      "Proportion of found matches: 97.85%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 70;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/.local/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 71;\n",
       "                var nbb_unformatted_code = \"# Reset index after Vincent's candidate addition\\np1 = p1.reset_index(drop=True)\\np2 = p2.reset_index(drop=True)\\n\\n# remove duplicate pairs\\np12 = pd.concat([p1[\\\"id\\\"], p2[\\\"id\\\"]], axis=1)\\np12.columns = [\\\"id\\\", \\\"id2\\\"]\\np12 = p12.reset_index()\\n\\n# flip - only keep one of the flipped pairs, the other one is truly redundant\\nidx = p12[\\\"id\\\"] > p12[\\\"id2\\\"]\\np12[\\\"t\\\"] = p12[\\\"id\\\"]\\np12[\\\"id\\\"].loc[idx] = p12[\\\"id2\\\"].loc[idx]\\np12[\\\"id2\\\"].loc[idx] = p12[\\\"t\\\"].loc[idx]\\n\\np12 = p12.sort_values(by=[\\\"id\\\", \\\"id2\\\"]).reset_index(drop=True)\\np12 = p12.drop_duplicates(subset=[\\\"id\\\", \\\"id2\\\"])\\n\\n# also drop id == id2 - it may happen\\np12 = p12.loc[p12[\\\"id\\\"] != p12[\\\"id2\\\"]]\\np1 = p1.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\np2 = p2.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\n\\ndel p12, idx\\ngc.collect()\";\n",
       "                var nbb_formatted_code = \"# Reset index after Vincent's candidate addition\\np1 = p1.reset_index(drop=True)\\np2 = p2.reset_index(drop=True)\\n\\n# remove duplicate pairs\\np12 = pd.concat([p1[\\\"id\\\"], p2[\\\"id\\\"]], axis=1)\\np12.columns = [\\\"id\\\", \\\"id2\\\"]\\np12 = p12.reset_index()\\n\\n# flip - only keep one of the flipped pairs, the other one is truly redundant\\nidx = p12[\\\"id\\\"] > p12[\\\"id2\\\"]\\np12[\\\"t\\\"] = p12[\\\"id\\\"]\\np12[\\\"id\\\"].loc[idx] = p12[\\\"id2\\\"].loc[idx]\\np12[\\\"id2\\\"].loc[idx] = p12[\\\"t\\\"].loc[idx]\\n\\np12 = p12.sort_values(by=[\\\"id\\\", \\\"id2\\\"]).reset_index(drop=True)\\np12 = p12.drop_duplicates(subset=[\\\"id\\\", \\\"id2\\\"])\\n\\n# also drop id == id2 - it may happen\\np12 = p12.loc[p12[\\\"id\\\"] != p12[\\\"id2\\\"]]\\np1 = p1.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\np2 = p2.loc[p12[\\\"index\\\"]].reset_index(drop=True)\\n\\ndel p12, idx\\ngc.collect()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset index after Vincent's candidate addition\n",
    "p1 = p1.reset_index(drop=True)\n",
    "p2 = p2.reset_index(drop=True)\n",
    "\n",
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates : 28.3M\n",
      "Proportion of positive candidates: 2.50%\n",
      "Proportion of found matches: 97.85%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 72;\n",
       "                var nbb_unformatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_formatted_code = \"print_infos(p1, p2, N_TO_FIND)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/kaggle/foursquare/src/matching.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[\"id2\"] = p2[\"id\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Highest reachable IoU : 0.9898\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 73;\n",
       "                var nbb_unformatted_code = \"get_CV(\\n    p1,\\n    p2,\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    train,\\n)\";\n",
       "                var nbb_formatted_code = \"get_CV(\\n    p1,\\n    p2,\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    np.array(p1[\\\"point_of_interest\\\"] == p2[\\\"point_of_interest\\\"]).astype(np.int8),\\n    train,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 74;\n",
       "                var nbb_unformatted_code = \"if not DEBUG:\\n    if IS_TEST:\\n        p1.to_csv(OUT_PATH + \\\"p1_yv_test.csv\\\", index=False)\\n        p2.to_csv(OUT_PATH + \\\"p2_yv_test.csv\\\", index=False)\\n    else:\\n        p1.to_csv(OUT_PATH + \\\"p1_yv_train.csv\\\", index=False)\\n        p2.to_csv(OUT_PATH + \\\"p2_yv_train.csv\\\", index=False)\\n\\nprint(\\\"Done !\\\")\";\n",
       "                var nbb_formatted_code = \"if not DEBUG:\\n    if IS_TEST:\\n        p1.to_csv(OUT_PATH + \\\"p1_yv_test.csv\\\", index=False)\\n        p2.to_csv(OUT_PATH + \\\"p2_yv_test.csv\\\", index=False)\\n    else:\\n        p1.to_csv(OUT_PATH + \\\"p1_yv_train.csv\\\", index=False)\\n        p2.to_csv(OUT_PATH + \\\"p2_yv_train.csv\\\", index=False)\\n\\nprint(\\\"Done !\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not DEBUG:\n",
    "    if IS_TEST:\n",
    "        p1.to_csv(OUT_PATH + \"p1_yv_test.csv\", index=False)\n",
    "        p2.to_csv(OUT_PATH + \"p2_yv_test.csv\", index=False)\n",
    "    else:\n",
    "        p1.to_csv(OUT_PATH + \"p1_yv_train.csv\", index=False)\n",
    "        p2.to_csv(OUT_PATH + \"p2_yv_train.csv\", index=False)\n",
    "\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_yv = p1.copy()\n",
    "p2_yv = p2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from numerize.numerize import numerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.preparation import prepare_train_data, prepare_triplet_data\n",
    "from data.dataset import SingleDataset\n",
    "from data.tokenization import get_tokenizer\n",
    "\n",
    "from model_zoo.models import SingleTransformer\n",
    "\n",
    "from utils.logger import Config\n",
    "from utils.torch import load_model_weights\n",
    "from utils.metrics import *\n",
    "\n",
    "from inference.predict import predict\n",
    "from inference.knn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = prepare_train_data(root=DATA_PATH)\n",
    "folds = pd.read_csv(DATA_PATH + \"folds_2.csv\")[[\"id\", \"fold\"]]\n",
    "df = df.merge(folds, how=\"left\", on=\"id\").set_index(\"id\")\n",
    "\n",
    "FOLD = 0\n",
    "df = df[df[\"fold\"] == FOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tmp = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "\n",
    "# poi_mapping = {\n",
    "#     p: m\n",
    "#     for p, m in zip(\n",
    "#         train_tmp[\"point_of_interest\"],\n",
    "#         train_tmp[\"point_of_interest\"].astype(\"category\").cat.codes,\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# del train_tmp\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = LOG_PATH + \"2022-05-19/2/\"  # 1 ep, d=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", \"r\")))\n",
    "\n",
    "tokenizer = get_tokenizer(config.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(EXP_FOLDER + f\"fts_val_{FOLD}.npy\"):\n",
    "    preds = np.load(EXP_FOLDER + f\"fts_val_{FOLD}.npy\")\n",
    "else:\n",
    "    dataset = SingleDataset(df, tokenizer, config.max_len, use_url=False)\n",
    "\n",
    "    weights = sorted(glob.glob(EXP_FOLDER + \"*.pt\"))\n",
    "\n",
    "    model = SingleTransformer(\n",
    "        config.name,\n",
    "        nb_layers=config.nb_layers,\n",
    "        no_dropout=config.no_dropout,\n",
    "        embed_dim=config.embed_dim,\n",
    "        nb_features=config.nb_features,\n",
    "    ).cuda()\n",
    "    model.zero_grad()\n",
    "\n",
    "    model = load_model_weights(model, weights[FOLD])\n",
    "\n",
    "    preds = predict(model, dataset, config.data_config)\n",
    "    np.save(EXP_FOLDER + f\"fts_val_{FOLD}.npy\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_matches = json.load(open(DATA_PATH + \"gt.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [100]:\n",
    "    # for n_neighbors in [50, 100]:\n",
    "    print(f\"\\n- -> n_neighbors={n_neighbors}\\n\")\n",
    "\n",
    "    nn_matches = find_matches(preds, df, n_neighbors)\n",
    "\n",
    "    naive_matches = json.load(\n",
    "        open(OUT_PATH + f\"dist_matches_{n_neighbors}_0.json\", \"r\")\n",
    "    )\n",
    "\n",
    "    # UNION\n",
    "    merged_matches = {\n",
    "        k: list(set(naive_matches[k] + nn_matches[k])) for k in nn_matches\n",
    "    }\n",
    "\n",
    "    df_pairs = create_pairs(nn_matches, naive_matches, n_neighbors, gt_matches)\n",
    "    prop = df_pairs[\"match\"].sum() / len(df_pairs) * 100\n",
    "    save_path = EXP_FOLDER + f\"df_pairs_{n_neighbors}.csv\"\n",
    "\n",
    "    if SAVE:\n",
    "        df_pairs.to_csv(save_path, index=False)\n",
    "        print(f\"-> Saved pairs to {save_path} - Positive proportion  {prop:.2f}%\\n\")\n",
    "    else:\n",
    "        print(f\"Positive proportion  {prop:.2f}%\\n\")\n",
    "\n",
    "    # INTERSECTION\n",
    "    merged_matches = {\n",
    "        k: list(set(naive_matches[k]).intersection(nn_matches[k])) for k in nn_matches\n",
    "    }\n",
    "    found_prop, missed = compute_found_prop(merged_matches, gt_matches)\n",
    "    n_matches = sum([len(merged_matches[k]) for k in merged_matches])\n",
    "    print(\"Merged matches - Intersection :\")\n",
    "    print(\n",
    "        f\"Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.\"\n",
    "    )\n",
    "    print(f\"Best reachable IoU : {compute_best_iou(merged_matches, gt_matches) :.3f}\")\n",
    "\n",
    "    df_pairs_i = df_pairs[\n",
    "        (df_pairs[\"rank\"] >= -0.5) & (df_pairs[\"rank_nn\"] >= -0.5)\n",
    "    ].reset_index(drop=True)\n",
    "    prop = df_pairs_i[\"match\"].sum() / len(df_pairs_i) * 100\n",
    "\n",
    "    if SAVE:\n",
    "        df_pairs_i.to_csv(save_path, index=False)\n",
    "        print(f\"-> Saved pairs to {save_path} - Positive proportion  {prop:.2f}%\\n\")\n",
    "    else:\n",
    "        print(f\"Positive proportion  {prop:.2f}%\\n\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pairs_used = df_pairs[df_pairs[\"rank_nn\"] >= 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs_used = df_pairs_i.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1_theo = (\n",
    "    (\n",
    "        df_pairs_used[[\"id_1\"]]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"id_1\": \"id\"})\n",
    "        .merge(df.reset_index()[[\"id\", \"point_of_interest\"]])\n",
    "    )\n",
    "    .sort_values(\"index\")\n",
    "    .set_index(\"index\")\n",
    ")\n",
    "\n",
    "p2_theo = (\n",
    "    (\n",
    "        df_pairs_used[[\"id_2\"]]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"id_2\": \"id\"})\n",
    "        .merge(df.reset_index()[[\"id\", \"point_of_interest\"]])\n",
    "    )\n",
    "    .sort_values(\"index\")\n",
    "    .set_index(\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_theo[\"point_of_interest\"] = p1_theo[\"point_of_interest\"].map(poi_mapping)\n",
    "p2_theo[\"point_of_interest\"] = p2_theo[\"point_of_interest\"].map(poi_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_infos(p1_theo, p2_theo, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_CV(\n",
    "    p1_theo,\n",
    "    p2_theo,\n",
    "    np.array(p1_theo[\"point_of_interest\"] == p2_theo[\"point_of_interest\"]).astype(\n",
    "        np.int8\n",
    "    ),\n",
    "    np.array(p1_theo[\"point_of_interest\"] == p2_theo[\"point_of_interest\"]).astype(\n",
    "        np.int8\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index after Vincent's candidate addition\n",
    "p1 = pd.concat([p1_final, p1_theo]).reset_index(drop=True)\n",
    "p2 = pd.concat([p2_final, p2_theo]).reset_index(drop=True)\n",
    "\n",
    "# remove duplicate pairs\n",
    "p12 = pd.concat([p1[\"id\"], p2[\"id\"]], axis=1)\n",
    "p12.columns = [\"id\", \"id2\"]\n",
    "p12 = p12.reset_index()\n",
    "\n",
    "# flip - only keep one of the flipped pairs, the other one is truly redundant\n",
    "idx = p12[\"id\"] > p12[\"id2\"]\n",
    "p12[\"t\"] = p12[\"id\"]\n",
    "p12[\"id\"].loc[idx] = p12[\"id2\"].loc[idx]\n",
    "p12[\"id2\"].loc[idx] = p12[\"t\"].loc[idx]\n",
    "\n",
    "p12 = p12.sort_values(by=[\"id\", \"id2\"]).reset_index(drop=True)\n",
    "p12 = p12.drop_duplicates(subset=[\"id\", \"id2\"])\n",
    "\n",
    "# also drop id == id2 - it may happen\n",
    "p12 = p12.loc[p12[\"id\"] != p12[\"id2\"]]\n",
    "p1 = p1.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "p2 = p2.loc[p12[\"index\"]].reset_index(drop=True)\n",
    "\n",
    "del p12, idx\n",
    "gc.collect()\n",
    "\n",
    "y = np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8)\n",
    "print(\n",
    "    \"removed duplicates\",\n",
    "    p1.shape[0],\n",
    "    (p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).sum(),\n",
    "    int(time.time() - start_time),\n",
    "    \"sec\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter n=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_infos(p1, p2, N_TO_FIND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_CV(\n",
    "    p1,\n",
    "    p2,\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    "    np.array(p1[\"point_of_interest\"] == p2[\"point_of_interest\"]).astype(np.int8),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
