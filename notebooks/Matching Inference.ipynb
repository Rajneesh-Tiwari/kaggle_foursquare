{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to infer pairs from NN models\n",
    "**TODO**:\n",
    "- Intersection\n",
    "- Filter using dist_nn < th_1 and dist_pos < th_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from numerize.numerize import numerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.preparation import prepare_train_data, prepare_triplet_data\n",
    "from data.dataset import SingleDataset\n",
    "from data.tokenization import get_tokenizer\n",
    "\n",
    "from model_zoo.models import SingleTransformer\n",
    "\n",
    "from utils.logger import Config\n",
    "from utils.torch import load_model_weights\n",
    "from utils.metrics import *\n",
    "\n",
    "from inference.predict import predict\n",
    "from inference.knn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = prepare_train_data(root=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv(DATA_PATH + \"folds_2.csv\")[['id', 'fold']]\n",
    "df = df.merge(folds, how=\"left\", on=\"id\").set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[df['fold'] == FOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_matches = json.load(open(DATA_PATH + \"gt.json\", 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_FOLDER = LOG_PATH + \"2022-05-18/2/\"\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-18/3/\"  # 10 ep\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-19/0/\"  # 2 ep, triplets_v2\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-19/1/\"  # 1 ep, triplets_v2, d=64\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-19/2/\"  # 1 ep, d=256\n",
    "EXP_FOLDER = LOG_PATH + \"2022-05-19/4/\"  # 1 ep, d=256, large\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-20/0/\"  # 1 ep, d=256, triplets_v2\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-20/1/\"  # robert-large\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-20/2/\"  # base + url\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-20/3/\"  # large + no address\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-22/0/\"  # 1 ep, d=256, large lower\n",
    "# EXP_FOLDER = LOG_PATH + \"2022-05-23/0/\"  # 1 ep, d=384, large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDER + \"config.json\", 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(config.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SingleDataset(df, tokenizer, config.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = sorted(glob.glob(EXP_FOLDER + \"*.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleTransformer(\n",
    "    config.name,\n",
    "    nb_layers=config.nb_layers,\n",
    "    no_dropout=config.no_dropout,\n",
    "    embed_dim=config.embed_dim,\n",
    "    nb_features=config.nb_features,\n",
    ").cuda()\n",
    "model.zero_grad()\n",
    "\n",
    "model = load_model_weights(model, weights[FOLD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(EXP_FOLDER + f\"fts_val_{FOLD}.npy\"):\n",
    "    preds = np.load(EXP_FOLDER + f\"fts_val_{FOLD}.npy\")\n",
    "else:\n",
    "    preds = predict(model, dataset, config.data_config)\n",
    "    np.save(EXP_FOLDER + f\"fts_val_{FOLD}.npy\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for n_neighbors in [20, 30, 40, 50]:\n",
    "    print(f'\\n- -> n_neighbors={n_neighbors}\\n')\n",
    "\n",
    "    preds_matches = find_matches(preds, df, n_neighbors)\n",
    "    found_prop, missed_nn = compute_found_prop(preds_matches, gt_matches)\n",
    "    n_matches = sum([len(preds_matches[k]) for k in preds_matches])\n",
    "    print(f' - Roberta :\\t Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')\n",
    "\n",
    "#     position_matches = json.load(open(OUT_PATH + f\"knn_preds_{n_neighbors}_0.json\", 'r'))\n",
    "#     position_matches = json.load(open(OUT_PATH + f\"dist_matches_{n_neighbors}_0.json\", 'r'))\n",
    "    position_matches = json.load(open(OUT_PATH + f\"dist-phone-url_matches_{n_neighbors}_0.json\", 'r'))\n",
    "\n",
    "    found_prop, missed_pos = compute_found_prop(position_matches, gt_matches)\n",
    "    n_matches = sum([len(position_matches[k]) for k in position_matches])\n",
    "    print(f' - Position :\\t Found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')\n",
    "\n",
    "    merged_matches = {k : list(set(position_matches[k] + preds_matches[k])) for k in preds_matches}\n",
    "    found_prop, missed = compute_found_prop(merged_matches, gt_matches)\n",
    "    n_matches = sum([len(merged_matches[k]) for k in merged_matches])\n",
    "    print(f' - Merged :\\t found {found_prop * 100 :.2f}% of matches with {numerize(n_matches)} candidates.')\n",
    "\n",
    "    if SAVE:\n",
    "        df_pairs = create_pairs(merged_matches, gt_matches=gt_matches)\n",
    "        prop = df_pairs['match'].sum() / len(df_pairs) * 100\n",
    "        save_path = EXP_FOLDER + f'df_pairs_{n_neighbors}.csv'\n",
    "        df_pairs.to_csv(save_path, index=False)\n",
    "        print(f'-> Saved pairs to {save_path} - Positive proportion  {prop:.2f}%')\n",
    "        \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base:\n",
    "- -> n_neighbors=20\n",
    "\n",
    " - Roberta :\t Found 61.96% of matches with 10.82M candidates.\n",
    " - Position :\t Found 81.25% of matches with 11.07M candidates.\n",
    " - Merged :\t found 91.57% of matches with 21.48M candidates.\n",
    "\n",
    "- -> n_neighbors=30\n",
    "\n",
    " - Roberta :\t Found 64.76% of matches with 16.51M candidates.\n",
    " - Position :\t Found 83.96% of matches with 16.51M candidates.\n",
    " - Merged :\t found 93.31% of matches with 32.53M candidates.\n",
    "\n",
    "- -> n_neighbors=40\n",
    "\n",
    " - Roberta :\t Found 66.67% of matches with 22.21M candidates.\n",
    " - Position :\t Found 85.63% of matches with 21.87M candidates.\n",
    " - Merged :\t found 94.32% of matches with 43.51M candidates.\n",
    "\n",
    "- -> n_neighbors=50\n",
    "\n",
    " - Roberta :\t Found 68.15% of matches with 27.9M candidates.\n",
    " - Position :\t Found 86.83% of matches with 27.13M candidates.\n",
    " - Merged :\t found 95.00% of matches with 54.41M candidates.\n",
    " \n",
    "Large:\n",
    "- -> n_neighbors=20\n",
    "\n",
    " - Roberta :\t Found 70.42% of matches with 10.82M candidates.\n",
    " - Position :\t Found 81.25% of matches with 11.07M candidates.\n",
    " - Merged :\t found 92.88% of matches with 21.46M candidates.\n",
    "\n",
    "- -> n_neighbors=30\n",
    "\n",
    " - Roberta :\t Found 72.90% of matches with 16.51M candidates.\n",
    " - Position :\t Found 83.96% of matches with 16.51M candidates.\n",
    " - Merged :\t found 94.42% of matches with 32.52M candidates.\n",
    "\n",
    "- -> n_neighbors=40\n",
    "\n",
    " - Roberta :\t Found 74.59% of matches with 22.21M candidates.\n",
    " - Position :\t Found 85.63% of matches with 21.87M candidates.\n",
    " - Merged :\t found 95.32% of matches with 43.5M candidates.\n",
    "\n",
    "- -> n_neighbors=50\n",
    "\n",
    " - Roberta :\t Found 75.87% of matches with 27.9M candidates.\n",
    " - Position :\t Found 86.83% of matches with 27.13M candidates.\n",
    " - Merged :\t found 95.92% of matches with 54.41M candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if len(gt_matches[df.index[i]]) <= 1:\n",
    "        continue\n",
    "    if not len(df.loc[list(missed[i])]):\n",
    "        continue\n",
    "\n",
    "    print('Query')\n",
    "    display(df.loc[[df.index[i]]])\n",
    "\n",
    "    print('Target')\n",
    "    display(df.loc[gt_matches[df.index[i]]])\n",
    "\n",
    "    print('Missed')\n",
    "    display(df.loc[list(missed[i])])\n",
    "\n",
    "#     print('Preds')\n",
    "#     display(df.loc[preds_matches[df.index[i]]].head(5))\n",
    "\n",
    "#     break\n",
    "    print('-' * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
