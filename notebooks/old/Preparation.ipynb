{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from params import *\n",
    "\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfg = df[[\"id\", \"point_of_interest\", \"country\"]].groupby('point_of_interest').agg(list)\n",
    "# dfg['country'] = dfg['country'].apply(lambda x: np.unique(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = pd.read_csv(DATA_PATH + \"pairs.csv\")\n",
    "\n",
    "pairs = pairs.merge(df[['id', 'point_of_interest']], left_on=\"id_1\", right_on=\"id\").drop('id', axis=1)\n",
    "pairs = pairs.merge(\n",
    "    df[['id', 'point_of_interest']],\n",
    "    left_on=\"id_2\",\n",
    "    right_on=\"id\",\n",
    "    suffixes=('_1', '_2')\n",
    ").drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(DATA_PATH + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster of pairs for folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pois in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = pairs[[\"point_of_interest_1\", \"point_of_interest_2\"]].copy()\n",
    "pois_ = pois.copy()\n",
    "pois_.columns = [\"point_of_interest_2\", \"point_of_interest_1\"]\n",
    "\n",
    "pois = pd.concat([pois, pois_]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_merged = pois[[\"point_of_interest_1\"]].drop_duplicates(keep=\"first\").merge(\n",
    "    pois, on=\"point_of_interest_1\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_merged = pois_merged.groupby('point_of_interest_1').agg(lambda x: sorted(np.unique(list(x)))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pois_merged['point_of_interest_2'] = pois_merged.apply(\n",
    "    lambda x: list(set([x.point_of_interest_1] + list(x.point_of_interest_2))), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_merged['len'] = pois_merged['point_of_interest_2'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_merged = pois_merged[pois_merged['len'] > 1].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pois_merged['pois'] = pois_merged['point_of_interest_2'].apply(lambda x: ' '.join(sorted(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_merged = pois_merged.drop_duplicates(keep=\"first\", subset='pois').copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pois_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POIs clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "num = 0\n",
    "merges = {}\n",
    "\n",
    "for i, pois in enumerate(tqdm(pois_merged['point_of_interest_2'])):    \n",
    "    found = False\n",
    "\n",
    "    assert len(pois) > 1\n",
    "    \n",
    "    for poi in pois:\n",
    "        try:\n",
    "            found_idx = mapping[poi]\n",
    "            found = True\n",
    "#             print(f'Found {poi} in clust {found_idx}')\n",
    "            break\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    if found:\n",
    "        already_found_ids = []\n",
    "        for poi in pois:\n",
    "            try:\n",
    "                already_found_ids.append(mapping[poi])\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "            mapping[poi] = found_idx\n",
    "\n",
    "        already_found_ids = list(set(already_found_ids))\n",
    "        try:\n",
    "            already_found_ids.remove(found_idx)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if len(already_found_ids):\n",
    "            for k in mapping:\n",
    "                if mapping[k] in already_found_ids:\n",
    "                    mapping[k] = found_idx\n",
    "             \n",
    "    else:\n",
    "        for poi in pois:            \n",
    "            mapping[poi] = num\n",
    "        num += 1\n",
    "        \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusts = pd.DataFrame.from_dict(mapping, orient=\"index\").reset_index()\n",
    "clusts.columns = [\"poi\", \"clust\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs.merge(clusts, left_on=\"point_of_interest_1\", right_on=\"poi\")\n",
    "\n",
    "pairs = pairs.merge(\n",
    "    clusts,\n",
    "    left_on=\"point_of_interest_2\",\n",
    "    right_on=\"poi\",\n",
    "    suffixes=('_1', '_2')\n",
    ")\n",
    "\n",
    "pairs[pairs['clust_1'] != pairs['clust_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusts.to_csv(DATA_PATH + \"poi_clusts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling no clusts pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(clusts, left_on=\"point_of_interest\", right_on=\"poi\", how=\"left\").drop('poi', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_map = list(set(df[df['clust'].isna()]['point_of_interest'].values))\n",
    "\n",
    "current = int(np.max(df['clust'])) + 1\n",
    "new_map = {i : k + current for k, i in enumerate(to_map)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_clusts = pd.DataFrame.from_dict(new_map, orient=\"index\").reset_index()\n",
    "new_clusts.columns = [\"point_of_interest\", \"clust\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(new_clusts, how=\"left\", on=\"point_of_interest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['clust_x', 'clust_y']] = df[['clust_x', 'clust_y']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clust'] = (df['clust_x'] + df['clust_y']).astype(int)\n",
    "df.drop(['clust_x', 'clust_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.read_csv(DATA_PATH + \"pairs.csv\")\n",
    "\n",
    "pairs = pairs.merge(df[['id', 'point_of_interest', 'clust']], left_on=\"id_1\", right_on=\"id\").drop('id', axis=1)\n",
    "pairs = pairs.merge(\n",
    "    df[['id', 'point_of_interest', 'clust']],\n",
    "    left_on=\"id_2\",\n",
    "    right_on=\"id\",\n",
    "    suffixes=('_1', '_2')\n",
    ").drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs[pairs['clust_1'] != pairs['clust_2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(DATA_PATH + \"df_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "\n",
    "gkf = GroupKFold(n_splits=K)\n",
    "\n",
    "splits = gkf.split(df, groups=df['clust'])\n",
    "\n",
    "\n",
    "df_split = df[[\"id\", \"point_of_interest\", \"clust\"]].copy()\n",
    "df_split['fold'] = -1\n",
    "\n",
    "for i, (_, val_idx) in enumerate(splits):\n",
    "    df_split.loc[val_idx, 'fold'] = i\n",
    "    \n",
    "# df_split.to_csv(DATA_PATH + f\"folds_{K}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df_split.groupby('clust').agg(list)\n",
    "dfg['fold'].apply(lambda x: len(np.unique(x))).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = pairs[[\"id_1\", \"id_2\", \"point_of_interest_1\", \"point_of_interest_2\", \"match\", \"clust_1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.read_csv(DATA_PATH + \"pairs.csv\")\n",
    "\n",
    "pairs = pairs.merge(df[['id', 'point_of_interest', 'clust']], left_on=\"id_1\", right_on=\"id\").drop('id', axis=1)\n",
    "pairs = pairs.merge(\n",
    "    df[['id', 'point_of_interest', 'clust']],\n",
    "    left_on=\"id_2\",\n",
    "    right_on=\"id\",\n",
    "    suffixes=('_1', '_2')\n",
    ").drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pairs[[\"id_1\", \"id_2\", \"match\"]].copy()\n",
    "ids_ = ids.copy()\n",
    "ids_.columns = [\"id_2\", \"id_1\", \"match\"]\n",
    "\n",
    "ids = pd.concat([ids, ids_]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = ids[[\"id_1\"]].drop_duplicates(keep=\"first\").merge(\n",
    "    ids, on=\"id_1\", how=\"left\"\n",
    ")\n",
    "\n",
    "triplets = triplets.groupby('id_1').agg(list).reset_index()\n",
    "\n",
    "triplets = triplets[triplets['match'].apply(lambda x: True in x)]\n",
    "# triplets = triplets[triplets['match'].apply(lambda x: True in x and False in x)]\n",
    "\n",
    "triplets.columns = ['id', 'paired_ids', 'matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets.to_csv(DATA_PATH + 'triplets.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
